---
name: Update LLM Extraction Prompt for Better Line Item Parsing
overview: ""
todos:
  - id: c6ebae06-040c-456a-bbba-5553bdc04881
    content: Remove cls parameter from PaddleOCR calls in ocr_processor.py (lines 177, 180, 251, 254)
    status: completed
  - id: 476e5dc0-eb3b-4295-8a0b-10f2f24dcb30
    content: Remove cls=True parameter from PaddleOCR call in table_extractor.py (line 235)
    status: completed
  - id: 55671f26-f375-48aa-9c93-9065fe3f22e9
    content: Create fix_db_schema.py script to add missing invoice_number and bbox columns safely
    status: completed
  - id: f9eeeb5d-1024-4caf-b00c-bd16daf1e390
    content: Remove cls parameter from PaddleOCR calls in ocr_processor.py (lines 177, 180, 251, 254)
    status: pending
  - id: 6999ffa4-2f63-4e62-b7d6-15fd6a16e182
    content: Remove cls=True parameter from PaddleOCR call in table_extractor.py (line 235)
    status: pending
  - id: ebe79e86-4869-4c22-8aab-26fc523db6fb
    content: Create fix_db_schema.py script to add missing invoice_number and bbox columns safely
    status: pending
---

# Update LLM Extraction Prompt for Better Line Item Parsing

## Current State

The system prompt is currently defined as `self.system_prompt` in the `__init__` method (lines 309-344) and used directly in `_call_ollama_with_retry` (line 420).

## Changes Required

### 1. Add `_get_extraction_prompt` method

Create a new method `_get_extraction_prompt(self, ocr_text: str) -> str` that returns the complete prompt (system + user) with the new, more robust system prompt.

**Location:** Add after `__init__` method, before `parse_document` method (around line 345)

**New system prompt features:**

- **Merged Columns Rule:** Handle cases like "6 12 LITRE PEPSI" where first number is quantity (6) and rest is description
- **Math Derivation Rule:** Calculate quantity from Total/Unit Price if missing or 0
- **Default Rule:** Default quantity to 1 if cannot be calculated
- **Better noise filtering:** Explicit rules for ignoring addresses, phone numbers, VAT IDs, etc.
- **Clearer output format:** More explicit JSON schema

### 2. Update `_call_ollama_with_retry` to use new method

Replace the inline prompt building (lines 412-420) with a call to `_get_extraction_prompt(ocr_text)`.

**Change:**

- Remove: `user_prompt = f"Extract invoice data from this OCR text:\n\n{ocr_text}"`
- Remove: `payload["prompt"] = f"{self.system_prompt}\n\n{user_prompt}"`
- Add: `payload["prompt"] = self._get_extraction_prompt(ocr_text)`

### 3. Remove old `self.system_prompt` from `__init__`

Since the prompt will now be generated by `_get_extraction_prompt`, we can remove the `self.system_prompt` assignment from `__init__` (lines 309-344).

**Note:** Keep the assignment but it won't be used, or remove it entirely. For safety, we'll keep it commented out initially.

## Implementation Details

The new `_get_extraction_prompt` method will:

1. Define the comprehensive system prompt with all the extraction rules
2. Format the user prompt with the OCR text
3. Return the combined prompt string

This addresses:

- **Stori Invoice Issue:** Will calculate `qty` from `total_price / unit_price` if missing
- **Red Dragon Invoice Issue:** Will split "6 12 LITRE" into `qty: 6` and `description: "12 LITRE PEPSI"`

## Files to Modify

- `backend/llm/invoice_parser.py`
- Add `_get_extraction_prompt` method (after `__init__`, before `parse_document`)
- Update `_call_ollama_with_retry` to use the new method
- Optionally remove/comment out old `self.system_prompt` in `__init__`