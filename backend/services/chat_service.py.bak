"""
Chat Service

Manages conversational chat with Ollama LLM, handles code context,
and formats responses for the chat assistant.
"""

import logging
import requests
import re
import time
import uuid
import json
import os
import threading
from typing import List, Dict, Any, Optional, Tuple, Callable
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from backend.services.code_reader import CodeReader
from backend.services.code_explorer import CodeExplorer
from backend.services.code_verifier import CodeVerifier
from backend.services.response_validator import ResponseValidator
from backend.services.runtime_verifier import RuntimeVerifier
from backend.services.architecture_analyzer import ArchitectureAnalyzer
from backend.services.response_rewriter import ResponseRewriter
from backend.services.model_registry import get_registry, ModelRegistry
from backend.services.chat_metrics import get_metrics
from backend.services.retry_handler import RetryHandler
from backend.services.explorer_config import get_config
from backend.services.exploration_history import save_exploration_session

logger = logging.getLogger("owlin.services.chat_service")

# Agent system prompts
AGENT_SYSTEM_PROMPT = """You are a code detective agent for Owlin. Your job is to FIND problems, not just analyze what's given to you.

WORKFLOW:
1. UNDERSTAND the problem
2. EXPLORE the codebase to find relevant code
3. TRACE data flows
4. ANALYZE the actual code you find
5. PROVIDE specific fixes with file:line references

AVAILABLE TOOLS:
- SEARCH <concept> - Find files/functions related to concept
- READ <file> - Read a file (you can request specific line ranges)
- TRACE <start> → <end> - Trace data flow between components
- GREP <pattern> - Search for code patterns
- FIND_CALLS <function> - Find where function is called

EXPLORATION STRATEGY:
When investigating a problem:
1. Start with broad searches (e.g., "line items", "upload display")
2. Narrow down to specific files
3. Read relevant code sections
4. Trace data flow through the code
5. Identify where the problem occurs

RESPONSE FORMAT:
After exploration, provide:
**Files Explored:** [list with line ranges]
**Data Flow:** [trace through code]
**Root Cause:** [specific file:line]
**Fix:** [exact code changes]

CRITICAL: You must explore and find the code yourself. Don't ask for code - search for it!"""

AGENT_DEBUGGING_PROMPT = """You are a code detective agent investigating a bug in Owlin.

YOUR MISSION: Find the problem by verifying runtime behavior, not just reading code.

CRITICAL: VERIFY BEFORE FIXING
- Don't assume code executes - verify with logs
- Don't assume data exists - check database/API
- Don't assume conditions are met - add diagnostic logging
- Focus on PRIMARY issues (why data is empty) not SECONDARY (error handling)

AVAILABLE TOOLS:
- SEARCH <concept> - Find files/functions related to concept
- READ <file> - Read a file completely
- TRACE <start> → <end> - Trace data flow
- GREP <pattern> - Search for pattern (VERIFY function names exist!)
- ANALYZE - Provide analysis after exploration

WORKFLOW:
1. READ THE CODEBASE FIRST - Use READ, GREP, SEARCH to understand actual implementation
2. VERIFY FUNCTION NAMES - Use GREP to find actual function definitions BEFORE mentioning them
3. VERIFY EXISTING CODE - Check what logging/functions already exist (use GREP)
4. DETECT FRAMEWORK - Check if FastAPI/Flask, React/vanilla JS (use GREP for decorators/imports)
5. TRACE ACTUAL EXECUTION PATH - Follow real code flow through actual files
6. IDENTIFY SPECIFIC ISSUES - Find mismatches (invoice_id vs doc_id), wrong field names
7. VERIFY runtime behavior - Ask for logs, database queries, API responses
8. ADD diagnostic logging - Only where it doesn't already exist (check first with GREP)
9. FIX the root cause - Based on actual code, not generic suggestions

CRITICAL RULES - NEVER VIOLATE THESE:

1. VERIFY FUNCTION NAMES BEFORE CLAIMING THEY EXIST:
   - NEVER mention a function name without first using GREP to verify it exists
   - Example: Before saying "upload_document()", run: GREP "def upload|@app.post.*upload"
   - If GREP finds nothing, the function DOES NOT EXIST - don't mention it
   - If GREP finds a different name (e.g., "upload_file"), use THAT name, not what you assumed
   - WRONG: "upload_document() calls extract_table_from_block()"
   - RIGHT: "GREP found upload_file() at backend/main.py:572, not upload_document()"

2. NEVER MAKE UP CODE EXAMPLES:
   - NEVER provide code examples unless you've READ the actual file
   - NEVER use Flask syntax (@app.route) if you haven't verified the framework
   - NEVER use request.form if you haven't verified FastAPI vs Flask
   - If you don't know the exact code, say: "I need to READ this file first to see the actual code"
   - WRONG: "@app.route('/upload', methods=['POST']) def upload_document(): doc_id = request.form['doc_id']"
   - RIGHT: "I need to READ backend/main.py to see the actual upload endpoint code"

3. VERIFY FRAMEWORK BEFORE ASSUMING:
   - Use GREP to check framework: GREP "@app\\.|@router\\.|from fastapi|from flask"
   - FastAPI uses: @app.post(), @router.post(), async def
   - Flask uses: @app.route(), def (not async)
   - React uses: import React, function Component, useState
   - Vanilla JS: No imports, just functions
   - WRONG: Assuming Flask when it's FastAPI
   - RIGHT: "GREP found @app.post() and 'from fastapi', so this is FastAPI, not Flask"

4. VERIFY LOGGING EXISTS BEFORE CLAIMING:
   - Use GREP to check if logging exists: GREP "Inserted.*line items|logger.info.*line"
   - If GREP finds nothing, the logging DOES NOT EXIST
   - WRONG: "logger.info(f'Inserted {len(line_items)} line items') exists"
   - RIGHT: "GREP found no logging for line items insertion. Actual logging is at ocr_service.py:316 with format [LINE_ITEMS]"

5. READ FILES COMPLETELY, NOT PARTIALLY:
   - READ the entire file, not just a snippet
   - Check function definitions, imports, decorators
   - Verify the actual function names, signatures, and structure
   - WRONG: Reading 10 lines and assuming the rest
   - RIGHT: READ backend/main.py completely to see all functions

6. USE ACTUAL FILE PATHS AND LINE NUMBERS:
   - NEVER use generic names like "file.py:100" or "ocr.py:50"
   - ALWAYS use actual paths: "backend/main.py:561" or "backend/services/ocr_service.py:266"
   - Use real line numbers from the code you've READ
   - Read files first to get exact line numbers

7. VERIFY RUNTIME BEHAVIOR FIRST - BE SPECIFIC:
   - Don't say: "Check backend logs"
   - Say: "Search logs for '[LINE_ITEMS]' messages and share: how many blocks, what types, any table blocks?"
   - Don't say: "Check if data is stored"
   - Say: "Run SQL: SELECT supplier, value, (SELECT COUNT(*) FROM invoice_line_items WHERE doc_id = invoices.doc_id) as items FROM invoices ORDER BY id DESC LIMIT 1;"
   - Don't say: "Check API response"
   - Say: "Run: curl http://localhost:8000/api/upload/status?doc_id=XXX and share the response"

8. PRIORITIZE BY LIKELIHOOD:
   - Don't list all possibilities equally
   - Rank by probability: "MOST LIKELY (90%): Table blocks not detected"
   - Then: "If that's not it (5%): Table extraction returns empty"
   - Then: "Least likely (5%): Format conversion fails"

9. PROVIDE EXACT LOGGING CODE (only if verified missing):
   - First: GREP to check if logging already exists
   - If missing: "Add: logger.info(f'[TABLE_EXTRACT] Block type: {{ocr_result.type}}') at backend/ocr/owlin_scan_pipeline.py:704"
   - Include the exact code snippet, file path, and line number
   - Use actual variable names from the code you READ

10. TRACE ACTUAL EXECUTION PATH:
   - Follow: Upload → OCR → Extraction → Storage → API → Frontend
   - Use actual file paths: "backend/main.py:561 → backend/services/ocr_service.py:266 → backend/app/db.py:323"
   - Use actual function names from GREP/READ, not assumed names
   - Check each step with specific queries/logs

11. FOCUS ON PRIMARY ISSUES:
   - Primary: Why is data empty/missing? (core problem)
   - Secondary: Error handling (can wait)
   - Don't fix secondary while primary is unsolved

VALIDATION CHECKLIST BEFORE ANALYSIS:
Before using ANALYZE, verify:
□ All function names mentioned were verified with GREP
□ All code examples are from actual READ files, not made up
□ Framework detected (FastAPI/Flask, React/vanilla JS) with GREP
□ All logging claims verified with GREP
□ All file paths are actual paths from READ commands
□ All line numbers are from actual code you READ
□ Execution path uses actual function names from GREP/READ

RESPONSE FORMAT - CRITICAL:
- Write primarily in TEXT explaining the problem
- Use SMALL code snippets (3-5 lines max) ONLY from actual code you READ
- Always include file:line references with each snippet
- DO NOT dump entire files or large code blocks
- Include: Verification steps, Runtime verification needed, Diagnostic steps, Root cause, Specific fix

Example format (based on ACTUAL code you READ):
**Code Analysis**:
- **Files Read**: READ backend/services/ocr_service.py, READ backend/app/db.py, READ backend/main.py
- **Framework Detected**: GREP found "@app.post" and "from fastapi" → FastAPI (not Flask)
- **Functions Verified**: 
  - GREP "def upload" → Found: upload_file() at backend/main.py:572 (NOT upload_document)
  - GREP "def.*line.*item" → Found: insert_line_items() at backend/app/db.py:323
- **Existing Logging**: GREP "Inserted.*line items" → NOT FOUND. GREP "[LINE_ITEMS]" → Found at ocr_service.py:316
- **Execution Path** (from actual code): upload_file() → _run_ocr_background() → process_document_ocr_v2() → insert_line_items()

**Prioritized Diagnosis:**

1. **MOST LIKELY (90%): invoice_id/doc_id mismatch**
   - **Verified**: READ backend/services/ocr_service.py:285 shows `invoice_id = doc_id`
   - **Verified**: READ backend/app/db.py:343 shows `get_line_items_for_invoice(invoice_id)` queries by invoice_id
   - **Issue Found**: "At ocr_service.py:285, invoice_id = doc_id, but get_line_items_for_invoice() at db.py:343 queries by invoice_id. If doc_id doesn't match invoice_id in database, query returns empty."
   - **Add logging** (GREP verified [INVOICE_ID] doesn't exist): `logger.info(f'[INVOICE_ID] doc_id={{doc_id}}, invoice_id={{invoice_id}}')` at `backend/services/ocr_service.py:285`
   - **Verify**: "Run SQL: SELECT doc_id, invoice_id FROM invoices ORDER BY id DESC LIMIT 1;"
   - **If not this**: Move to next item

**Execution Path** (from actual code):
`backend/main.py:572` (upload_file - VERIFIED with GREP) → `backend/services/ocr_service.py:266` (process_document_ocr_v2 - VERIFIED with READ) → `backend/app/db.py:323` (insert_line_items - VERIFIED with READ)

**Root Cause**: "invoice_id/doc_id mismatch: ocr_service.py:285 sets invoice_id = doc_id, but database query at db.py:343 may not find matching invoice_id"

**Fix**: [Show actual code change with BEFORE/AFTER from real code you READ]

CRITICAL: 
- NEVER mention a function name without GREP verification
- NEVER provide code examples unless you READ the actual file
- NEVER assume framework - verify with GREP
- If you're not certain, say "I need to verify this first" and use GREP/READ"""


class TaskTracker:
    """Tracks agent mode tasks with status, timing, and results."""
    
    def __init__(self):
        self.tasks: Dict[str, Dict] = {}
        self.task_order: List[str] = []
    
    def add_task(self, task_id: str, task_type: str, description: str, metadata: Optional[Dict] = None):
        """Add a new task to track."""
        self.tasks[task_id] = {
            "id": task_id,
            "type": task_type,
            "title": description,  # Alias for description
            "description": description,
            "status": "pending",
            "progress": 0,
            "start_time": None,
            "end_time": None,
            "duration": None,
            "started_at": None,  # Timestamp in ms
            "ended_at": None,  # Timestamp in ms
            "duration_ms": None,  # Duration in ms
            "result_count": 0,
            "error": None,
            "note": None,  # Short error message or note
            "metadata": metadata or {}
        }
        if task_id not in self.task_order:
            self.task_order.append(task_id)
    
    def start_task(self, task_id: str):
        """Mark a task as running."""
        if task_id in self.tasks:
            self.tasks[task_id]["status"] = "running"
            self.tasks[task_id]["start_time"] = time.time()
            self.tasks[task_id]["started_at"] = int(time.time() * 1000)  # Timestamp in ms
    
    def update_progress(self, task_id: str, progress: int):
        """Update task progress (0-100)."""
        if task_id in self.tasks:
            self.tasks[task_id]["progress"] = max(0, min(100, progress))
    
    def complete_task(self, task_id: str, result_count: int = 0, error: Optional[str] = None):
        """Mark a task as completed or failed."""
        if task_id in self.tasks:
            self.tasks[task_id]["status"] = "done" if not error else "failed"
            self.tasks[task_id]["end_time"] = time.time()
            self.tasks[task_id]["ended_at"] = int(time.time() * 1000)  # Timestamp in ms
            if self.tasks[task_id]["start_time"]:
                duration = self.tasks[task_id]["end_time"] - self.tasks[task_id]["start_time"]
                self.tasks[task_id]["duration"] = duration
                self.tasks[task_id]["duration_ms"] = int(duration * 1000)
            self.tasks[task_id]["progress"] = 100  # Set to 100 on completion
            self.tasks[task_id]["result_count"] = result_count
            if error:
                self.tasks[task_id]["error"] = error
                self.tasks[task_id]["note"] = error[:200] if error else None  # Short note
    
    def get_tasks(self) -> List[Dict]:
        """Get all tasks in order."""
        return [self.tasks[tid] for tid in self.task_order if tid in self.tasks]


class ChatService:
    """Service for managing chat conversations with code context."""
    
    def __init__(self, ollama_url: str = "http://localhost:11434", models: Optional[List[str]] = None):
        """
        Initialize the chat service with multi-model support.
        
        Args:
            ollama_url: Base URL for Ollama API
            models: List of model names in priority order (default: auto-detect from registry)
        """
        self.ollama_url = ollama_url
        self.code_reader = CodeReader()
        self.code_explorer = CodeExplorer()
        self.code_verifier = CodeVerifier()
        self.response_validator = ResponseValidator()
        self.runtime_verifier = RuntimeVerifier()
        self.architecture_analyzer = ArchitectureAnalyzer()
        self.response_rewriter = ResponseRewriter(code_verifier=self.code_verifier)
        config = get_config()
        self.retry_handler = RetryHandler(
            failure_threshold=config.circuit_breaker_threshold,
            timeout=config.circuit_breaker_timeout
        )
        self.ollama_available = self._check_ollama_available()
        
        # Initialize model registry
        self.model_registry = get_registry(ollama_url)
        
        # Set up model priority list
        if models:
            self.models = models
        else:
            # Default priority order
            self.models = [
                "qwen2.5-coder:7b",
                "deepseek-coder:6.7b",
                "codellama:7b",
                "llama3.2:3b"
            ]
        
        # Get available models from registry
        self.available_models = [
            model.name for model in self.model_registry.get_available_models()
            if model.name in self.models
        ]
        
        # Performance metrics tracking
        self._perf_metrics = []
        self._metrics_file = Path(self.code_reader.repo_root) / "data" / "chat_metrics.jsonl"
        self._metrics_file.parent.mkdir(parents=True, exist_ok=True)
    
    def _log_perf_metric(self, phase: str, **kwargs):
        """
        Log a performance metric as single-line JSON.
        
        Args:
            phase: Phase name (e.g., "build_plan", "searches", "reads", "traces", "analysis_call", "first_token")
            **kwargs: Additional metric data (ms, count, files, model, ctx, attempts, etc.)
        """
        metric = {
            "phase": phase,
            "timestamp": time.time(),
            **kwargs
        }
        self._perf_metrics.append(metric)
        
        # Write to file immediately (append mode)
        try:
            with open(self._metrics_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(metric) + "\n")
        except Exception as e:
            logger.warning(f"Failed to write perf metric: {e}")
        
        # Set primary model (first available from priority list)
        if self.available_models:
            self.model = self.available_models[0]
        elif models and len(models) > 0:
            self.model = models[0]  # Fallback to first in list even if not available
        else:
            self.model = "codellama:7b"  # Final fallback
        
        if self.ollama_available:
            logger.info(
                f"ChatService initialized with Ollama at {ollama_url}\n"
                f"Primary model: {self.model}\n"
                f"Available models: {', '.join(self.available_models) if self.available_models else 'none'}\n"
                f"Model priority: {', '.join(self.models)}"
            )
        else:
            logger.info("ChatService initialized without Ollama (will use fallback responses)")
    
    def _check_ollama_available(self) -> bool:
        """Check if Ollama is running and accessible."""
        endpoints_to_try = [
            "/api/tags",
            "/api/version", 
            "/"
        ]
        
        for endpoint in endpoints_to_try:
            try:
                url = f"{self.ollama_url}{endpoint}"
                logger.info(f"Checking Ollama at: {url}")
                response = requests.get(url, timeout=5)
                logger.info(f"Ollama response: status={response.status_code}, url={url}")
                
                if response.status_code == 200:
                    logger.info(f"✓ Ollama is available at {url}")
                    return True
            except requests.exceptions.ConnectionError as e:
                logger.warning(f"Connection failed to {url}: {e}")
            except requests.exceptions.Timeout as e:
                logger.warning(f"Timeout connecting to {url}: {e}")
            except Exception as e:
                logger.warning(f"Unexpected error checking {url}: {e}")
        
        logger.error(f"Ollama is NOT available at {self.ollama_url} (tried all endpoints)")
        return False
    
    def _classify_question(self, message: str) -> Dict[str, Any]:
        """
        Classify question type and extract intent.
        
        Returns:
            Dict with type, complexity, concepts, relationships
        """
        message_lower = message.lower()
        
        # Question type patterns
        debugging_patterns = [
            r"why\s+did\s+(.+?)\s+(?:but|however|yet)\s+(.+?)\s+(?:not|isn't|aren't|doesn't)",  # "why did X but Y not"
            r"why\s+did\s+(.+?)\s+(?:upload|work|succeed|complete)\s+but\s+(.+?)\s+(?:not|isn't|aren't|doesn't)",  # "why did upload but not showing"
            r"why\s+(?:doesn't|does|isn't|is|can't|can|won't|will)\s+(.+?)(?:\s+work|\s+show|\s+display|\s+appear)?",
            r"why\s+(?:is|are)\s+(.+?)\s+(?:not|missing|broken|wrong)",
            r"(.+?)\s+(?:doesn't|isn't|can't|won't)\s+(?:work|show|display|appear|load)",
            r"(.+?)\s+(?:not|missing|broken|wrong|failing)",
            r"(.+?)\s+(?:aren't|are\s+not)\s+(?:displayed|shown|appearing|working)",  # "aren't displayed"
            r"(.+?)\s+(?:was|were)\s+(?:successful|success)\s+but\s+(.+?)\s+(?:not|aren't|isn't)",  # "was successful but"
            r"(.+?)\s+(?:upload|uploaded)\s+but\s+(.+?)\s+(?:not|isn't|aren't|doesn't)",  # "uploaded but not showing"
            r"problem\s+with\s+(.+)",
            r"issue\s+with\s+(.+)",
            r"error\s+(?:with|in)\s+(.+)",
            r"find\s+(?:the|an?)\s+(?:issue|problem|bug|fault)",  # "find the issue"
            r"can\s+you\s+find\s+(?:the|an?)\s+(?:issue|problem|bug|fault)",  # "can you find the issue"
            r"can\s+you\s+provide\s+(?:the|actual)\s+code",  # "can you provide the actual code"
            r"show\s+me\s+(?:the|actual)\s+code",  # "show me the actual code"
            r"what\s+(?:is|are)\s+(?:the|an?)\s+(?:problem|issue|bug|fault)",  # "what is the problem"
        ]
        
        how_to_patterns = [
            r"how\s+(?:do|does|can|should)\s+(?:i|you|we)\s+(.+)",
            r"how\s+(?:to|do)\s+(.+)",
            r"how\s+(?:does|is)\s+(.+?)\s+(?:work|implemented|done)",
            r"steps?\s+(?:to|for)\s+(.+)",
            r"way\s+to\s+(.+)"
        ]
        
        what_is_patterns = [
            r"what\s+(?:is|are)\s+(.+)",
            r"what\s+(?:does|do)\s+(.+?)\s+(?:do|mean)",
            r"explain\s+(.+)",
            r"tell\s+me\s+about\s+(.+)",
            r"describe\s+(.+)"
        ]
        
        flow_patterns = [
            r"how\s+(?:does|do)\s+(.+?)\s+(?:flow|go|travel|move)",
            r"flow\s+(?:of|from|to)\s+(.+)",
            r"path\s+(?:of|from|to)\s+(.+)",
            r"(.+?)\s+(?:to|→|->)\s+(.+)",
            r"from\s+(.+?)\s+to\s+(.+)"
        ]
        
        comparison_patterns = [
            r"difference\s+(?:between|in)\s+(.+?)\s+(?:and|vs|versus)\s+(.+)",
            r"compare\s+(.+?)\s+(?:and|with|to)\s+(.+)",
            r"(.+?)\s+vs\s+(.+)",
            r"(.+?)\s+versus\s+(.+)"
        ]
        
        # Determine question type
        question_type = "general"
        complexity = "simple"
        concepts = []
        relationships = []
        
        # Check for debugging questions
        for pattern in debugging_patterns:
            match = re.search(pattern, message_lower, re.IGNORECASE)
            if match:
                question_type = "debugging"
                complexity = "complex"
                concepts.append(match.group(1).strip() if match.lastindex else "")
                break
        
        # Check for how-to questions
        if question_type == "general":
            for pattern in how_to_patterns:
                match = re.search(pattern, message_lower, re.IGNORECASE)
                if match:
                    question_type = "how-to"
                    concepts.append(match.group(1).strip() if match.lastindex else "")
                    break
        
        # Check for what-is questions
        if question_type == "general":
            for pattern in what_is_patterns:
                match = re.search(pattern, message_lower, re.IGNORECASE)
                if match:
                    question_type = "what-is"
                    concepts.append(match.group(1).strip() if match.lastindex else "")
                    break
        
        # Check for flow questions
        for pattern in flow_patterns:
            match = re.search(pattern, message_lower, re.IGNORECASE)
            if match:
                if question_type == "general":
                    question_type = "flow"
                complexity = "complex"
                if match.lastindex and match.lastindex >= 2:
                    relationships.append({
                        "from": match.group(1).strip(),
                        "to": match.group(2).strip()
                    })
                break
        
        # Check for comparison questions
        for pattern in comparison_patterns:
            match = re.search(pattern, message_lower, re.IGNORECASE)
            if match:
                question_type = "comparison"
                complexity = "complex"
                if match.lastindex and match.lastindex >= 2:
                    relationships.append({
                        "item1": match.group(1).strip(),
                        "item2": match.group(2).strip()
                    })
                break
        
        # Extract additional concepts from message
        if not concepts:
            # Extract noun phrases and technical terms
            tech_terms = [
                "upload", "line items", "line_items", "invoice", "card", "cards",
                "database", "api", "endpoint", "frontend", "backend", "ocr",
                "parsing", "storage", "display", "rendering", "component",
                "content", "contents", "data", "show", "appear", "visible"
            ]
            for term in tech_terms:
                if term in message_lower:
                    concepts.append(term)
        
        # Determine complexity based on indicators
        complexity_indicators = [
            "but", "however", "although", "even though",
            "not showing", "not displayed", "not appearing", "not working",
            "why", "problem", "issue", "error", "fail", "broken"
        ]
        if any(indicator in message_lower for indicator in complexity_indicators):
            complexity = "complex"
        
        # Check for multiple concepts (indicates complex question)
        if len(concepts) > 2 or relationships:
            complexity = "complex"
        
        return {
            "type": question_type,
            "complexity": complexity,
            "concepts": concepts,
            "relationships": relationships,
            "original_message": message
        }
    
    def _extract_concepts_and_flow(self, message: str, question_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract concepts and understand data flow relationships.
        
        Returns:
            Dict with concepts, relationships, flow_paths, implicit_files
        """
        concepts = question_analysis.get("concepts", [])
        relationships = question_analysis.get("relationships", [])
        flow_paths = []
        implicit_files = []
        
        # Common data flow patterns in Owlin
        flow_patterns = {
            "upload": ["backend/main.py", "backend/services/ocr_service.py", "backend/app/db.py"],
            "line items": ["backend/services/ocr_service.py", "backend/app/db.py", "backend/routes/invoices_submit.py"],
            "cards": ["frontend_clean/src/components/InvoiceCard.tsx", "frontend_clean/src/pages/Invoices.tsx"],
            "invoice": ["backend/app/db.py", "backend/routes/invoices_submit.py", "frontend_clean/src/lib/api.ts"],
            "ocr": ["backend/services/ocr_service.py", "backend/ocr/"],
            "database": ["backend/app/db.py", "migrations/"],
            "api": ["backend/routes/", "backend/main.py"],
            "frontend": ["frontend_clean/src/"]
        }
        
        # Extract flow relationships
        message_lower = message.lower()
        
        # Detect flow patterns like "upload → line items → cards"
        if "upload" in message_lower and ("line items" in message_lower or "line_items" in message_lower):
            flow_paths.append({
                "from": "upload",
                "to": "line items",
                "path": ["upload", "ocr", "parsing", "storage", "api", "frontend"]
            })
        
        if ("line items" in message_lower or "line_items" in message_lower) and "card" in message_lower:
            flow_paths.append({
                "from": "line items",
                "to": "cards",
                "path": ["database", "api", "frontend", "display"]
            })
        
        # Find implicit files based on concepts
        for concept in concepts:
            concept_lower = concept.lower()
            for pattern, files in flow_patterns.items():
                if pattern in concept_lower:
                    implicit_files.extend(files)
        
        # Remove duplicates
        implicit_files = list(set(implicit_files))
        
        # Extract additional concepts from relationships
        for rel in relationships:
            if "from" in rel:
                concepts.append(rel["from"])
            if "to" in rel:
                concepts.append(rel["to"])
            if "item1" in rel:
                concepts.append(rel["item1"])
            if "item2" in rel:
                concepts.append(rel["item2"])
        
        # Remove duplicates and empty strings
        concepts = list(set([c for c in concepts if c and len(c) > 1]))
        
        return {
            "concepts": concepts,
            "relationships": relationships,
            "flow_paths": flow_paths,
            "implicit_files": implicit_files
        }
    
    def chat(
        self,
        message: str,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        context: Optional[Dict[str, Any]] = None,
        context_size: Optional[int] = None,
        use_search_mode: bool = False,
        use_agent_mode: bool = False,
        progress_callback: Optional[Callable[[str, int, int], None]] = None,
        request_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Process a chat message and return response.
        
        Args:
            message: User's message
            conversation_history: Previous conversation messages
            context: Additional context (current page, etc.)
            
        Returns:
            Dict with response, code_references, and metadata
        """
        # Generate unique request ID and start timing
        if not request_id:
            request_id = str(uuid.uuid4())[:8]
        self._current_request_id = request_id
        start_time = time.time()
        
        # Initialize metrics tracking
        metrics = get_metrics()
        generic_detected = False
        forced_retry = False
        success = False
        
        # Classify question type and extract concepts
        question_analysis = self._classify_question(message)
        question_type = question_analysis.get('type', 'general')
        logger.info(
            f"[{request_id}] Question classified as: {question_type}, "
            f"complexity: {question_analysis.get('complexity')}"
        )
        
        # Detect if user is asking to read a file
        code_context = self._extract_code_requests(message)
        
        # Enhanced concept extraction for complex questions
        if question_analysis.get('complexity') == 'complex':
            enhanced_context = self._extract_concepts_and_flow(message, question_analysis)
            # Merge enhanced context into code_context
            code_context.setdefault("concepts", []).extend(enhanced_context.get("concepts", []))
            code_context.setdefault("relationships", []).extend(enhanced_context.get("relationships", []))
            code_context.setdefault("flow_paths", []).extend(enhanced_context.get("flow_paths", []))
            code_context.setdefault("implicit_files", []).extend(enhanced_context.get("implicit_files", []))
        
        # Determine which mode to use
        use_search_mode_active = False
        use_agent_mode_active = False
        
        if use_search_mode:
            # User explicitly requested Search mode (information gathering)
            logger.info(f"[{request_id}] Search mode requested by user")
            use_search_mode_active = True
        elif use_agent_mode:
            # User explicitly requested Agent mode (problem-solving)
            logger.info(f"[{request_id}] Agent mode requested by user")
            use_agent_mode_active = True
        elif question_analysis and question_analysis.get("type") == "debugging":
            # Check if user question is vague and needs exploration
            if self._needs_exploration(message):
                logger.info(f"[{request_id}] Vague debugging question detected - using agent mode")
                use_agent_mode_active = True
            else:
                # User provided specific location, use existing flow
                auto_files = self._get_related_files_for_debugging(message, code_context)
                for file_path in auto_files:
                    if file_path not in code_context.get("files", []):
                        code_context.setdefault("files", []).append(file_path)
                        logger.info(f"Auto-included related file for debugging: {file_path}")
        
        logger.info(f"Extracted code context: {len(code_context.get('files', []))} files, {len(code_context.get('search_queries', []))} search queries")
        
        # Re-check Ollama availability (in case it started after initialization)
        self.ollama_available = self._check_ollama_available()
        
        # Use provided context_size or default to 128k (max available for local models)
        effective_context_size = context_size if context_size is not None else 128000
        logger.info(f"Using context size: {effective_context_size} tokens")
        
        # Optimize context budget to fit within token limits
        optimized_code_context = self._optimize_context_budget(
            code_context,
            effective_context_size,
            conversation_history
        )
        
        # Use Search or Agent mode
        exploration_metadata = None
        if use_search_mode_active and self.ollama_available:
            logger.info(f"[{request_id}] Using Search mode workflow")
            try:
                # Search mode: comprehensive information gathering
                response_text, metadata = self._search_mode_with_metadata(
                    message,
                    question_analysis,
                    conversation_history,
                    progress_callback
                )
                exploration_metadata = metadata
            except Exception as e:
                import traceback
                error_trace = traceback.format_exc()
                logger.error(f"Search mode failed: {e}\n{error_trace}")
                use_search_mode_active = False
                exploration_metadata = None
        elif use_agent_mode_active and self.ollama_available:
            logger.info(f"[{request_id}] Using Agent mode workflow")
            try:
                # Agent mode: autonomous problem-solving
                response_text = self._agent_mode_conversation(
                    message,
                    progress_callback=progress_callback,
                    context_size=context_size
                )
                exploration_metadata = {
                    "mode": "agent",
                    "turns": "multiple"
                }
            except Exception as e:
                import traceback
                error_trace = traceback.format_exc()
                logger.error(f"Agent mode failed: {e}\n{error_trace}")
                
                # Fallback to Search mode when Agent cannot start (guard against 0-turn stalls)
                if "timeout" in str(e).lower() or "0 turns" in str(e).lower():
                    logger.warning("Agent mode timed out or produced 0 turns, falling back to Search mode")
                    try:
                        response_text, metadata = self._search_mode_with_metadata(
                            message,
                            question_analysis,
                            conversation_history,
                            progress_callback
                        )
                        exploration_metadata = metadata
                        use_agent_mode_active = False
                    except Exception as fallback_error:
                        logger.error(f"Search mode fallback also failed: {fallback_error}")
                        response_text = f"Both Agent and Search modes failed. Error: {str(e)}"
                        exploration_metadata = None
                else:
                    # Check if Ollama became unavailable during agent mode
                    self.ollama_available = self._check_ollama_available()
                    if not self.ollama_available:
                        logger.warning("Ollama became unavailable during agent mode, falling back to standard mode")
                    
                    use_agent_mode_active = False
                    response_text = f"Agent mode failed: {str(e)}"
                    exploration_metadata = None
        
        # Standard flow (or fallback from Search/Agent mode)
        if not use_search_mode_active and not use_agent_mode_active:
            # Build prompt with optimized code context
            prompt = self._build_prompt(message, conversation_history, optimized_code_context, context, question_analysis)
            
            # Get response from Ollama with cascading model fallback
            if self.ollama_available:
                response_text = self._call_ollama_with_fallback(
                    prompt,
                    conversation_history,
                    effective_context_size,
                    question_analysis,
                    optimized_code_context
                )
            else:
                logger.debug("Ollama not available, using fallback response")
                response_text = self._fallback_response(message, optimized_code_context, question_analysis)
        
        # Extract code references from response
        code_references = self._extract_code_references(response_text, code_context)
        logger.info(f"Generated response with {len(code_references)} code references")
        
        # Determine which model was used
        code_files_count = len(code_context.get("files", []))
        selected_model, _ = self._select_model_for_request(
            question_analysis.get("type", "general"),
            effective_context_size,
            code_files_count
        )
        model_used = selected_model or self.model
        
        # Calculate response time and mark success
        response_time = time.time() - start_time
        success = True
        
        # Check if response is generic (for metrics)
        if question_analysis.get("type") == "debugging":
            generic_detected = self._is_generic_response(response_text)
        
        # Log metrics
        try:
            metrics.log_request(
                request_id=request_id,
                message=message,
                question_type=question_type,
                context_size=effective_context_size,
                files_count=code_files_count,
                model_selected=model_used,
                response_time=response_time,
                success=success,
                generic_detected=generic_detected,
                forced_retry=forced_retry,
                code_references_count=len(code_references)
            )
        except Exception as e:
            logger.error(f"Failed to log metrics: {e}")
        
        logger.info(
            f"[{request_id}] Request completed in {response_time:.2f}s: "
            f"model={model_used}, files={code_files_count}, refs={len(code_references)}, "
            f"generic={generic_detected}"
        )
        
        return {
            "response": response_text,
            "code_references": code_references,
            "model_used": model_used,
            "ollama_available": self.ollama_available,
            "request_id": request_id,
            "exploration_mode": use_search_mode_active or use_agent_mode_active,
            "exploration_metadata": exploration_metadata
        }
    
    def _extract_code_requests(self, message: str) -> Dict[str, Any]:
        """Extract file reading requests from user message."""
        code_context = {
            "files": [],
            "search_queries": []
        }
        
        # Look for file references with improved patterns for full paths
        # Pattern 1: "show me backend/main.py" or "read frontend_clean/src/pages/Invoices.tsx"
        # Pattern 2: "backend/main.py file" or "main.py code"
        # Pattern 3: "in backend/main.py" or "from upload.py"
        # Pattern 4: Paths with slashes/backslashes (Windows compatible)
        file_patterns = [
            r"(?:show|read|open|see|view)\s+(?:me\s+)?(?:the\s+)?(?:file\s+)?([a-zA-Z0-9_/\\\.-]+(?:/[a-zA-Z0-9_/\\\.-]+)*\.(?:py|tsx?|ts|jsx?|js|json|yaml|yml|md))",
            r"([a-zA-Z0-9_/\\\.-]+(?:/[a-zA-Z0-9_/\\\.-]+)*\.(?:py|tsx?|ts|jsx?|js))\s+(?:file|code)",
            r"(?:in|from|at)\s+([a-zA-Z0-9_/\\\.-]+(?:/[a-zA-Z0-9_/\\\.-]+)*\.(?:py|tsx?|ts|jsx?|js))",
            # Match paths that look like file paths (contain slashes and extension)
            r"([a-zA-Z0-9_]+(?:[/\\][a-zA-Z0-9_/\\\.-]+)+\.(?:py|tsx?|ts|jsx?|js|json|yaml|yml|md))"
        ]
        
        found_paths = set()  # Track found paths to avoid duplicates
        
        for pattern in file_patterns:
            matches = re.finditer(pattern, message, re.IGNORECASE)
            for match in matches:
                file_path = match.group(1).strip()
                # Normalize path separators (Windows backslash to forward slash)
                file_path = file_path.replace("\\", "/")
                
                if file_path in found_paths:
                    continue
                
                # Try direct path resolution first (for full paths like "backend/main.py")
                if "/" in file_path or "\\" in file_path:
                    # Try reading the file directly
                    file_data = self.code_reader.read_file(file_path)
                    if file_data.get("success"):
                        code_context["files"].append(file_path)
                        found_paths.add(file_path)
                        logger.info(f"Found file via direct path: {file_path}")
                        continue
                
                # If direct path didn't work, try smart resolution
                resolved = self.code_reader.resolve_file_path(file_path)
                if resolved:
                    code_context["files"].append(resolved)
                    found_paths.add(resolved)
                    logger.info(f"Found file via resolution: {file_path} -> {resolved}")
                    continue
                
                # Fallback: search by filename only
                filename = file_path.split("/")[-1]
                found_files = self.code_reader.find_files_by_name(filename, max_results=1)
                if found_files and found_files[0] not in found_paths:
                    code_context["files"].append(found_files[0])
                    found_paths.add(found_files[0])
                    logger.info(f"Found file via name search: {filename} -> {found_files[0]}")
        
        # Look for search queries (e.g., "find upload function", "search for database connection", "read the upload code")
        search_patterns = [
            r"(?:find|search|look\s+for|read|show)\s+(?:me\s+)?(?:the\s+)?(.+?)(?:\s+function|\s+code|\s+in\s+code)?$",
            r"where\s+(?:is|does)\s+(.+?)(?:\s+defined|\s+located)?",
            r"(?:the\s+)?(.+?)\s+(?:code|function|connection|endpoint|route)"  # Catch "upload code", "database connection", etc.
        ]
        
        for pattern in search_patterns:
            matches = re.finditer(pattern, message, re.IGNORECASE)
            for match in matches:
                query = match.group(1).strip()
                if len(query) > 3:  # Ignore very short queries
                    code_context["search_queries"].append(query)
        
        return code_context
    
    def _optimize_context_budget(
        self,
        code_context: Dict[str, Any],
        effective_context_size: int,
        conversation_history: Optional[List[Dict[str, str]]],
        system_prompt_estimate: int = 2000
    ) -> Dict[str, Any]:
        """
        Optimize context budget to fit within token limits.
        
        Budget allocation:
        - 20% for system prompt + conversation history
        - 50% for code context
        - 30% reserved for response
        
        Args:
            code_context: Code files and context
            effective_context_size: Total context window size
            conversation_history: Chat history
            system_prompt_estimate: Estimated tokens for system prompt
            
        Returns:
            Optimized code_context with pruned files if necessary
        """
        # Calculate token budgets - maximize code context for local processing
        reserved_for_response = int(effective_context_size * 0.10)  # Reduced to 10% (was 30%)
        reserved_for_system = int(effective_context_size * 0.05)   # Reduced to 5% (was 20%)
        available_for_code = effective_context_size - reserved_for_response - reserved_for_system  # Now 85% for code!
        
        # Estimate conversation history tokens (rough: 4 chars per token)
        conv_tokens = 0
        if conversation_history:
            conv_chars = sum(len(msg.get("content", "")) for msg in conversation_history)  # Use all history
            conv_tokens = conv_chars // 4
        
        # Adjust available budget
        available_for_code = max(1000, available_for_code - conv_tokens)
        
        logger.info(
            f"Context budget: total={effective_context_size}, "
            f"code={available_for_code}, system={reserved_for_system}, "
            f"response={reserved_for_response}"
        )
        
        # Estimate tokens for code files (rough: 4 chars per token)
        files = code_context.get("files", [])
        file_priorities = []
        
        for idx, file_path in enumerate(files):
            # Earlier files in the list are more relevant (based on pattern matching)
            priority = len(files) - idx
            
            # Estimate file size
            file_data = self.code_reader.read_file(file_path, max_lines=1)
            total_lines = file_data.get("total_lines", 100)
            estimated_tokens = total_lines * 20  # Rough estimate: 20 tokens per line
            
            file_priorities.append({
                "path": file_path,
                "priority": priority,
                "estimated_tokens": estimated_tokens,
                "max_lines": min(estimated_tokens // 20, total_lines)  # Use full file if budget allows
            })
        
        # Sort by priority
        file_priorities.sort(key=lambda x: x["priority"], reverse=True)
        
        # Allocate budget to files
        optimized_files = []
        tokens_used = 0
        
        for file_info in file_priorities:
            if tokens_used >= available_for_code:
                logger.info(f"Context budget exhausted, skipping {file_info['path']}")
                break
            
            remaining_budget = available_for_code - tokens_used
            
            if file_info["estimated_tokens"] <= remaining_budget:
                # File fits completely
                optimized_files.append({
                    "path": file_info["path"],
                    "max_lines": file_info["max_lines"],
                    "truncated": False
                })
                tokens_used += file_info["estimated_tokens"]
            elif remaining_budget > 500:  # At least 500 tokens left
                # Include partial file
                lines_to_include = int((remaining_budget / 20) * 0.8)  # Use 80% of remaining
                optimized_files.append({
                    "path": file_info["path"],
                    "max_lines": max(50, lines_to_include),  # Removed 500 cap - use full budget
                    "truncated": True
                })
                tokens_used += remaining_budget
                logger.info(f"Truncating {file_info['path']} to {lines_to_include} lines")
            else:
                logger.info(f"Insufficient budget for {file_info['path']}, skipping")
        
        # Update code_context with optimized files
        optimized_context = code_context.copy()
        optimized_context["files"] = [f["path"] for f in optimized_files]
        optimized_context["file_specs"] = optimized_files  # Include line limits
        
        logger.info(f"Optimized context: {len(optimized_files)}/{len(files)} files, ~{tokens_used} tokens")
        
        return optimized_context
    
    def _get_framework_context(self, code_context: Dict[str, Any], message: str) -> str:
        """
        Detect framework from code context and generate framework-specific prompt context.
        
        Args:
            code_context: Code context with files
            message: User message (to detect if framework-relevant)
            
        Returns:
            Framework context string to add to system prompt
        """
        # Only detect framework if message is about code/API/endpoints
        framework_keywords = ["endpoint", "api", "route", "upload", "post", "get", "decorator", 
                              "fastapi", "flask", "async", "def ", "function"]
        if not any(keyword in message.lower() for keyword in framework_keywords):
            return ""
        
        # Try to detect framework from files in context
        framework_result = None
        files_to_check = code_context.get("files", [])
        
        # Also check common backend files if no files in context
        if not files_to_check:
            common_files = ["backend/main.py", "backend/routes/upload.py", "backend/routes/documents.py"]
            files_to_check = common_files
        
        for file_path in files_to_check[:3]:  # Check up to 3 files
            try:
                result = self.architecture_analyzer.detect_framework(file_path)
                if result.get("confidence", 0) > 0.7:
                    framework_result = result
                    break
            except:
                continue
        
        if not framework_result or framework_result.get("framework") == "unknown":
            return ""
        
        framework = framework_result["framework"]
        syntax_rules = framework_result.get("syntax_rules", {})
        confidence = framework_result.get("confidence", 0)
        
        # Build framework context
        context_parts = [
            f"\n\n=== FRAMEWORK DETECTION ===",
            f"This codebase uses {framework.upper()} (detected with {confidence:.0%} confidence).",
            f"You MUST use {framework.upper()} syntax in all code suggestions."
        ]
        
        # Add framework-specific rules
        if syntax_rules:
            allowed_decorators = syntax_rules.get("allowed_decorators", [])
            forbidden_decorators = syntax_rules.get("forbidden_decorators", [])
            forbidden_patterns = syntax_rules.get("forbidden_patterns", [])
            async_required = syntax_rules.get("async_required", False)
            request_access = syntax_rules.get("request_access", "")
            
            if allowed_decorators:
                context_parts.append(f"\nALLOWED DECORATORS: {', '.join(allowed_decorators[:3])}")
            if forbidden_decorators:
                context_parts.append(f"FORBIDDEN DECORATORS: {', '.join(forbidden_decorators)} - DO NOT USE THESE")
            if forbidden_patterns:
                context_parts.append(f"FORBIDDEN PATTERNS: {', '.join(forbidden_patterns)} - DO NOT USE THESE")
            if async_required:
                context_parts.append(f"ASYNC REQUIRED: Use 'async def' for endpoint functions")
            if request_access:
                context_parts.append(f"REQUEST ACCESS: {request_access}")
            
            example = syntax_rules.get("example", "")
            if example:
                context_parts.append(f"\nEXAMPLE SYNTAX:\n{example}")
        
        context_parts.append("=== END FRAMEWORK DETECTION ===\n")
        
        return "\n".join(context_parts)
    
    def _build_prompt(
        self,
        message: str,
        conversation_history: Optional[List[Dict[str, str]]],
        code_context: Dict[str, Any],
        context: Optional[Dict[str, Any]],
        question_analysis: Optional[Dict[str, Any]] = None
    ) -> str:
        """Build the prompt for Ollama with code context."""
        
        # Detect framework and get framework context
        framework_context = self._get_framework_context(code_context, message)
        
        # System prompt - enhanced for complex questions
        if question_analysis and question_analysis.get("complexity") == "complex":
            question_type = question_analysis.get("type", "general")
            if question_type == "debugging":
                system_prompt = """You are an expert code debugger analyzing Owlin's codebase.""" + framework_context + """

CRITICAL RULES:
1. EXPLAIN PROBLEMS IN TEXT - write clear explanations, don't just dump code
2. USE SMALL CODE SNIPPETS - 3-5 lines max per snippet, only to illustrate points
3. ALWAYS INCLUDE file:line references with each snippet
4. ANALYZE THE PROVIDED CODE - reference specific line numbers
5. TRACE DATA FLOW - show how data moves through files (in text, with file:line references)
6. IDENTIFY ROOT CAUSE - explain what's breaking and why (in text)
7. PROVIDE SPECIFIC FIX - exact code changes with file paths (small snippets only)
8. NEVER dump entire files or large code blocks
9. NEVER give generic troubleshooting advice

SYSTEM CONTEXT:
- Owlin is a fully offline invoice and delivery-verification platform
- Data flow: Upload → OCR → Parse → Normalize → Match → Detect Issues → Forecast → Dashboard
- Frontend: React 18 + Vite + TanStack Query (port 5176)
- Backend: FastAPI Python 3.12+ (port 8000)
- Database: SQLite WAL mode (data/owlin.db)
- Key tables: documents, invoices, invoice_line_items, issues

MANDATORY RESPONSE FORMAT:
Your response MUST follow this exact format. Do not skip steps.

**Step 1: Files Analyzed**
List the specific files you analyzed with line ranges:
- `file1.ts` (lines 217-263)
- `file2.py` (lines 630-723)
- etc.

**Step 2: Data Flow Trace**
Trace the exact path through the code showing how data flows:
- Start: [file.py:494] Upload endpoint returns {status: 'processing'}
- Step 1: [file.ts:224] Frontend polls /api/upload/status
- Step 2: [file.py:630] Status endpoint returns {items: [...]}
- Step 3: [file.ts:240] Normalize function merges response
- End: [file.tsx:12] Component displays metadata.lineItems

**Step 3: Code Analysis**
EXPLAIN THE PROBLEM IN TEXT. Use small code snippets (3-5 lines max) to illustrate your points. DO NOT dump entire files.

For each relevant code section, explain the problem in text, then show a small snippet:
The issue is that the condition doesn't handle the 'ready' status properly. Here's the problematic code:

```typescript
// upload.ts:236-238
const hasData = statusData.parsed || statusData.invoice || (statusData.status && statusData.status !== 'processing')
const hasItems = Array.isArray(statusData.items) && statusData.items.length > 0
```

**Explanation:** This condition checks if data exists, but if status is 'ready' and parsed is null, hasData will be false even though the invoice is processed. The logic assumes parsed or invoice must exist, but doesn't account for the 'ready' state where data might be in a different field.

**Step 4: Root Cause**
Identify the exact problem with file:line references:
- Issue: In `upload.ts:236`, the condition doesn't account for status='ready' when parsed is null
- Impact: Polling stops before data is available
- Location: `frontend_clean/src/lib/upload.ts` line 236

**Step 5: Fix**
Provide exact code changes with before/after:

BEFORE (upload.ts:236):
```typescript
const hasData = statusData.parsed || statusData.invoice || (statusData.status && statusData.status !== 'processing')
```

AFTER (upload.ts:236):
```typescript
const hasData = statusData.parsed || statusData.invoice || statusData.status === 'ready' || statusData.status === 'scanned' || (statusData.status && statusData.status !== 'processing')
```

CHAIN-OF-THOUGHT REQUIREMENT:
Show your reasoning step-by-step. For each file you analyze, explain:
- What this code does
- How it relates to the problem
- What you found (or didn't find) that's relevant

RESPONSE VALIDATION CHECKLIST:
Before submitting your response, verify:
- Did I quote actual code lines? (If no, add them)
- Did I provide file paths and line numbers? (If no, add them)
- Did I trace the data flow through actual code? (If no, do it)
- Am I giving generic advice? (If yes, replace with code-specific analysis)

CODE FLOW DIAGRAM:
For complex questions, include a text-based flow diagram showing the actual code path:
```
Upload → [main.py:494] → Status Check → [upload.ts:224] → Normalize → [upload.ts:59] → Display → [InvoiceDetailPanel.tsx:12]
```

CRITICAL ANTI-HALLUCINATION RULES:
- ONLY reference code that is actually provided in the context above
- If you don't see specific code, say "I need to see the code for [component]" - DO NOT make up code
- Always cite file paths and line numbers: `file.py:123`
- If you're unsure, ask for more code rather than guessing
- Base your analysis ONLY on the code provided, not assumptions or general knowledge
- When suggesting fixes, quote the exact code from the files provided
- If a file isn't in the context, explicitly state you need to see it
- DO NOT give generic responses like "check your configuration" or "ensure dependencies are installed"
- DO NOT provide troubleshooting steps without analyzing the actual code first

EXAMPLES:

BAD RESPONSE #1 (DO NOT DO THIS):
"Here are potential causes:
1. File format issues
2. File size limitations
3. File corruption
..."
This is generic and doesn't analyze the actual code.

BAD RESPONSE #2 (DO NOT DO THIS):
"Based on your description, it seems like there may be an issue with the OCR processing..."
This gives generic advice without looking at code.

BAD RESPONSE #3 (DO NOT DO THIS):
"Check if data is being extracted correctly. Verify data is stored in the database..."
This provides troubleshooting steps without analyzing actual code.

GOOD RESPONSE (DO THIS - COMPLETE EXAMPLE):
**Step 1: Files Analyzed**
Analyzing the following files:
- `frontend_clean/src/lib/upload.ts` (lines 217-263)
- `backend/main.py` (lines 630-723)
- `frontend_clean/src/components/InvoiceDetailPanel.tsx` (lines 1-50)

**Step 2: Data Flow Trace**
Tracing the data flow through the code:
- Start: [main.py:494] Upload endpoint receives file, returns {doc_id, status: 'processing'}
- Step 1: [upload.ts:224] Frontend calls `/api/upload/status?doc_id=${docId}`
- Step 2: [main.py:630] Status endpoint queries database: `SELECT ... FROM invoices WHERE doc_id = ?`
- Step 3: [main.py:691] Status endpoint returns {parsed: {...}, items: [...], invoice: {...}}
- Step 4: [upload.ts:240] Merged response combines statusData.parsed, statusData.invoice, and statusData.items
- Step 5: [upload.ts:250] normalizeUploadResponse extracts lineItems from merged response
- Step 6: [InvoiceDetailPanel.tsx:12] Component reads `metadata.lineItems || []`
- End: [InvoiceDetailPanel.tsx:15] Component renders lineItems array

**Step 3: Code Analysis**

File: frontend_clean/src/lib/upload.ts (lines 236-238)
```
 236: const hasData = statusData.parsed || statusData.invoice || (statusData.status && statusData.status !== 'processing')
 237: const hasItems = Array.isArray(statusData.items) && statusData.items.length > 0
 238: if (hasData || hasItems) {
```
Analysis: The condition on line 236 checks if parsed or invoice exists, OR if status is not 'processing'. However, if the status is 'ready' but parsed is null (which can happen if invoice was created but parsed data wasn't stored), hasData will be false. This causes polling to stop before the invoice data is available.

File: frontend_clean/src/lib/upload.ts (lines 240-248)
```
 240: const mergedResponse = {
 241:   ...statusData,
 242:   ...statusData.parsed,
 243:   ...statusData.invoice,
 244:   line_items: statusData.items || statusData.invoice?.items || [],
 245:   items: statusData.items || statusData.invoice?.items || [],
 246:   raw: statusData,
 247: }
 248: return normalizeUploadResponse(mergedResponse, undefined, Date.now())
```
Analysis: The merge looks correct - it combines all data sources. However, if statusData.items is an empty array (not null/undefined), line_items will be [] even if invoice.items has data.

**Step 4: Root Cause**
- Primary Issue: In `upload.ts:236`, the polling condition doesn't account for status='ready' when parsed is null
- Secondary Issue: In `upload.ts:244`, if statusData.items is [] (empty array), it takes precedence over statusData.invoice?.items
- Impact: Frontend stops polling before invoice data is available, or gets empty line_items even when invoice.items has data
- Location: `frontend_clean/src/lib/upload.ts` lines 236 and 244

**Step 5: Fix**

Fix #1: Update polling condition (upload.ts:236)
BEFORE:
```typescript
const hasData = statusData.parsed || statusData.invoice || (statusData.status && statusData.status !== 'processing')
```

AFTER:
```typescript
const hasData = statusData.parsed || statusData.invoice || statusData.status === 'ready' || statusData.status === 'scanned' || (statusData.status && statusData.status !== 'processing')
```

Fix #2: Improve items merge logic (upload.ts:244)
BEFORE:
```typescript
line_items: statusData.items || statusData.invoice?.items || [],
```

AFTER:
```typescript
line_items: (Array.isArray(statusData.items) && statusData.items.length > 0) ? statusData.items : (statusData.invoice?.items || []),
```

This analyzes actual code with specific line numbers and provides exact fixes.

Be thorough, systematic, and provide actionable code-level solutions."""
            elif question_type == "flow":
                system_prompt = """You are a code flow analyzer for Owlin. Explain how data flows through the system:""" + framework_context + """

SYSTEM CONTEXT:
- Owlin processes invoices through: Upload → OCR → Parse → Normalize → Match → Detect Issues → Forecast → Dashboard
- Frontend (React) communicates with Backend (FastAPI) via REST API
- Data is stored in SQLite database (data/owlin.db)

FLOW ANALYSIS:
1. Show the complete path from start to end
2. Identify each component (file, function, API endpoint)
3. Show code at each stage with file paths
4. Highlight data transformations and structure changes
5. Show how frontend consumes backend APIs

Use clear step-by-step explanations with code references."""
            else:
                system_prompt = """You are a helpful code assistant for Owlin, an offline invoice processing platform.""" + framework_context + """

SYSTEM CONTEXT:
- Owlin processes invoices: Upload → OCR → Parse → Normalize → Match → Detect Issues → Forecast → Dashboard
- Frontend: React 18 + Vite (frontend_clean/src/)
- Backend: FastAPI Python (backend/)
- Database: SQLite (data/owlin.db)

Provide detailed, structured answers with code examples. Reference specific files and line numbers."""
        else:
            system_prompt = """You are a debugging assistant for Owlin, an offline invoice processing platform for hospitality venues.""" + framework_context + """

SYSTEM OVERVIEW:
- Owlin processes invoices through: Upload → OCR → Parse → Normalize → Match → Detect Issues → Forecast → Dashboard
- Frontend: React 18 + Vite + TanStack Query (frontend_clean/src/)
- Backend: FastAPI Python 3.12+ (backend/)
- Database: SQLite WAL mode (data/owlin.db)
- Key components: documents, invoices, invoice_line_items tables

YOUR ROLE:
- Analyze code systematically to find bugs and issues
- Trace data flow through the system
- Identify where problems occur in the pipeline
- Provide specific code fixes with file paths and line numbers
- Check for mismatches between frontend expectations and backend responses
- Verify database operations and API endpoints

When analyzing code:
1. Read the actual code files provided
2. Identify what's wrong (missing data, incorrect logic, API mismatches)
3. Trace where the problem occurs in the data flow
4. Suggest specific fixes with code examples

CRITICAL ANTI-HALLUCINATION RULES:
- ONLY reference code that is actually provided in the context above
- If you don't see specific code, say "I need to see the code for [component]" - DO NOT make up code
- Always cite file paths and line numbers: `file.py:123`
- If you're unsure, ask for more code rather than guessing
- Base your analysis ONLY on the code provided, not assumptions
- When suggesting fixes, quote the exact code from the files provided

Be thorough, systematic, and provide actionable solutions."""

        # Add code context if files were requested
        code_snippets = []
        if code_context["files"]:
            # Check if file_specs are provided (from context budgeting optimization)
            file_specs = code_context.get("file_specs", [])
            file_specs_dict = {spec["path"]: spec for spec in file_specs if isinstance(spec, dict)}
            
            for file_path in code_context["files"]:  # No limit (local processing)
                # Use max_lines from file_specs if available, otherwise use high default for local processing
                max_lines = 5000  # Increased default for local processing
                if file_path in file_specs_dict:
                    max_lines = file_specs_dict[file_path].get("max_lines", 5000)  # Increased default
                
                file_data = self.code_reader.read_file(file_path, max_lines=max_lines)
                if file_data.get("success"):
                    content = file_data['content']
                    lines_read = file_data.get('lines_read', 0)
                    total_lines = file_data.get('total_lines', 0)
                    start_line = 1  # Files are read from the beginning
                    end_line = lines_read
                    
                    # Add line numbers to code content
                    # Handle edge cases: trailing newlines, Windows line endings, empty files
                    lines = content.split('\n')
                    # Remove trailing empty line if file ends with newline
                    if lines and lines[-1] == '' and content.endswith('\n'):
                        lines = lines[:-1]
                    
                    numbered_lines = []
                    for i, line in enumerate(lines, start=start_line):
                        # Remove Windows line endings if present
                        line = line.rstrip('\r')
                        numbered_lines.append(f"{i:4}: {line}")
                    numbered_content = '\n'.join(numbered_lines)
                    
                    # Format file header with line range
                    truncation_note = ""
                    if file_data.get("truncated"):
                        truncation_note = f" (showing lines {start_line}-{end_line} of {total_lines})"
                    else:
                        truncation_note = f" (lines {start_line}-{end_line})"
                    
                    code_snippets.append(f"File: {file_path}{truncation_note}\n```\n{numbered_content}\n```")
                else:
                    logger.warning(f"Failed to read file {file_path}: {file_data.get('error')}")
        
        # Add search results if queries were made
        if code_context["search_queries"]:
            for query in code_context["search_queries"]:  # No limit (local processing)
                results = self.code_reader.search_codebase(query, max_results=50)  # Increased for local processing
                if results:
                    snippets = "\n".join([f"{r['file_path']}:{r['line']} - {r['content']}" for r in results])
                    code_snippets.append(f"Search results for '{query}':\n{snippets}")
        
        # Log if no code was provided for debugging questions
        if question_analysis and question_analysis.get("type") == "debugging" and not code_snippets:
            logger.warning("DEBUGGING QUESTION BUT NO CODE CONTEXT PROVIDED - This will lead to generic responses!")
            logger.warning(f"Code context had {len(code_context.get('files', []))} files but none were readable")
        
        # Build full prompt
        prompt_parts = [system_prompt]
        
        if code_snippets:
            prompt_parts.append("\n=== CODE CONTEXT - YOU MUST ANALYZE THIS CODE ===")
            prompt_parts.append("The following code files are provided with line numbers. You MUST analyze them to answer the question.")
            prompt_parts.append("DO NOT give generic responses. You MUST reference specific code lines and file paths.")
            prompt_parts.append("The code includes line numbers in the format 'LINE: code'. Use these line numbers in your analysis.")
            prompt_parts.extend(code_snippets)
            prompt_parts.append("\n=== END CODE CONTEXT ===")
            if question_analysis and question_analysis.get("type") == "debugging":
                prompt_parts.append("\n=== MANDATORY ANALYSIS REQUIREMENTS ===")
                prompt_parts.append("You MUST start your response with 'Analyzing the following files:' and list all files you examined.")
                prompt_parts.append("You MUST quote at least 3 specific code lines from different files with their line numbers.")
                prompt_parts.append("You MUST show the data transformation at each step of the flow.")
                prompt_parts.append("You MUST follow the 5-step response format: Files Analyzed → Data Flow Trace → Code Analysis → Root Cause → Fix")
                prompt_parts.append("DO NOT give generic troubleshooting advice. Analyze the actual code provided above.")
                prompt_parts.append("DO NOT skip any steps in the mandatory response format.")
        else:
            if question_analysis and question_analysis.get("type") == "debugging":
                prompt_parts.append("\n=== WARNING: No code context provided ===")
                prompt_parts.append("This is a debugging question but no code files were found. Please request specific files to analyze.")
        
        if context and context.get("error_logs"):
            log_data = self.code_reader.read_error_logs(max_lines=30)
            if log_data.get("success"):
                prompt_parts.append(f"\n=== Recent Error Logs ===\n{log_data['content']}")
                
                # Add code context from files referenced in errors
                if log_data.get("file_references"):
                    error_code_snippets = []
                    for ref in log_data["file_references"]:  # No limit (local processing)
                        file_data = self.code_reader.read_file_with_context(
                            ref["file"], 
                            ref["line"], 
                            context_lines=10
                        )
                        if file_data.get("success"):
                            error_code_snippets.append(
                                f"File: {ref['file']}:{ref['line']} (from error log)\n"
                                f"Error context: {ref.get('error_line', 'N/A')}\n"
                                f"```\n{file_data['content']}\n```"
                            )
                    if error_code_snippets:
                        prompt_parts.append("\n=== Code from Error Locations ===")
                        prompt_parts.extend(error_code_snippets)
        
        # Add document/invoice context if user is asking about documents
        if context and ("document" in message.lower() or "invoice" in message.lower() or "fail" in message.lower() or "error" in message.lower()):
            doc_context = self._get_document_context(context)
            if doc_context:
                prompt_parts.append(f"\n=== Document Context ===\n{doc_context}")
        
        # Build comprehensive context for complex questions
        if question_analysis and question_analysis.get("complexity") == "complex":
            comprehensive_context = self._build_comprehensive_context(
                question_analysis.get("type", "general"),
                code_context.get("concepts", []),
                code_context
            )
            if comprehensive_context:
                prompt_parts.append("\n=== Comprehensive Context ===")
                for section, content in comprehensive_context.items():
                    if content:
                        prompt_parts.append(f"\n--- {section} ---\n{content}")
        
        prompt_parts.append(f"\n=== User Question ===\n{message}")
        
        return "\n".join(prompt_parts)
    
    def _build_comprehensive_context(self, question_type: str, concepts: List[str], code_context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Build comprehensive context for complex questions.
        
        Returns:
            Dict with files, code_snippets, api_routes, db_operations, errors, related_code
        """
        context = {
            "files": [],
            "code_snippets": [],
            "api_routes": [],
            "db_operations": [],
            "related_code": []
        }
        
        # Gather files from implicit_files if available
        implicit_files = code_context.get("implicit_files", [])
        for file_path in implicit_files:  # No limit (local processing)
            file_data = self.code_reader.read_file(file_path, max_lines=5000)  # Increased from 100
            if file_data.get("success"):
                context["files"].append(file_path)
                context["code_snippets"].append(f"**{file_path}**\n```\n{file_data['content']}\n```")
        
        # For flow questions, trace the data flow
        if question_type == "flow" or question_type == "debugging":
            flow_paths = code_context.get("flow_paths", [])
            for flow in flow_paths:
                if "from" in flow and "to" in flow:
                    traced_flow = self.code_reader.trace_data_flow(flow["from"], flow["to"])
                    if traced_flow:
                        flow_text = "Data Flow:\n"
                        for step in traced_flow:
                            flow_text += f"  {step['step']}: {step['concept']} - {step['description']}\n"
                            flow_text += f"    Files: {', '.join(step.get('files', []))}\n"
                        context["related_code"].append(flow_text)
        
        # For debugging questions, check database operations
        if question_type == "debugging":
            for concept in concepts:
                if "line items" in concept.lower() or "invoice" in concept.lower():
                    db_analysis = self.code_reader.analyze_database_flow("invoice_line_items", "select")
                    if db_analysis.get("operations"):
                        context["db_operations"].append(f"Database operations for invoice_line_items:\n")
                        for op in db_analysis["operations"]:  # No limit (local processing)
                            context["db_operations"].append(f"  {op['file']}:{op['line']} - {op['code']}")
        
        # Find API routes related to concepts
        for concept in concepts:
            if "api" in concept.lower() or "endpoint" in concept.lower():
                # Search for API routes
                api_results = self.code_reader.search_codebase("api", file_pattern="*.py", max_results=50)  # Increased for local
                if api_results:
                    context["api_routes"].extend([f"{r['file_path']}:{r['line']}" for r in api_results])
        
        return context
    
    def _get_document_context(self, context: Optional[Dict[str, Any]]) -> Optional[str]:
        """Get context about documents/invoices from database."""
        try:
            import sqlite3
            import os
            
            db_path = "data/owlin.db"
            if not os.path.exists(db_path):
                return None
            
            con = sqlite3.connect(db_path, check_same_thread=False)
            cur = con.cursor()
            
            # Get recent documents with low confidence or errors
            cur.execute("""
                SELECT d.id, d.filename, d.status, d.ocr_error, d.ocr_confidence,
                       i.supplier, i.date, i.value, i.confidence
                FROM documents d
                LEFT JOIN invoices i ON i.doc_id = d.id
                WHERE d.status = 'error' OR d.ocr_confidence < 0.5 OR i.confidence < 0.5
                ORDER BY d.uploaded_at DESC
                LIMIT 5
            """)
            
            rows = cur.fetchall()
            con.close()
            
            if not rows:
                return None
            
            context_lines = ["Recent documents with issues:"]
            for row in rows:
                doc_id, filename, status, ocr_error, ocr_conf, supplier, date, value, inv_conf = row
                context_lines.append(f"- {filename} (ID: {doc_id})")
                context_lines.append(f"  Status: {status}, OCR Confidence: {ocr_conf or 0}")
                if ocr_error:
                    context_lines.append(f"  Error: {ocr_error}")
                if supplier:
                    context_lines.append(f"  Supplier: {supplier}, Invoice Confidence: {inv_conf or 0}")
            
            return "\n".join(context_lines)
        except Exception as e:
            logger.warning(f"Failed to get document context: {e}")
            return None
    
    def _select_model_for_request(
        self,
        question_type: str,
        context_size: int,
        code_files_count: int
    ) -> Tuple[Optional[str], int]:
        """
        Select best model for this request using the model registry.
        
        Args:
            question_type: Type of question ("debugging", "code_flow", "general")
            context_size: Requested context size
            code_files_count: Number of code files to include
            
        Returns:
            Tuple of (selected_model_name, effective_context_size)
        """
        selected_model, effective_context = self.model_registry.select_best_model(
            question_type=question_type,
            context_size=context_size,
            code_files_count=code_files_count,
            preferred_models=self.models
        )
        
        # Fallback to primary model if selection fails
        if not selected_model:
            logger.warning("Model selection failed, using primary model")
            selected_model = self.model
            # Use full requested context size (local processing)
            effective_context = context_size
        
        return selected_model, effective_context
    
    def _call_ollama_with_fallback(
        self,
        prompt: str,
        conversation_history: Optional[List[Dict[str, str]]],
        context_size: int,
        question_analysis: Optional[Dict[str, Any]],
        code_context: Dict[str, Any]
    ) -> str:
        """
        Call Ollama with cascading model fallback chain.
        
        Tries models in priority order:
        1. Primary model (best for request)
        2. Secondary models (if primary fails)
        3. Enhanced fallback (if all models fail)
        """
        # Try each available model in order
        for attempt, model_name in enumerate(self.available_models):
            try:
                logger.info(f"Attempt {attempt + 1}/{len(self.available_models)}: trying model {model_name}")
                
                # Temporarily set the model
                original_model = self.model
                self.model = model_name
                
                # Call Ollama
                response_text = self._call_ollama(
                    prompt,
                    conversation_history,
                    context_size,
                    question_analysis,
                    code_context
                )
                
                # Restore original model
                self.model = original_model
                
                # Validate response - if generic, try next model
                if self._is_generic_response(response_text):
                    logger.warning(f"Model {model_name} returned generic response, trying next model")
                    continue
                
                logger.info(f"Model {model_name} returned valid response")
                return response_text
                
            except Exception as e:
                logger.warning(f"Model {model_name} failed: {e}")
                # Restore original model
                self.model = original_model
                
                # If this was the last model, raise the exception
                if attempt == len(self.available_models) - 1:
                    logger.error("All models failed, falling back to code-based response")
                    return self._generate_code_based_fallback(code_context, question_analysis)
                
                # Otherwise, try next model
                continue
        
        # If no models available, use enhanced fallback
        logger.warning(f"No models available (checked {len(self.available_models)} models), using code-based fallback")
        logger.info(f"Ollama available: {self.ollama_available}, Available models: {self.available_models}, Primary model: {self.model}")
        return self._generate_code_based_fallback(code_context, question_analysis)
    
    def _call_ollama(self, prompt: str, conversation_history: Optional[List[Dict[str, str]]], context_size: int = 100000, question_analysis: Optional[Dict[str, Any]] = None, code_context: Optional[Dict[str, Any]] = None) -> str:
        """Call Ollama API for chat response with intelligent model selection."""
        
        # Select best model for this request
        question_type = question_analysis.get("type", "general") if question_analysis else "general"
        code_files_count = len(code_context.get("files", [])) if code_context else 0
        
        selected_model, effective_context = self._select_model_for_request(
            question_type,
            context_size,
            code_files_count
        )
        
        logger.info(
            f"Selected model: {selected_model} with {effective_context}k context "
            f"for {question_type} question with {code_files_count} files"
        )
        
        # Build messages array
        messages = []
        
        # Add system message
        messages.append({
            "role": "system",
            "content": """You are a debugging assistant for Owlin, an offline invoice processing platform.

SYSTEM: Upload → OCR → Parse → Normalize → Match → Detect Issues → Forecast → Dashboard
Analyze code systematically, trace data flow, find bugs, and provide specific fixes."""
        })
        
        # Add conversation history
        if conversation_history:
            messages.extend(conversation_history)  # Use all history (local processing)
        
        # Add current prompt
        messages.append({
            "role": "user",
            "content": prompt
        })
        
        # Determine temperature based on question type (lower for debugging = less hallucination)
        is_debugging = False
        if question_analysis and question_analysis.get("type") == "debugging":
            is_debugging = True
        elif conversation_history:
            # Check last few messages for debugging keywords
            recent_messages = ' '.join([msg.get('content', '') for msg in conversation_history[-3:]])
            debugging_keywords = ['debug', 'issue', 'problem', 'broken', 'not working', 'not showing', 'error', 'fix']
            is_debugging = any(keyword in recent_messages.lower() for keyword in debugging_keywords)
        
        # Use even lower temperature for debugging questions (0.3) for maximum determinism
        temperature = 0.3 if is_debugging else 0.5
        
        # Calculate timeout based on context size (2.5s per 1k tokens, minimum 60s, maximum 300s)
        timeout = min(300, max(60, int(effective_context / 1000) * 2.5))
        
        response = requests.post(
            f"{self.ollama_url}/api/chat",
            json={
                "model": selected_model,  # Use selected model
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": -1,  # -1 = unlimited output (local processing)
                    "num_ctx": effective_context,  # Use effective context from model selection
                    "top_p": 0.9,
                    "repeat_penalty": 1.1,
                    "top_k": 40
                }
            },
            timeout=timeout
        )
        
        if response.status_code != 200:
            raise Exception(f"Ollama API returned status {response.status_code}")
        
        result = response.json()
        response_text = result.get("message", {}).get("content", "I'm sorry, I couldn't generate a response.")
        
        # Validate response format for debugging questions
        if is_debugging and code_context:
            logger.info(f"Debugging question detected. Code context has {len(code_context.get('files', []))} files")
            logger.debug(f"Response preview: {response_text[:200]}...")
            # Pre-check: Reject generic responses entirely
            if self._is_generic_response(response_text):
                logger.warning("LLM returned generic response for debugging question - forcing code analysis!")
                # Force a more explicit prompt with stronger instructions
                response_text = self._force_code_analysis(prompt, conversation_history, context_size, code_context, question_analysis)
            else:
                response_text = self._validate_and_enforce_debugging_format(response_text, code_context)
        elif is_debugging and not code_context:
            logger.warning("Debugging question detected but no code context provided!")
        
        return response_text
    
    def _fallback_response(self, message: str, code_context: Dict[str, Any], question_analysis: Optional[Dict[str, Any]] = None) -> str:
        """Generate a fallback response when Ollama is unavailable, actually reading files when requested."""
        
        # Use enhanced response generation for complex questions
        if question_analysis and question_analysis.get("complexity") == "complex":
            return self._generate_enhanced_response(
                question_analysis.get("type", "general"),
                code_context,
                question_analysis
            )
        
        message_lower = message.lower()
        response_parts = []
        
        # If files were requested, read and display them
        if code_context.get("files"):
            for file_path in code_context["files"]:  # No limit (local processing)
                file_data = self.code_reader.read_file(file_path, max_lines=5000)  # Increased for local processing
                if file_data.get("success"):
                    truncation_note = ""
                    if file_data.get("truncated"):
                        truncation_note = f"\n(Showing first {file_data['lines_read']} of {file_data['total_lines']} lines)"
                    
                    response_parts.append(f"""Here's the code from `{file_path}`{truncation_note}:

```python
{file_data['content']}
```""")
                else:
                    response_parts.append(f"Could not read file `{file_path}`: {file_data.get('error', 'Unknown error')}")
        
        # If search queries were made, show results
        if code_context.get("search_queries"):
            for query in code_context["search_queries"]:  # No limit (local processing)
                # Try multiple file patterns
                results = self.code_reader.search_codebase(query, file_pattern="*.py,*.tsx,*.ts,*.jsx,*.js", max_results=50)  # Increased
                if results:
                    response_parts.append(f"Search results for '{query}':\n")
                    for result in results:
                        response_parts.append(f"**{result['file_path']}:{result['line']}**\n```\n{result['context']}\n```\n")
                else:
                    # If no results, try to find files by name
                    found_files = self.code_reader.find_files_by_name(query.split()[-1] if query.split() else query, max_results=20)  # Increased
                    if found_files:
                        response_parts.append(f"Found files matching '{query}':\n")
                        for file_path in found_files:
                            file_data = self.code_reader.read_file(file_path, max_lines=5000)  # Increased for local
                            if file_data.get("success"):
                                response_parts.append(f"**{file_path}**\n```\n{file_data['content'][:500]}...\n```\n")
                    else:
                        response_parts.append(f"No results found for '{query}'. Try being more specific or asking to read a specific file.")
        
        # If error-related, include error logs and code from error locations
        if "error" in message_lower or "fail" in message_lower or "wrong" in message_lower:
            log_data = self.code_reader.read_error_logs(max_lines=30)
            if log_data.get("success") and log_data.get("error_count", 0) > 0:
                log_content = log_data['content']
                # Show last 2000 chars if log is very long
                if len(log_content) > 2000:
                    log_content = "... (earlier logs) ...\n" + log_content[-2000:]
                response_parts.append(f"""Recent error logs ({log_data['error_count']} errors found):

```
{log_content}
```""")
                
                # Show code from files referenced in errors
                if log_data.get("file_references"):
                    response_parts.append("**Code from error locations:**\n")
                    for ref in log_data["file_references"]:  # No limit (local processing)
                        file_data = self.code_reader.read_file_with_context(
                            ref["file"],
                            ref["line"],
                            context_lines=10
                        )
                        if file_data.get("success"):
                            response_parts.append(
                                f"**{ref['file']}:{ref['line']}** (referenced in error)\n"
                                f"```\n{file_data['content']}\n```\n"
                            )
        
        # If we have file/search results, return them
        if response_parts:
            # Only show a subtle note at the end, not prominently
            if not self.ollama_available:
                response_parts.append("\n---\n*Tip: Install Ollama for AI-powered explanations (optional)*")
            return "\n\n".join(response_parts)
        
        # Otherwise, provide helpful guidance
        if any(word in message_lower for word in ["show", "read", "see", "view"]):
            # Try to extract what they want to read
            if "upload" in message_lower:
                upload_results = self.code_reader.search_codebase("upload", file_pattern="*.py", max_results=50)  # Increased
                if upload_results:
                    response = "I found upload-related code:\n\n"
                    for result in upload_results:
                        response += f"**{result['file_path']}:{result['line']}**\n```\n{result['context']}\n```\n\n"
                    # Don't show the note here - it's redundant
                    return response
            
            return """I can read code files for you. Try asking:
- "Show me backend/main.py"
- "Read the upload code"
- "View frontend_clean/src/pages/Invoices.tsx"

*Tip: Install Ollama for AI-powered explanations (optional)*"""
        
        elif "upload" in message_lower or "file" in message_lower:
            # Try to find upload-related code
            upload_results = self.code_reader.search_codebase("upload", file_pattern="*.py", max_results=50)  # Increased
            if upload_results:
                response = "I found upload-related code:\n\n"
                for result in upload_results:
                    response += f"**{result['file_path']}:{result['line']}**\n```\n{result['context']}\n```\n\n"
                # Don't show redundant note
                return response
            return """I can help you with file uploads. The upload endpoint is in `backend/main.py` at the `/api/upload` route.

To see the code, you can ask me: "Show me backend/main.py" or "Read backend/main.py"

*Tip: Install Ollama for AI-powered explanations (optional)*"""
        
        else:
            return """I'm here to help with your code! I can:
- Read and explain code files
- Search for code patterns
- Help debug errors
- Answer questions about the codebase

Try asking:
- "Show me backend/main.py"
- "Find the upload function"
- "Why did my upload fail?"

*Tip: Install Ollama for AI-powered explanations (optional)*"""
    
    def _generate_enhanced_response(self, question_type: str, code_context: Dict[str, Any], question_analysis: Dict[str, Any]) -> str:
        """
        Generate high-quality response based on question type and context.
        
        Returns:
            Structured, detailed response
        """
        response_parts = []
        concepts = question_analysis.get("concepts", [])
        flow_paths = code_context.get("flow_paths", [])
        
        if question_type == "debugging":
            # Structured debugging response
            response_parts.append("## Problem Analysis\n")
            
            # Identify the issue
            problem_desc = question_analysis.get("original_message", "")
            response_parts.append(f"**Issue**: {problem_desc}\n")
            
            # Trace the flow
            if flow_paths:
                response_parts.append("## Data Flow Analysis\n")
                for flow in flow_paths:
                    if "from" in flow and "to" in flow:
                        traced = self.code_reader.trace_data_flow(flow["from"], flow["to"])
                        if traced:
                            # Generate flow diagram
                            diagram = self.code_reader.generate_flow_diagram(traced)
                            response_parts.append(f"**Flow from {flow['from']} to {flow['to']}:**\n")
                            response_parts.append(f"```\n{diagram}\n```\n")
                            
                            # Detailed steps
                            response_parts.append("**Detailed Steps:**\n")
                            for step in traced:
                                response_parts.append(f"1. **{step['concept']}** - {step['description']}")
                                if step.get('files'):
                                    response_parts.append(f"   - Files: {', '.join(step['files'])}")  # No limit
                            response_parts.append("")
            
            # Show relevant code - collect from all sources
            response_parts.append("## Relevant Code\n")
            
            # Collect files from multiple sources
            all_files = []
            
            # From implicit_files
            all_files.extend(code_context.get("implicit_files", []))
            
            # From regular files
            all_files.extend(code_context.get("files", []))
            
            # Extract from flow_paths
            for flow in flow_paths:
                if "from" in flow and "to" in flow:
                    traced = self.code_reader.trace_data_flow(flow["from"], flow["to"])
                    for step in traced:
                        step_files = step.get("files", [])
                        if isinstance(step_files, list):
                            all_files.extend(step_files)
                        elif step_files:
                            all_files.append(step_files)
            
            # Remove duplicates and filter out virtual environment files
            seen = set()
            filtered_files = []
            for file_path in all_files:
                if not file_path:
                    continue
                # Normalize path
                if isinstance(file_path, str):
                    file_path_str = file_path.replace("\\", "/")
                else:
                    file_path_str = str(file_path)
                
                # Skip if already seen or in virtual environment
                if file_path_str in seen:
                    continue
                if self.code_reader._should_skip_file(file_path_str):
                    continue
                
                seen.add(file_path_str)
                filtered_files.append(file_path_str)
            
            # Prioritize files: frontend components and API endpoints first
            # Sort by relevance: frontend components > API routes > services > others
            def file_priority(file_path):
                path_lower = file_path.lower()
                message_lower = question_analysis.get("original_message", "").lower()
                
                # Highest priority: frontend components that display invoices/cards
                if any(keyword in path_lower for keyword in ["invoicecard", "invoicedetail", "invoices.tsx", "invoicepanel"]):
                    return 0
                # High priority: upload.ts if question is about contents not showing
                elif ("content" in message_lower or "show" in message_lower) and "upload.ts" in path_lower:
                    return 1
                # High priority: upload/status endpoint in main.py if question is about contents
                elif ("content" in message_lower or "show" in message_lower) and "main.py" in path_lower:
                    return 1
                # High priority: API routes that return invoices/line_items
                elif ("/api/invoices" in path_lower or "invoices_submit" in path_lower or 
                      ("main.py" in path_lower and "invoices" in path_lower)):
                    return 2
                # Medium priority: database functions for line items
                elif ("get_line_items" in path_lower or "db.py" in path_lower):
                    return 3
                # Medium priority: OCR service that extracts line items
                elif "ocr_service" in path_lower:
                    return 4
                # Lower priority: other API routes
                elif "/api" in path_lower or "routes" in path_lower:
                    return 5
                # Lower priority: services
                elif "services" in path_lower:
                    return 6
                # Lowest priority: other frontend files
                elif "frontend" in path_lower:
                    return 7
                else:
                    return 8
            
            filtered_files.sort(key=file_priority)
            
            # Process all relevant files (local processing - no limit)
            # filtered_files = filtered_files[:5]  # Removed limit
            
            # Actually read and display code
            if filtered_files:
                for file_path in filtered_files:
                    file_data = self.code_reader.read_file(file_path, max_lines=5000)  # Increased for local
                    if file_data.get("success"):
                        truncation_note = ""
                        if file_data.get("truncated"):
                            truncation_note = f" *(showing first {file_data['lines_read']} of {file_data['total_lines']} lines)*"
                        response_parts.append(f"### {file_path}{truncation_note}\n```\n{file_data['content']}\n```\n")
                    else:
                        response_parts.append(f"### {file_path}\n*Could not read file: {file_data.get('error', 'Unknown error')}*\n")
            else:
                response_parts.append("*No relevant code files found. Try asking about specific files or functions.*\n")
            
            # Actionable suggestions - tailored to the specific issue
            response_parts.append("## Suggested Steps to Debug\n")
            
            # Check if this is about line items not showing
            message_lower = question_analysis.get("original_message", "").lower()
            if "line" in message_lower and ("item" in message_lower or "card" in message_lower):
                response_parts.append("**For line items not showing on cards:**\n")
                response_parts.append("1. **Check upload/status endpoint**: Verify `/api/upload/status?doc_id=...` returns `items` or `line_items` in the response")
                response_parts.append("2. **Check database**: Query `invoice_line_items` table to confirm line items are stored after OCR")
                response_parts.append("3. **Check frontend normalization**: In `frontend_clean/src/lib/upload.ts`, the `normalizeUploadResponse` function looks for `raw.line_items` or `raw.items` - verify the upload response includes these")
                response_parts.append("4. **Check polling**: The frontend polls `/api/upload/status` - verify `statusData.items` is populated when OCR completes")
                response_parts.append("5. **Alternative**: The `/api/invoices` endpoint returns `line_items` (line 347), but the frontend uses upload metadata. Consider fetching from `/api/invoices` after upload completes")
                response_parts.append("6. **Browser console**: Check for errors when polling status or normalizing the response")
            elif "content" in message_lower or ("invoice" in message_lower and ("show" in message_lower or "display" in message_lower or "appear" in message_lower)):
                response_parts.append("**For invoice contents not showing:**\n")
                response_parts.append("1. **Check upload/status endpoint**: After upload, the frontend polls `/api/upload/status?doc_id=...` - verify it returns:")
                response_parts.append("   - `parsed` object with supplier, date, value")
                response_parts.append("   - `items` array with line items")
                response_parts.append("   - `invoice` object with complete invoice data")
                response_parts.append("2. **Check OCR processing**: Verify the document status in database (`documents.status`) - should be 'scanned' or 'completed'")
                response_parts.append("3. **Check invoice creation**: Query `invoices` table to confirm an invoice record exists for the document (`doc_id`)")
                response_parts.append("4. **Check frontend polling**: In `frontend_clean/src/lib/upload.ts`, the `pollUploadStatus` function waits for `statusData.parsed` or `statusData.items` - verify these are present")
                response_parts.append("5. **Check InvoiceDetailPanel**: The component reads `metadata.lineItems` from upload response - verify the normalized response includes `lineItems`")
                response_parts.append("6. **Database check**: Run `SELECT * FROM invoices WHERE doc_id = ?` and `SELECT * FROM invoice_line_items WHERE doc_id = ?` to verify data exists")
                response_parts.append("7. **Browser console**: Check for errors in the upload/polling process or when rendering InvoiceDetailPanel")
            else:
                response_parts.append("1. Check if data is being extracted correctly")
                response_parts.append("2. Verify data is stored in the database")
                response_parts.append("3. Check API endpoints return the data")
                response_parts.append("4. Verify frontend components receive and display the data")
                response_parts.append("5. Check browser console for errors")
            
        elif question_type == "flow":
            # Flow explanation
            response_parts.append("## Data Flow Explanation\n")
            
            if flow_paths:
                for flow in flow_paths:
                    if "from" in flow and "to" in flow:
                        traced = self.code_reader.trace_data_flow(flow["from"], flow["to"])
                        if traced:
                            # Generate flow diagram
                            diagram = self.code_reader.generate_flow_diagram(traced)
                            response_parts.append(f"### Flow: {flow['from']} → {flow['to']}\n")
                            response_parts.append(f"```\n{diagram}\n```\n")
                            
                            # Detailed steps
                            for i, step in enumerate(traced, 1):
                                response_parts.append(f"{i}. **{step['concept']}**")
                                response_parts.append(f"   - {step['description']}")
                                if step.get('files'):
                                    response_parts.append(f"   - Code location: {', '.join(step['files'][:2])}")
                            response_parts.append("")
            
            # Show code at each step
            response_parts.append("## Code at Each Step\n")
            implicit_files = code_context.get("implicit_files", [])
            for file_path in implicit_files:  # No limit (local processing)
                file_data = self.code_reader.read_file(file_path, max_lines=5000)  # Increased for local
                if file_data.get("success"):
                    response_parts.append(f"### {file_path}\n```\n{file_data['content']}\n```\n")
        
        elif question_type == "how-to":
            # How-to guide
            response_parts.append("## How-To Guide\n")
            
            # Find similar code patterns
            for concept in concepts:
                results = self.code_reader.search_codebase(concept, max_results=50)  # Increased
                if results:
                    response_parts.append(f"### Related Code for '{concept}'\n")
                    for result in results:
                        response_parts.append(f"**{result['file_path']}:{result['line']}**\n```\n{result['context']}\n```\n")
        
        elif question_type == "what-is":
            # Definition/explanation
            response_parts.append("## Explanation\n")
            
            # Find definition and usage
            for concept in concepts:
                results = self.code_reader.search_codebase(concept, max_results=50)  # Increased
                if results:
                    response_parts.append(f"### '{concept}' in the codebase\n")
                    for result in results:  # No limit (local processing)
                        response_parts.append(f"**{result['file_path']}:{result['line']}**\n```\n{result['context']}\n```\n")
        
        else:
            # General complex question
            response_parts.append("## Analysis\n")
            response_parts.append(f"**Question**: {question_analysis.get('original_message', '')}\n")
            response_parts.append(f"**Concepts identified**: {', '.join(concepts)}\n")
            
            # Show relevant code
            if code_context.get("files") or code_context.get("implicit_files"):
                response_parts.append("## Relevant Code\n")
                all_files = code_context.get("files", []) + code_context.get("implicit_files", [])
                for file_path in all_files:  # No limit (local processing)
                    file_data = self.code_reader.read_file(file_path, max_lines=5000)  # Increased for local
                    if file_data.get("success"):
                        truncation_note = ""
                        if file_data.get("truncated"):
                            truncation_note = f"\n(Showing first {file_data['lines_read']} of {file_data['total_lines']} lines)"
                        response_parts.append(f"### {file_path}{truncation_note}\n```\n{file_data['content']}\n```\n")
        
        # Only show subtle tip at the end if Ollama is not available
        if not self.ollama_available:
            response_parts.append("\n---\n*Tip: Install Ollama for AI-powered explanations (optional)*")
        
        return "\n".join(response_parts)
    
    def _get_related_files_for_debugging(self, message: str, code_context: Dict[str, Any]) -> List[str]:
        """
        Automatically find and include related files for debugging questions.
        ALWAYS returns 3-10 files to ensure comprehensive code analysis.
        """
        message_lower = message.lower()
        related_files = []
        
        # Core file patterns - comprehensive coverage
        file_patterns = {
            # Upload flow patterns
            "upload": [
                "frontend_clean/src/lib/upload.ts",
                "backend/main.py",
                "backend/services/ocr_service.py",
                "backend/app/db.py"
            ],
            # Display/UI patterns
            "display": [
                "frontend_clean/src/pages/Invoices.tsx",
                "frontend_clean/src/components/InvoiceDetailPanel.tsx",
                "frontend_clean/src/lib/upload.ts",
                "backend/main.py"
            ],
            # Invoice/card patterns
            "invoice": [
                "frontend_clean/src/pages/Invoices.tsx",
                "frontend_clean/src/components/InvoiceDetailPanel.tsx",
                "backend/routes/invoices_submit.py",
                "backend/app/db.py",
                "frontend_clean/src/lib/upload.ts"
            ],
            # OCR/extraction patterns
            "ocr": [
                "backend/services/ocr_service.py",
                "backend/services/engine_select.py",
                "backend/main.py",
                "backend/app/db.py"
            ],
            # Database/storage patterns
            "database": [
                "backend/app/db.py",
                "backend/main.py",
                "backend/services/ocr_service.py"
            ],
            # API/endpoint patterns
            "api": [
                "backend/main.py",
                "backend/routes/invoices_submit.py",
                "frontend_clean/src/lib/api.ts",
                "frontend_clean/src/lib/upload.ts"
            ]
        }
        
        # Expanded keyword matching for each pattern
        pattern_keywords = {
            "upload": ["upload", "file", "status", "poll", "submit"],
            "display": ["display", "show", "shown", "appear", "visible", "render", "card", "content", "contents"],
            "invoice": ["invoice", "supplier", "items", "line items", "line_items", "card", "detail", "panel"],
            "ocr": ["ocr", "scan", "extract", "parse", "text", "recognition"],
            "database": ["database", "db", "table", "query", "sql", "store", "save"],
            "api": ["api", "endpoint", "request", "response", "fetch", "call"]
        }
        
        # Match patterns and collect files
        matched_patterns = set()
        for pattern_name, keywords in pattern_keywords.items():
            if any(keyword in message_lower for keyword in keywords):
                matched_patterns.add(pattern_name)
                related_files.extend(file_patterns[pattern_name])
        
        # Special combined patterns for common debugging scenarios
        # Scenario 1: Upload but not showing (full flow)
        if ("upload" in message_lower or "submit" in message_lower) and \
           any(word in message_lower for word in ["not", "isn't", "doesn't", "missing", "empty"]) and \
           any(word in message_lower for word in ["show", "display", "appear", "content", "items"]):
            logger.info("Detected full flow debugging: upload → processing → display")
            related_files.extend([
                "frontend_clean/src/lib/upload.ts",  # Upload normalization
                "frontend_clean/src/pages/Invoices.tsx",  # Display component
                "backend/main.py",  # API endpoints
                "backend/services/ocr_service.py",  # OCR processing
                "backend/app/db.py",  # Data storage
                "frontend_clean/src/components/InvoiceDetailPanel.tsx"  # Detail view
            ])
        
        # Scenario 2: Supplier name not showing
        if "supplier" in message_lower and any(word in message_lower for word in ["not", "missing", "empty", "unknown"]):
            logger.info("Detected supplier extraction issue")
            related_files.extend([
                "frontend_clean/src/lib/upload.ts",
                "backend/services/ocr_service.py",
                "backend/main.py"
            ])
        
        # Scenario 3: Line items not showing
        if ("line" in message_lower and "item" in message_lower) or "line_item" in message_lower:
            logger.info("Detected line items issue")
            related_files.extend([
                "frontend_clean/src/lib/upload.ts",
                "frontend_clean/src/components/InvoiceDetailPanel.tsx",
                "backend/app/db.py",
                "backend/services/ocr_service.py"
            ])
        
        # Remove duplicates while preserving order
        seen = set()
        unique_files = []
        for file_path in related_files:
            if file_path not in seen:
                seen.add(file_path)
                unique_files.append(file_path)
        
        # Validate files exist and collect valid ones
        valid_files = []
        for file_path in unique_files:
            file_data = self.code_reader.read_file(file_path, max_lines=1)
            if file_data.get("success"):
                valid_files.append(file_path)
        
        # ENSURE MINIMUM 3 FILES for debugging questions
        if len(valid_files) < 3:
            logger.warning(f"Only {len(valid_files)} files found, adding fallback core files")
            # Add core fallback files
            fallback_files = [
                "frontend_clean/src/pages/Invoices.tsx",
                "backend/main.py",
                "frontend_clean/src/lib/upload.ts",
                "backend/services/ocr_service.py",
                "backend/app/db.py"
            ]
            for fallback_file in fallback_files:
                if fallback_file not in valid_files:
                    file_data = self.code_reader.read_file(fallback_file, max_lines=1)
                    if file_data.get("success"):
                        valid_files.append(fallback_file)
                        if len(valid_files) >= 3:
                            break
        
        # No limit for local processing - use all valid files
        # if len(valid_files) > 10:
        #     logger.info(f"Limiting files from {len(valid_files)} to 10 for performance")
        #     valid_files = valid_files[:10]
        
        logger.info(f"Auto-included {len(valid_files)} files for debugging: {', '.join(valid_files)}")
        return valid_files
    
    def _is_generic_response(self, response_text: str) -> bool:
        """
        Strict validation - check if response is generic troubleshooting without code analysis.
        Returns True if response should be rejected.
        
        Rejection criteria:
        - Generic phrase count > 3
        - Code references < 2 file names
        - No line numbers or function names for debugging questions
        - Numbered generic troubleshooting lists
        """
        response_lower = response_text.lower()
        
        # Enhanced generic phrase patterns
        generic_patterns = [
            # Generic troubleshooting starters
            r'\d+\.\s+\*\*(?:issue|problem)\s+\d+:',  # "1. **Issue 1:"
            r'\d+\.\s+(?:incorrect|check|verify|ensure|try|adjust|make\s+sure)',
            r'issue\s+\d+:\s+(?:incorrect|poor|insufficient)',
            
            # Vague cause statements
            r'(?:could|might|may)\s+be\s+(?:caused|due\s+to|related\s+to)',
            r'(?:possible|potential)\s+(?:reasons|causes|issues)',
            r'there\s+(?:are|is|could\s+be)\s+(?:\d+|several|many|some)\s+(?:possible|potential)\s+(?:reasons|causes)',
            r'here\s+are\s+(?:some|a few|several|many)\s+(?:possible|potential)',
            
            # Generic troubleshooting advice
            r'(?:check|verify|ensure|confirm)\s+(?:that|if|whether)',
            r'you\s+(?:should|must|can|may)\s+(?:check|verify|ensure)',
            r'to\s+troubleshoot\s+(?:this|these)\s+(?:issue|problem)',
            r'try\s+(?:the\s+following|these)\s+steps',
            
            # Vague references
            r'based\s+on\s+(?:your|the)\s+(?:description|information|error)',
            r'it\s+seems\s+(?:like|that)',
            r'this\s+(?:could|might|may)\s+be',
            
            # Insufficient/incorrect patterns
            r'insufficient\s+(?:resources|memory|data)',
            r'incorrect\s+(?:configuration|format|file|data)',
            r'improper\s+(?:setup|configuration)',
            
            # Generic advice closings
            r'i\s+hope\s+this\s+helps',
            r'let\s+me\s+know\s+if',
            r'feel\s+free\s+to',
        ]
        
        # Count generic phrases
        generic_count = 0
        import re
        for pattern in generic_patterns:
            matches = re.findall(pattern, response_lower)
            generic_count += len(matches)
            if matches:
                logger.debug(f"Found {len(matches)} generic phrase(s): {pattern}")
        
        # Rejection threshold: > 3 generic phrases
        if generic_count > 3:
            logger.warning(f"Generic response rejected: {generic_count} generic phrases detected")
            return True
        
        # Check for code references
        # Pattern 1: File names with extensions
        file_pattern = r'\b[\w/\\-]+\.(?:py|ts|tsx|js|jsx|json|yaml)\b'
        file_matches = re.findall(file_pattern, response_text)
        unique_files = set(file_matches)
        
        # Pattern 2: Line numbers (file.py:123 or line 123)
        line_pattern = r'(?:\w+\.(?:py|ts|tsx|js|jsx):\d+|line\s+\d+)'
        line_matches = re.findall(line_pattern, response_lower)
        
        # Pattern 3: Function/method names (def function_name, function functionName)
        function_pattern = r'(?:def|function|const|class)\s+\w+'
        function_matches = re.findall(function_pattern, response_text)
        
        # Pattern 4: Code snippets (triple backticks)
        code_snippet_pattern = r'```[\s\S]*?```'
        code_snippets = re.findall(code_snippet_pattern, response_text)
        
        logger.debug(
            f"Code reference analysis: "
            f"files={len(unique_files)}, lines={len(line_matches)}, "
            f"functions={len(function_matches)}, snippets={len(code_snippets)}"
        )
        
        # Strict requirements for code references
        if len(unique_files) < 2:
            logger.warning(f"Generic response rejected: only {len(unique_files)} file(s) referenced (need 2+)")
            return True
        
        if len(line_matches) == 0 and len(function_matches) == 0:
            logger.warning("Generic response rejected: no line numbers or function names referenced")
            return True
        
        # If lots of generic phrases and few code references, reject
        if generic_count > 1 and len(unique_files) < 3:
            logger.warning(f"Generic response rejected: {generic_count} generic phrases with only {len(unique_files)} files")
            return True
        
        logger.debug("Response validation passed: appears code-specific")
        return False
    
    def _force_code_analysis(self, prompt: str, conversation_history: Optional[List[Dict[str, str]]], context_size: int, code_context: Dict[str, Any], question_analysis: Optional[Dict[str, Any]]) -> str:
        """
        Force the LLM to analyze code by sending a MUCH stricter prompt.
        This is called when the LLM returns a generic response.
        Uses maximum pressure to get code-specific analysis.
        """
        logger.warning("FORCING code analysis with maximum strictness - previous response was generic")
        
        # Get file list and read actual code
        files_provided = code_context.get("files", [])
        file_specs = code_context.get("file_specs", [])
        file_specs_dict = {spec["path"]: spec for spec in file_specs if isinstance(spec, dict)}
        
        # Read actual code from files
        code_snippets = []
        for file_path in files_provided:  # No limit (local processing)
            # Use max_lines from file_specs if available, otherwise use config default
            max_lines = 5000  # Increased default for local processing
            if file_path in file_specs_dict:
                max_lines = file_specs_dict[file_path].get("max_lines", 5000)  # Removed 500 cap
            
            file_data = self.code_reader.read_file(file_path, max_lines=max_lines)
            if file_data.get("success"):
                content = file_data['content']
                lines = content.split('\n')
                # Remove trailing empty line if file ends with newline
                if lines and lines[-1] == '' and content.endswith('\n'):
                    lines = lines[:-1]
                
                # Add line numbers
                numbered_lines = []
                for i, line in enumerate(lines, 1):
                    line = line.rstrip('\r')
                    numbered_lines.append(f"{i:4}: {line}")
                
                numbered_content = '\n'.join(numbered_lines)
                lines_read = file_data.get('lines_read', len(lines))
                total_lines = file_data.get('total_lines', len(lines))
                
                truncation_note = ""
                if file_data.get("truncated"):
                    truncation_note = f" (showing lines 1-{lines_read} of {total_lines})"
                else:
                    truncation_note = f" (lines 1-{lines_read})"
                
                code_snippets.append(f"File: {file_path}{truncation_note}\n```\n{numbered_content}\n```")
        
        files_list = "\n".join([f"- {f}" for f in files_provided])
        code_block = "\n\n".join(code_snippets) if code_snippets else "No code files could be read."
        
        # Build extremely forceful prompt with actual code
        force_prompt = "="*80 + "\n"
        force_prompt += "PREVIOUS RESPONSE WAS TOO GENERIC. TRY AGAIN.\n"
        force_prompt += "="*80 + "\n\n"
        force_prompt += "CODE FILES PROVIDED FOR ANALYSIS:\n"
        force_prompt += files_list + "\n\n"
        force_prompt += "="*80 + "\n"
        force_prompt += "ACTUAL CODE FROM FILES (YOU MUST ANALYZE THIS):\n"
        force_prompt += "="*80 + "\n\n"
        force_prompt += code_block + "\n\n"
        force_prompt += "="*80 + "\n"
        force_prompt += "MANDATORY REQUIREMENTS:\n"
        force_prompt += "1. EXPLAIN PROBLEMS IN TEXT - don't just dump code\n"
        force_prompt += "2. Use SMALL code snippets (3-5 lines max) to illustrate points\n"
        force_prompt += "3. Always include file:line references with each snippet\n"
        force_prompt += "4. Cite at least 3 specific files from the code provided above\n"
        force_prompt += "5. Reference at least 5 line numbers (use file.py:123 format)\n"
        force_prompt += "6. Quote actual code snippets (small, 3-5 lines) from the files above\n"
        force_prompt += "7. Trace data flow through multiple files (explain in text with file:line refs)\n"
        force_prompt += "8. Identify exact breaking point in code (explain in text)\n"
        force_prompt += "9. Provide specific code changes (BEFORE/AFTER) - small snippets only\n"
        force_prompt += "10. DO NOT dump entire files or large code blocks\n"
        force_prompt += "="*80 + "\n\n"
        force_prompt += "ORIGINAL QUESTION:\n"
        force_prompt += prompt + "\n\n"
        force_prompt += "="*80 + "\n"
        force_prompt += "ANALYZE THE CODE ABOVE. DO NOT GIVE GENERIC TROUBLESHOOTING.\n"
        force_prompt += "="*80 + "\n"
        
        # Call Ollama again with forced prompt
        # Select best model again (possibly try a different one)
        question_type = question_analysis.get("type", "debugging") if question_analysis else "debugging"
        code_files_count = len(code_context.get("files", []))
        selected_model, effective_context = self._select_model_for_request(
            question_type,
            context_size,
            code_files_count
        )
        
        messages = []
        messages.append({
            "role": "system",
            "content": """You are an expert code debugger. You MUST analyze actual code with specific line numbers.

ABSOLUTE REQUIREMENTS:
1. Reference at least 3 files by name
2. Include at least 5 file:line references (e.g., upload.ts:240)
3. Quote actual code snippets
4. Trace data flow through code
5. NEVER give generic troubleshooting

If you give generic advice again, your response will be rejected."""
        })
        
        if conversation_history:
            messages.extend(conversation_history[-10:])
        
        messages.append({
            "role": "user",
            "content": force_prompt
        })
        
        # Use extremely low temperature for forced analysis
        temperature = 0.1  # Minimum hallucination
        
        # Longer timeout for forced analysis
        timeout = min(300, max(90, int(effective_context / 1000) * 3))
        
        try:
            response = requests.post(
                f"{self.ollama_url}/api/chat",
                json={
                    "model": selected_model,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "num_predict": -1,  # -1 = unlimited output (local processing)
                        "num_ctx": effective_context,
                        "top_p": 0.85,  # Slightly more focused
                        "repeat_penalty": 1.2,  # Discourage repetition
                        "top_k": 30  # More focused
                    }
                },
                timeout=timeout
            )
            
            if response.status_code != 200:
                raise Exception(f"Ollama API returned status {response.status_code}")
            
            result = response.json()
            response_text = result.get("message", {}).get("content", "I'm sorry, I couldn't generate a response.")
            
            # Validate again - if still generic, give up and return enhanced fallback
            if self._is_generic_response(response_text):
                logger.error("Forced code analysis still returned generic response - using enhanced fallback")
                return self._generate_code_based_fallback(code_context, question_analysis)
            
            # Validate format
            response_text = self._validate_and_enforce_debugging_format(response_text, code_context)
            
            return response_text
        except Exception as e:
            logger.error(f"Failed to force code analysis: {e}")
            # Return code-based fallback instead of error
            return self._generate_code_based_fallback(code_context, question_analysis)
    
    def _generate_code_based_fallback(self, code_context: Dict[str, Any], question_analysis: Optional[Dict[str, Any]]) -> str:
        """
        Generate a code-based response without LLM.
        Always shows actual code and provides structured analysis.
        """
        logger.info("Generating code-based fallback response")
        
        files = code_context.get("files", [])
        response_parts = []
        
        # Check why we're in fallback mode
        fallback_reason = "LLM unavailable"
        if self.ollama_available:
            if not self.available_models:
                fallback_reason = "No models available in Ollama"
            else:
                fallback_reason = "LLM returned generic response or all models failed"
        else:
            fallback_reason = f"Ollama not available at {self.ollama_url}"
        
        response_parts.append("## Code Analysis (Fallback Mode)")
        response_parts.append(f"**Status:** {fallback_reason}\n")
        
        # Show all code files with analysis
        if files:
            response_parts.append("### Files Analyzed\n")
            for file_path in files:  # No limit (local processing)
                file_data = self.code_reader.read_file(file_path, max_lines=5000)  # Increased for local
                if file_data.get("success"):
                    total_lines = file_data.get("total_lines", 0)
                    lines_shown = file_data.get("lines_read", 0)
                    truncated = " (truncated)" if file_data.get("truncated") else ""
                    
                    response_parts.append(f"#### `{file_path}` ({total_lines} lines total){truncated}\n")
                    
                    # Extract key information from the code
                    content = file_data['content']
                    
                    # Find function definitions
                    import re
                    functions = re.findall(r'def\s+(\w+)\s*\([^)]*\):', content)
                    classes = re.findall(r'class\s+(\w+)', content)
                    imports = re.findall(r'^import\s+(\S+)|^from\s+(\S+)\s+import', content, re.MULTILINE)
                    
                    if functions or classes:
                        response_parts.append("**Key Components:**\n")
                        if classes:
                            response_parts.append(f"- Classes: {', '.join(classes[:10])}\n")
                        if functions:
                            response_parts.append(f"- Functions: {', '.join(functions[:15])}\n")
                        if imports:
                            import_list = [imp[0] or imp[1] for imp in imports[:10]]
                            response_parts.append(f"- Imports: {', '.join(import_list)}\n")
                        response_parts.append("")
                    
                    # Show code (limit to first 200 lines for readability)
                    code_lines = content.split('\n')
                    if len(code_lines) > 200:
                        response_parts.append(f"```\n{chr(10).join(code_lines[:200])}\n... ({len(code_lines) - 200} more lines) ...\n```\n")
                    else:
                        response_parts.append(f"```\n{content}\n```\n")
                else:
                    response_parts.append(f"**Error reading `{file_path}`:** {file_data.get('error', 'Unknown error')}\n")
        else:
            response_parts.append("**No files provided in context.**\n")
            response_parts.append("To get code analysis, ask about specific files, e.g., 'read backend/main.py'\n")
        
        # Provide structured guidance based on question type
        if question_analysis and question_analysis.get("type") == "debugging":
            response_parts.append("### Debugging Steps\n")
            response_parts.append("1. **Review the code above** - Look for data flow issues, missing null checks, type mismatches")
            response_parts.append("2. **Check API responses** - Verify backend returns expected data structure (use browser DevTools Network tab)")
            response_parts.append("3. **Trace normalization** - Check if frontend normalizes backend responses correctly")
            response_parts.append("4. **Verify database** - Check if data is stored correctly (use SQL queries)")
            response_parts.append("5. **Console logs** - Check browser/server logs for errors\n")
            
            # Add data flow diagram
            response_parts.append("### Typical Data Flow\n")
            response_parts.append("```")
            response_parts.append("Upload → /api/upload → OCR Processing → Database")
            response_parts.append("↓")
            response_parts.append("Frontend polls /api/upload/status")
            response_parts.append("↓")
            response_parts.append("Response normalized in upload.ts")
            response_parts.append("↓")
            response_parts.append("Display in Invoices.tsx / InvoiceDetailPanel.tsx")
            response_parts.append("```\n")
        else:
            response_parts.append("### Next Steps\n")
            response_parts.append("1. Review the code files above")
            response_parts.append("2. Check for common issues: missing imports, incorrect function calls, data type mismatches")
            response_parts.append("3. Verify the code matches your expectations\n")
        
        # Add diagnostic information
        response_parts.append("### Diagnostic Information\n")
        response_parts.append(f"- **Ollama Status:** {'Available' if self.ollama_available else 'Not Available'}")
        if self.ollama_available:
            response_parts.append(f"- **Available Models:** {', '.join(self.available_models) if self.available_models else 'None'}")
            response_parts.append(f"- **Primary Model:** {self.model}")
        response_parts.append(f"- **Ollama URL:** {self.ollama_url}\n")
        
        # Installation instructions
        if not self.ollama_available:
            response_parts.append("### To Enable AI Analysis\n")
            response_parts.append("1. Install Ollama: https://ollama.ai")
            response_parts.append("2. Pull a model: `ollama pull qwen2.5-coder:7b` or `ollama pull deepseek-coder:6.7b`")
            response_parts.append("3. Restart the application\n")
        elif not self.available_models:
            response_parts.append("### To Enable AI Analysis\n")
            response_parts.append("Install a model: `ollama pull qwen2.5-coder:7b` or `ollama pull deepseek-coder:6.7b`\n")
        
        return "\n".join(response_parts)
    
    def _validate_and_enforce_debugging_format(self, response_text: str, code_context: Dict[str, Any]) -> str:
        """
        Validate that debugging response follows the mandatory format.
        If not, add warnings and attempt to guide the LLM.
        """
        response_lower = response_text.lower()
        
        # Check for required sections
        has_step1 = "step 1" in response_lower and ("files analyzed" in response_lower or "files examined" in response_lower)
        has_step2 = "step 2" in response_lower and ("data flow" in response_lower or "flow trace" in response_lower)
        has_step3 = "step 3" in response_lower and ("code analysis" in response_lower or "analysis" in response_lower)
        has_step4 = "step 4" in response_lower and ("root cause" in response_lower)
        has_step5 = "step 5" in response_lower and ("fix" in response_lower)
        
        # Check for file:line references (pattern like "file.ts:123" or "file.py:456")
        import re
        file_line_pattern = r'\b\w+\.(py|ts|tsx|js|jsx):\d+'
        has_file_references = bool(re.search(file_line_pattern, response_text))
        
        # Check for code blocks with line numbers
        has_numbered_code = bool(re.search(r'\d{1,4}:\s+', response_text))
        
        # Check for generic troubleshooting patterns - MORE AGGRESSIVE
        generic_patterns = [
            r'check\s+(?:your|the)\s+(?:configuration|settings|file format)',
            r'ensure\s+(?:that|your|the)',
            r'verify\s+(?:that|your|the)',
            r'make\s+sure\s+(?:that|your|the)',
            r'potential\s+causes?:',
            r'here\s+are\s+(?:some|a few|potential)',
            r'there\s+(?:are|is)\s+(?:two|three|four|five|six|seven|eight|nine|ten|several|many)\s+(?:possible|potential)\s+(?:reasons|causes)',  # "there are two possible reasons"
            r'incorrect\s+(?:ocr|supplier|file|database|configuration|pipeline|storage|connection|upload|file\s+format|file\s+size|file\s+type)',  # "Incorrect OCR confidence"
            r'you\s+can\s+try\s+(?:adjusting|using|changing|modifying)',  # "You can try adjusting"
            r'\d+\.\s+(?:Incorrect|Check|Verify|Ensure|Make\s+sure|Try|Adjust)',  # Numbered generic list starting with "1. Incorrect..."
            r'based\s+on\s+(?:your|the)\s+(?:description|information|question)',  # "Based on your description"
            r'it\s+seems\s+(?:like|that)\s+(?:there|you)',  # "It seems like there are..."
            r'i\s+hope\s+this\s+(?:information|helps)',  # "I hope this information helps"
            r'troubleshoot\s+(?:and\s+)?resolve',  # "troubleshoot and resolve"
        ]
        has_generic_advice = any(re.search(pattern, response_lower) for pattern in generic_patterns)
        
        # Build validation warnings
        warnings = []
        if not has_step1:
            warnings.append("MISSING: Step 1 (Files Analyzed)")
        if not has_step2:
            warnings.append("MISSING: Step 2 (Data Flow Trace)")
        if not has_step3:
            warnings.append("MISSING: Step 3 (Code Analysis)")
        if not has_step4:
            warnings.append("MISSING: Step 4 (Root Cause)")
        if not has_step5:
            warnings.append("MISSING: Step 5 (Fix)")
        if not has_file_references:
            warnings.append("MISSING: File:line references (e.g., 'file.ts:123')")
        if not has_numbered_code:
            warnings.append("MISSING: Code quotes with line numbers")
        if has_generic_advice:
            warnings.append("WARNING: Contains generic troubleshooting advice - should analyze actual code instead")
        
        # If validation fails, prepend warning to response
        if warnings:
            warning_header = "\n⚠️ RESPONSE FORMAT VALIDATION FAILED ⚠️\n"
            warning_header += "The response does not follow the mandatory debugging format:\n"
            for warning in warnings:
                warning_header += f"- {warning}\n"
            warning_header += "\nPlease reformat your response to include all 5 mandatory steps with file:line references.\n"
            warning_header += "=" * 60 + "\n\n"
            
            # Prepend warning but keep original response
            response_text = warning_header + response_text
            logger.warning(f"Debugging response validation failed: {', '.join(warnings)}")
        
        return response_text
    
    def _needs_exploration(self, message: str) -> bool:
        """
        Detect if user question suggests they don't know where problem is.
        
        Indicators:
        - "why did X but Y not" (vague)
        - "something is wrong" (no location)
        - "not working" (no specifics)
        - No file paths mentioned
        
        Args:
            message: User's question
            
        Returns:
            True if exploration is needed, False otherwise
        """
        message_lower = message.lower()
        
        # Vague patterns that suggest user doesn't know where problem is
        vague_patterns = [
            "why did", "but", "not showing", "not working",
            "something", "issue", "problem", "broken",
            "doesn't work", "isn't working", "can't find",
            "where is", "what happened", "why isn't"
        ]
        
        # Specific patterns that indicate user knows where problem is
        specific_patterns = [
            r'\b\w+\.(py|ts|tsx|js):\d+',  # File:line references
            "in the", "at line", "function", "endpoint",
            "in file", "in code", "in ", "file:", "line:"
        ]
        
        has_vague = any(p in message_lower for p in vague_patterns)
        has_specific = any(re.search(p, message) for p in specific_patterns)
        
        # If vague but no specific location mentioned, needs exploration
        return has_vague and not has_specific
    
    def _parse_exploration_plan(self, plan_text: str) -> Dict[str, Any]:
        """
        Parse LLM exploration plan into structured dict.
        
        Args:
            plan_text: LLM response with exploration plan
            
        Returns:
            Dict with SEARCH, FILES, TRACE, FUNCTIONS lists
        """
        plan = {
            "SEARCH": [],
            "FILES": [],
            "TRACE": [],
            "FUNCTIONS": []
        }
        
        lines = plan_text.split('\n')
        current_section = None
        
        for line in lines:
            line_upper = line.upper().strip()
            
            # Detect section headers
            if line_upper.startswith('SEARCH:'):
                current_section = "SEARCH"
                # Extract search terms from same line
                parts = line.split(':', 1)
                if len(parts) > 1:
                    terms = parts[1].strip()
                    if terms:
                        plan["SEARCH"].extend([t.strip() for t in terms.split(',') if t.strip()])
            elif line_upper.startswith('FILES:'):
                current_section = "FILES"
                parts = line.split(':', 1)
                if len(parts) > 1:
                    files = parts[1].strip()
                    if files:
                        plan["FILES"].extend([f.strip() for f in files.split(',') if f.strip()])
            elif line_upper.startswith('TRACE:'):
                current_section = "TRACE"
                parts = line.split(':', 1)
                if len(parts) > 1:
                    trace = parts[1].strip()
                    if trace:
                        plan["TRACE"].append(trace)
            elif line_upper.startswith('FUNCTIONS:'):
                current_section = "FUNCTIONS"
                parts = line.split(':', 1)
                if len(parts) > 1:
                    funcs = parts[1].strip()
                    if funcs:
                        plan["FUNCTIONS"].extend([f.strip() for f in funcs.split(',') if f.strip()])
            elif current_section and line.strip():
                # Continue reading items for current section
                items = [item.strip() for item in line.split(',') if item.strip()]
                plan[current_section].extend(items)
        
        return plan
    
    def _validate_exploration_plan(self, plan: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate and sanitize exploration plan.
        
        Args:
            plan: Exploration plan dict
            
        Returns:
            Validated and sanitized plan
        """
        validated = {
            "SEARCH": [],
            "FILES": [],
            "TRACE": [],
            "FUNCTIONS": []
        }
        
        # Validate and filter search terms
        for term in plan.get("SEARCH", []):
            if term and isinstance(term, str):
                term = term.strip()
                # Check length (2-100 chars)
                if 2 <= len(term) <= 100:
                    # Remove duplicates
                    if term not in validated["SEARCH"]:
                        validated["SEARCH"].append(term)
                else:
                    logger.warning(f"Invalid search term length: '{term[:50]}...' (length: {len(term)})")
        
        # Validate file paths exist
        for file_path in plan.get("FILES", []):
            if file_path and isinstance(file_path, str):
                file_path = file_path.strip()
                # Try to read file to validate it exists
                file_data = self.code_reader.read_file(file_path, max_lines=1)
                if file_data.get("success"):
                    if file_path not in validated["FILES"]:
                        validated["FILES"].append(file_path)
                else:
                    logger.warning(f"Invalid file in plan: {file_path} - {file_data.get('error', 'not found')}")
        
        # Validate trace descriptions
        for trace in plan.get("TRACE", []):
            if trace and isinstance(trace, str):
                trace = trace.strip()
                # Check if trace has valid format (contains arrow or "to")
                if ('→' in trace or '->' in trace or ' to ' in trace) and len(trace) <= 200:
                    if trace not in validated["TRACE"]:
                        validated["TRACE"].append(trace)
                else:
                    logger.warning(f"Invalid trace format: '{trace[:50]}...'")
        
        # Validate function names
        for func_name in plan.get("FUNCTIONS", []):
            if func_name and isinstance(func_name, str):
                func_name = func_name.strip()
                # Check if valid identifier (alphanumeric, underscore, dot)
                if re.match(r'^[a-zA-Z_][a-zA-Z0-9_.]*$', func_name) and len(func_name) <= 100:
                    if func_name not in validated["FUNCTIONS"]:
                        validated["FUNCTIONS"].append(func_name)
                else:
                    logger.warning(f"Invalid function name: '{func_name}'")
        
        logger.info(
            f"Plan validation: {len(validated['SEARCH'])} searches, "
            f"{len(validated['FILES'])} files, {len(validated['TRACE'])} traces, "
            f"{len(validated['FUNCTIONS'])} functions"
        )
        
        return validated
    
    def _validate_file_path(self, file_path: str) -> bool:
        """
        Validate file path before operations.
        
        Args:
            file_path: File path to validate
            
        Returns:
            True if path is valid, False otherwise
        """
        if not file_path or not isinstance(file_path, str):
            return False
        
        # Normalize path
        file_path = file_path.strip().replace('\\', '/')
        
        # Check for path traversal attempts
        if '..' in file_path or file_path.startswith('/'):
            logger.warning(f"Potential path traversal attempt: {file_path}")
            return False
        
        # Check if path is within repo root
        try:
            full_path = self.code_reader.repo_root / file_path
            resolved = full_path.resolve()
            repo_root_resolved = self.code_reader.repo_root.resolve()
            
            if not str(resolved).startswith(str(repo_root_resolved)):
                logger.warning(f"Path outside repo root: {file_path}")
                return False
        except Exception as e:
            logger.warning(f"Path validation error for {file_path}: {e}")
            return False
        
        return True
    
    def _estimate_tokens(self, text: str) -> int:
        """
        Estimate token count for text.
        Rough estimation: ~4 characters per token (conservative).
        
        Args:
            text: Text to estimate tokens for
            
        Returns:
            Estimated token count
        """
        if not text:
            return 0
        # Conservative estimate: 4 chars per token
        # More accurate would be using tiktoken, but this is fast and good enough
        return len(text) // 4
    
    def _truncate_findings(self, findings: List[Dict], max_tokens: int = None) -> List[Dict]:
        """
        Truncate findings to fit within token budget (disabled for local processing).
        Prioritizes high-score results and important finding types.
        
        Args:
            findings: List of exploration result dicts
            max_tokens: Maximum tokens to allow (None = no limit for local processing)
            
        Returns:
            Truncated list of findings (or all if max_tokens is None)
        """
        if not findings:
            return []
        
        # If no limit specified, return all findings (local processing)
        if max_tokens is None or max_tokens <= 0:
            return findings
        
        # Sort by priority: file > trace > search > function_call > other
        type_priority = {
            "file": 4,
            "trace": 3,
            "search": 2,
            "function_call": 2,
            "grep": 1,
            "unknown": 0
        }
        
        # Sort findings by priority and score
        def get_priority(finding: Dict) -> tuple:
            finding_type = finding.get("type", "unknown")
            priority = type_priority.get(finding_type, 0)
            score = finding.get("score", 0)
            return (-priority, -score)  # Negative for descending sort
        
        sorted_findings = sorted(findings, key=get_priority)
        
        # Truncate based on token budget
        truncated = []
        total_tokens = 0
        
        for finding in sorted_findings:
            # Estimate tokens for this finding
            finding_text = str(finding)
            tokens = self._estimate_tokens(finding_text)
            
            if total_tokens + tokens > max_tokens:
                logger.info(f"Truncating findings at {len(truncated)}/{len(findings)} (token budget: {max_tokens})")
                break
            
            truncated.append(finding)
            total_tokens += tokens
        
        return truncated
    
    def _deduplicate_findings(self, findings: List[Dict]) -> List[Dict]:
        """
        Remove duplicate findings to improve quality.
        Merges similar findings (same file:line, different context).
        
        Args:
            findings: List of exploration result dicts
            
        Returns:
            Deduplicated list of findings
        """
        if not findings:
            return []
        
        seen = set()
        unique = []
        
        for finding in findings:
            finding_type = finding.get("type", "unknown")
            file_path = finding.get("file") or finding.get("file_path", "unknown")
            line = finding.get("line", 0)
            
            # Create key based on file:line:type
            key = (file_path, line, finding_type)
            
            if key not in seen:
                seen.add(key)
                unique.append(finding)
            else:
                # Same file:line:type - merge contexts if available
                # Find existing finding and merge context
                for existing in unique:
                    if (existing.get("file") or existing.get("file_path")) == file_path and \
                       existing.get("line") == line and \
                       existing.get("type") == finding_type:
                        # Merge context if new finding has better context
                        new_context = finding.get("context", "")
                        existing_context = existing.get("context", "")
                        if new_context and len(new_context) > len(existing_context):
                            existing["context"] = new_context
                        break
        
        logger.debug(f"Deduplication: {len(findings)} -> {len(unique)} findings")
        return unique
    
    def _format_findings(self, findings: List[Dict]) -> str:
        """
        Format exploration results for LLM consumption.
        
        Args:
            findings: List of exploration result dicts
            
        Returns:
            Formatted string with findings
        """
        if not findings:
            return "No findings from exploration."
        
        formatted = []
        formatted.append("=== EXPLORATION FINDINGS ===\n")
        
        for i, finding in enumerate(findings, 1):
            finding_type = finding.get("type", "unknown")
            
            if finding_type == "file":
                file_path = finding.get("file", "unknown")
                content = finding.get("content", "")
                formatted.append(f"\n[{i}] File: {file_path}")
                formatted.append(f"Content preview:\n{content[:500]}...")
            elif finding_type == "search":
                file_path = finding.get("file", finding.get("file_path", "unknown"))
                line = finding.get("line", 0)
                match = finding.get("match", finding.get("content", ""))
                context = finding.get("context", "")
                formatted.append(f"\n[{i}] Search match in {file_path}:{line}")
                formatted.append(f"Match: {match}")
                if context:
                    formatted.append(f"Context:\n{context}")
            elif finding_type == "trace":
                file_path = finding.get("file", "unknown")
                description = finding.get("description", "")
                code = finding.get("code", "")
                formatted.append(f"\n[{i}] Data flow: {file_path}")
                formatted.append(f"Description: {description}")
                if code:
                    formatted.append(f"Code preview:\n{code[:500]}...")
            elif finding_type == "error":
                # Format errors with suggestions
                error_msg = finding.get("error", "Unknown error")
                command = finding.get("command", {})
                suggestions = finding.get("suggestions", "")
                
                formatted.append(f"\n❌ ERROR [{i}]: {error_msg}")
                if command:
                    cmd_type = command.get('type', 'UNKNOWN')
                    cmd_param = command.get('file', command.get('term', command.get('pattern', '')))
                    formatted.append(f"   Command: {cmd_type} {cmd_param}")
                if suggestions:
                    formatted.append(f"   💡 {suggestions}")
            else:
                # Generic finding
                formatted.append(f"\n[{i}] {finding}")
        
        formatted.append("\n=== END FINDINGS ===\n")
        return "\n".join(formatted)
    
    def _parse_agent_commands(self, response: str) -> List[Dict]:
        """
        Parse LLM response for agent commands.
        Supports multiple formats and variations.
        
        Args:
            response: LLM response text
            
        Returns:
            List of command dicts with type and parameters
        """
        commands = []
        
        # Validate input
        if not response or not isinstance(response, str):
            return commands
        
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            line_upper = line.upper()
            
            # SEARCH command - multiple formats
            if line_upper.startswith('SEARCH '):
                term = line[7:].strip()
                # Remove quotes if present
                term = term.strip('"\'')
                if term:
                    commands.append({"type": "SEARCH", "term": term})
            elif 'SEARCH:' in line_upper or 'SEARCH -' in line_upper:
                # Handle "SEARCH: term" or "SEARCH - term"
                term = line.split(':', 1)[-1].split('-', 1)[-1].strip().strip('"\'')
                if term:
                    commands.append({"type": "SEARCH", "term": term})
            
            # READ command - multiple formats
            elif line_upper.startswith('READ '):
                file_path = line[5:].strip()
                # Remove quotes if present
                file_path = file_path.strip('"\'')
                # Handle "READ: file" format
                if ':' in file_path:
                    file_path = file_path.split(':', 1)[-1].strip()
                if file_path:
                    commands.append({"type": "READ", "file": file_path})
            elif line_upper.startswith('READ FILE ') or line_upper.startswith('READ FILE:'):
                file_path = line.split(':', 1)[-1].split('FILE', 1)[-1].strip().strip('"\'')
                if file_path:
                    commands.append({"type": "READ", "file": file_path})
            
            # TRACE command
            elif line_upper.startswith('TRACE '):
                trace = line[6:].strip()
                if '→' in trace or '->' in trace or ' to ' in trace or ' TO ' in trace:
                    # Parse start and end
                    if '→' in trace:
                        parts = trace.split('→', 1)
                    elif '->' in trace:
                        parts = trace.split('->', 1)
                    elif ' TO ' in trace:
                        parts = trace.split(' TO ', 1)
                    else:
                        parts = trace.split(' to ', 1)
                    
                    start = parts[0].strip().strip('"\'') if len(parts) > 0 else ""
                    end = parts[1].strip().strip('"\'') if len(parts) > 1 else ""
                    
                    # Only add TRACE command if both start and end are present
                    if start and end:
                        commands.append({
                            "type": "TRACE",
                            "start": start,
                            "end": end
                        })
            
            # GREP command
            elif line_upper.startswith('GREP '):
                pattern = line[5:].strip().strip('"\'')
                if pattern:
                    commands.append({"type": "GREP", "pattern": pattern})
            elif 'GREP:' in line_upper or 'GREP -' in line_upper:
                pattern = line.split(':', 1)[-1].split('-', 1)[-1].strip().strip('"\'')
                if pattern:
                    commands.append({"type": "GREP", "pattern": pattern})
            
            # ANALYZE command
            elif line_upper.startswith('ANALYZE') or line_upper == 'ANALYZE':
                commands.append({"type": "ANALYZE"})
        
        return commands
    
    def _search_mode_with_metadata(
        self,
        message: str,
        question_analysis: Dict[str, Any],
        conversation_history: Optional[List[Dict[str, str]]],
        progress_callback: Optional[Callable[[str, int, int], None]] = None
    ) -> Tuple[str, Dict[str, Any]]:
        """
        Search mode: comprehensive information gathering workflow with metadata.
        
        Returns tuple of (response_text, metadata_dict)
        """
        # Pass progress callback through
        response_text = self._search_mode(
            message,
            question_analysis,
            conversation_history,
            progress_callback
        )
        
        # Extract metadata from exploration (we'll track this during execution)
        metadata = getattr(self, '_last_exploration_metadata', {
            "mode": "single_turn",
            "files_searched": 0,
            "files_read": [],
            "searches_executed": 0,
            "search_terms": [],
            "findings_count": 0,
            "exploration_time": 0,
            "timed_out": False
        })
        
        return response_text, metadata
    
    def _search_mode(
        self,
        message: str,
        question_analysis: Dict[str, Any],
        conversation_history: Optional[List[Dict[str, str]]],
        progress_callback: Optional[Callable[[str, int, int], None]] = None
    ) -> str:
        """
        Search mode: Comprehensive information gathering workflow.
        
        Step 1: Understand what information is needed and generate search plan
        Step 2: Execute comprehensive searches (search, trace, read files)
        Step 3: Collect and organize all findings
        Step 4: Generate comprehensive summary of what was found
        
        Focus: Information discovery and collection, not problem-solving.
        
        Args:
            message: User's information request
            question_analysis: Question classification results
            conversation_history: Previous conversation messages
            
        Returns:
            Comprehensive information summary with code references
        """
        logger.info("Starting Search mode workflow")
        config = get_config()
        exploration_start_time = time.time()
        exploration_timeout = config.exploration_timeout
        
        # Initialize exploration session
        self._last_exploration_session_id = str(uuid.uuid4())
        try:
            from backend.services.exploration_history import save_exploration_session
            save_exploration_session(
                session_id=self._last_exploration_session_id,
                user_message=message,
                response_text=None,  # Will be updated later
                exploration_metadata={},  # Will be updated later
                findings=[],  # Will be updated later
                request_id=getattr(self, '_current_request_id', None)
            )
        except Exception as e:
            logger.warning(f"Failed to create exploration session: {e}")
        
        if progress_callback:
            progress_callback("Generating exploration plan...", 0, 4)
        
        # Step 1: Generate search plan using LLM
        search_plan_prompt = f"""You are a code information gatherer. The user wants to find information about: "{message}"

Generate a comprehensive search plan to gather ALL relevant information. Think broadly - what should we search for? What files might contain relevant code? What functions or data flows are related?

Your goal is to find and collect information, not to solve problems.

Respond in this format:
SEARCH: [list of search terms, comma-separated - be comprehensive]
FILES: [list of file paths to check, comma-separated]
TRACE: [data flows to trace, e.g., "upload → normalize → display"]
FUNCTIONS: [function names to find, comma-separated]

Be thorough and comprehensive - cast a wide net to gather all relevant information."""
        
        # Get search plan from LLM
        try:
            plan_text = self._call_ollama_simple(search_plan_prompt, temperature=0.3)
            logger.info(f"Exploration plan generated: {plan_text[:200]}...")
        except Exception as e:
            logger.warning(f"Failed to generate exploration plan: {e}, using heuristics")
            # Fallback to heuristic-based suggestions
            plan_text = ""
            suggestions = self.code_explorer.suggest_exploration_path(message)
            plan = {
                "SEARCH": [message],
                "FILES": suggestions,  # No limit (local processing)
                "TRACE": [],
                "FUNCTIONS": []
            }
        else:
            # Parse plan
            plan = self._parse_exploration_plan(plan_text)
        
        # Validate and sanitize exploration plan
        plan = self._validate_exploration_plan(plan)
        
        # Check if plan has any valid actions
        total_actions = (
            len(plan.get("SEARCH", [])) + 
            len(plan.get("FILES", [])) + 
            len(plan.get("TRACE", [])) + 
            len(plan.get("FUNCTIONS", []))
        )
        if total_actions == 0:
            logger.warning("Plan validation removed all items, using heuristic fallback")
            suggestions = self.code_explorer.suggest_exploration_path(message)
            plan = {
                "SEARCH": [message] if message else ["code", "function"],
                "FILES": suggestions[:config.max_files_per_plan],
                "TRACE": [],
                "FUNCTIONS": []
            }
            # Re-validate the fallback plan
            plan = self._validate_exploration_plan(plan)
            total_actions = (
                len(plan.get("SEARCH", [])) + 
                len(plan.get("FILES", [])) + 
                len(plan.get("TRACE", [])) + 
                len(plan.get("FUNCTIONS", []))
            )
            if total_actions == 0:
                logger.error("Even heuristic fallback produced empty plan")
                return (
                    "[Exploration Mode] Unable to generate exploration plan.\n\n"
                    "**Issue:** No valid files or search terms could be determined from your question.\n\n"
                    "**Suggestions:**\n"
                    "1. Include specific file names (e.g., 'in upload.ts')\n"
                    "2. Mention specific functions or features\n"
                    "3. Use more concrete terms related to the problem\n"
                    "4. Try rephrasing with technical keywords"
                )
        
        # Check circuit breaker before starting
        ollama_cb = self.retry_handler.circuit_breakers.get("ollama_api")
        if ollama_cb and not ollama_cb.can_attempt():
            logger.warning("Circuit breaker is open for ollama_api, skipping exploration")
            return (
                "[Exploration Mode] Ollama API is temporarily unavailable (circuit breaker open).\n\n"
                "**Status:** Too many recent failures detected. System is protecting itself.\n\n"
                "**Recovery Actions:**\n"
                "1. Wait 60 seconds and try again\n"
                "2. Check if Ollama is running: `ollama serve`\n"
                "3. Verify Ollama is accessible at the configured URL\n"
                "4. Try a more specific question that doesn't require exploration"
            )
        
        if progress_callback:
            progress_callback("Plan validated, starting exploration...", 1, 4)
        
        # Check timeout before starting exploration
        if time.time() - exploration_start_time > exploration_timeout:
            logger.warning("Exploration timeout before starting execution")
            return (
                "[Exploration Mode] Exploration timed out during planning phase.\n\n"
                "**Issue:** Planning took too long (>2 minutes).\n\n"
                "**Suggestions:**\n"
                "1. Try a more specific question\n"
                "2. Include file paths or function names\n"
                "3. Break down complex questions into smaller parts"
            )
        
        # Step 2: Execute exploration
        findings = []
        files_searched = 0
        searches_executed = 0
        files_read = []
        search_terms_used = []
        
        # Execute searches in parallel with per-step timeout
        max_searches = config.max_files_per_plan  # Reuse max_files for searches too
        search_terms = plan.get("SEARCH", [])[:max_searches]
        total_searches = len(search_terms)
        
        if total_searches > 0:
            if progress_callback:
                progress_callback(f"Starting {total_searches} parallel searches...", 1, 4)
            
            # Define search function for parallel execution
            def execute_search(search_term: str) -> Tuple[str, List[Dict], Optional[str]]:
                """Execute a single search and return (term, results, error)."""
                try:
                    if time.time() - exploration_start_time > exploration_timeout:
                        return (search_term, [], "timeout")
                    
                    search_start = time.time()
                    results = self.code_explorer.search_concept(search_term, max_results=config.max_search_results)
                    search_time = time.time() - search_start
                    logger.info(f"Search '{search_term}' completed in {search_time:.2f}s, found {len(results)} results")
                    return (search_term, results, None)
                except Exception as e:
                    logger.warning(f"Search failed for '{search_term}': {e}")
                    return (search_term, [], str(e))
            
            # Execute searches in parallel (cap max_workers to avoid context switching)
            completed_searches = 0
            max_workers = min(config.max_parallel_searches, total_searches, min(8, os.cpu_count() or 4))
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit all search tasks
                future_to_term = {
                    executor.submit(execute_search, term): term 
                    for term in search_terms
                }
                
                # Process results as they complete
                for future in as_completed(future_to_term):
                    if time.time() - exploration_start_time > exploration_timeout:
                        logger.warning(f"Exploration timeout during parallel searches")
                        # Cancel remaining tasks
                        for f in future_to_term:
                            f.cancel()
                        findings.append({
                            "type": "timeout",
                            "file": "exploration",
                            "line": 0,
                            "match": f"Exploration timed out after {searches_executed} searches",
                            "context": "Exploration exceeded time limit. Partial results may be incomplete."
                        })
                        break
                    
                    try:
                        search_term, results, error = future.result()
                        completed_searches += 1
                        
                        # Update progress
                        if progress_callback:
                            progress_callback(
                                f"Searching... ({completed_searches}/{total_searches} complete)",
                                1,
                                4
                            )
                        
                        if error:
                            logger.warning(f"Search '{search_term}' failed: {error}")
                        else:
                            search_terms_used.append(search_term)
                            searches_executed += 1
                            for result in results:
                                findings.append({
                                    "type": "search",
                                    "file": result.get("file"),
                                    "line": result.get("line"),
                                    "match": result.get("match"),
                                    "context": result.get("context")
                                })
                    except Exception as e:
                        search_term = future_to_term[future]
                        logger.error(f"Unexpected error in search '{search_term}': {e}")
            
            logger.info(f"Parallel search execution completed: {searches_executed}/{total_searches} successful")
        
        # Extract unique file paths from search results to read them
        files_from_searches = set()
        for finding in findings:
            if finding.get("type") == "search" and finding.get("file"):
                file_path = finding.get("file")
                if self._validate_file_path(file_path):
                    files_from_searches.add(file_path)
                    # Limit files from searches to avoid reading too many
                    if len(files_from_searches) >= config.max_files_per_plan:
                        break
        
        # Combine plan files with files found in searches (prioritize plan files)
        plan_files = plan.get("FILES", [])[:config.max_files_per_plan]
        # Merge: plan files first, then search files (up to max limit)
        all_file_paths = list(plan_files)
        remaining_slots = config.max_files_per_plan - len(all_file_paths)
        if remaining_slots > 0:
            search_files_list = list(files_from_searches)[:remaining_slots]
            # Add search files that aren't already in plan
            for search_file in search_files_list:
                if search_file not in all_file_paths:
                    all_file_paths.append(search_file)
        
        total_files = len(all_file_paths)
        logger.info(f"Files to read: {len(plan_files)} from plan, {len(files_from_searches)} from searches, {total_files} total")
        
        if total_files > 0:
            if progress_callback:
                progress_callback(f"Reading {total_files} files in parallel...", 2, 4)
            
            # Define file read function for parallel execution
            def execute_file_read(file_path: str) -> Tuple[str, Optional[Dict], Optional[str]]:
                """Execute a single file read and return (path, file_data, error)."""
                try:
                    if time.time() - exploration_start_time > exploration_timeout:
                        return (file_path, None, "timeout")
                    
                    # Validate path before reading
                    if not self._validate_file_path(file_path):
                        return (file_path, None, "invalid_path")
                    
                    file_start = time.time()
                    file_data = self.code_reader.read_file(file_path, max_lines=config.max_lines_per_file)
                    file_time = time.time() - file_start
                    logger.info(f"Read file '{file_path}' in {file_time:.2f}s")
                    return (file_path, file_data, None)
                except Exception as e:
                    logger.warning(f"Error reading file '{file_path}': {e}")
                    return (file_path, None, str(e))
            
            # Execute file reads in parallel (cap max_workers to avoid context switching)
            completed_reads = 0
            max_workers = min(config.max_parallel_file_reads, total_files, min(8, os.cpu_count() or 4))
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit all file read tasks
                future_to_path = {
                    executor.submit(execute_file_read, path): path 
                    for path in all_file_paths
                }
                
                # Process results as they complete
                for future in as_completed(future_to_path):
                    if time.time() - exploration_start_time > exploration_timeout:
                        logger.warning(f"Exploration timeout during parallel file reads")
                        # Cancel remaining tasks
                        for f in future_to_path:
                            f.cancel()
                        findings.append({
                            "type": "timeout",
                            "file": "exploration",
                            "line": 0,
                            "match": f"Exploration timed out after reading {files_searched} files",
                            "context": "Exploration exceeded time limit. Partial results may be incomplete."
                        })
                        break
                    
                    try:
                        file_path, file_data, error = future.result()
                        completed_reads += 1
                        
                        # Update progress
                        if progress_callback:
                            progress_callback(
                                f"Reading files... ({completed_reads}/{total_files} complete)",
                                2,
                                4
                            )
                        
                        if error == "timeout":
                            break
                        elif error == "invalid_path":
                            logger.warning(f"Invalid file path: {file_path}")
                            continue
                        elif error:
                            logger.warning(f"File read failed for '{file_path}': {error}")
                            continue
                        
                        files_searched += 1
                        if file_data and file_data.get("success"):
                            files_read.append(file_path)
                            findings.append({
                                "type": "file",
                                "file": file_path,
                                "line": 0,
                                "match": file_data.get("content", "")[:200],  # First 200 chars
                                "context": file_data.get("content", "")
                            })
                    except Exception as e:
                        file_path = future_to_path[future]
                        logger.error(f"Unexpected error reading file '{file_path}': {e}")
            
            logger.info(f"Parallel file read execution completed: {files_searched}/{total_files} successful")
        
        # Execute traces
        for trace_desc in plan.get("TRACE", [])[:config.max_traces]:
            try:
                # Parse trace (e.g., "upload → normalize → display")
                if '→' in trace_desc or '->' in trace_desc or ' to ' in trace_desc:
                    if '→' in trace_desc:
                        parts = trace_desc.split('→')
                    elif '->' in trace_desc:
                        parts = trace_desc.split('->')
                    else:
                        parts = trace_desc.split(' to ')
                    
                    if len(parts) >= 2:
                        start = parts[0].strip()
                        end = parts[-1].strip()
                        trace_results = self.code_explorer.trace_data_flow(start, end)
                        findings.extend([{"type": "trace", **r} for r in trace_results])
            except Exception as e:
                logger.warning(f"Trace failed for '{trace_desc}': {e}")
        
        # Find function calls
        for func_name in plan.get("FUNCTIONS", [])[:config.max_functions]:
            try:
                calls = self.code_explorer.find_function_calls(func_name)
                for call in calls:
                    findings.append({
                        "type": "function_call",
                        "file": call.get("file"),
                        "line": call.get("line"),
                        "context": call.get("context")
                    })
            except Exception as e:
                logger.warning(f"Function call search failed for '{func_name}': {e}")
        
        exploration_time = time.time() - exploration_start_time
        timed_out = exploration_time >= exploration_timeout
        
        # Store metadata for retrieval
        self._last_exploration_metadata = {
            "mode": "single_turn",
            "files_searched": files_searched,
            "files_read": files_read,  # No limit (local processing)
            "searches_executed": searches_executed,
            "search_terms": search_terms_used,  # No limit (local processing)
            "findings_count": len(findings),
            "exploration_time": round(exploration_time, 2),
            "timed_out": timed_out,
            "success": True,
            "model_used": getattr(self, 'model', None)
        }
        
        logger.info(
            f"Exploration completed in {exploration_time:.2f}s: "
            f"{len(findings)} findings, {searches_executed} searches, {files_searched} files read"
        )
        
        # Handle empty findings gracefully
        if not findings:
            # Safety check: total_actions might not be defined if early return occurred
            plan_actions_info = f"- Plan actions: {total_actions}\n" if 'total_actions' in locals() else ""
            helpful_msg = (
                f"[Exploration Mode] No code found matching: '{message}'\n\n"
                f"**Exploration Summary:**\n"
                f"- Searches executed: {searches_executed}\n"
                f"- Files checked: {files_searched}\n"
                f"- Time taken: {exploration_time:.2f}s\n"
                f"{plan_actions_info}\n"
                f"**Why this might happen:**\n"
                f"1. The code you're looking for might not exist\n"
                f"2. Search terms might be too vague or incorrect\n"
                f"3. Files might be in a different location than expected\n\n"
                f"**Suggestions:**\n"
                f"1. Try rephrasing with specific file/function names (e.g., 'in upload.ts')\n"
                f"2. Use more specific search terms related to the feature\n"
                f"3. Check if the feature exists in the codebase\n"
                f"4. Provide file paths if you know them\n"
                f"5. Try breaking down the question into smaller parts"
            )
            
            # Log metrics for empty findings
            try:
                metrics = get_metrics()
                request_id = str(uuid.uuid4())[:8]
                metrics.log_exploration(
                    request_id=request_id,
                    message=message,
                    findings_count=0,
                    searches_executed=searches_executed,
                    files_read=files_searched,
                    traces_executed=len(plan.get("TRACE", [])),
                    exploration_time=exploration_time,
                    success=False,
                    timeout=timed_out
                )
            except Exception as e:
                logger.warning(f"Failed to log exploration metrics: {e}")
            
            return helpful_msg
        
        # Log exploration metrics (success case)
        try:
            metrics = get_metrics()
            request_id = str(uuid.uuid4())[:8]
            metrics.log_exploration(
                request_id=request_id,
                message=message,
                findings_count=len(findings),
                searches_executed=searches_executed,
                files_read=files_searched,
                traces_executed=len(plan.get("TRACE", [])),
                exploration_time=exploration_time,
                success=True,
                timeout=timed_out
            )
        except Exception as e:
            logger.warning(f"Failed to log exploration metrics: {e}")
        
        if progress_callback:
            progress_callback(f"Processing {len(findings)} findings...", 2, 4)
        
        # Deduplicate findings
        findings = self._deduplicate_findings(findings)
        logger.info(f"After deduplication: {len(findings)} findings")
        
        # Truncate findings by count first (max_findings = 300)
        if len(findings) > config.max_findings:
            logger.info(f"Truncating findings from {len(findings)} to {config.max_findings} by count")
            # Sort by priority before truncating
            type_priority = {"file": 4, "trace": 3, "search": 2, "function_call": 2, "grep": 1, "unknown": 0}
            findings.sort(key=lambda f: (-type_priority.get(f.get("type", "unknown"), 0), -f.get("score", 0)))
            findings = findings[:config.max_findings]
        
        # Then truncate by tokens if limit is set (disabled for local processing)
        if config.max_findings_tokens > 0:
            findings = self._truncate_findings(findings, max_tokens=config.max_findings_tokens)
        else:
            findings = self._truncate_findings(findings, max_tokens=None)  # No limit
        logger.info(f"After truncation: {len(findings)} findings")
        
        if progress_callback:
            progress_callback("Analyzing findings with LLM...", 3, 4)
        
        # Step 3: Analyze findings with LLM
        findings_text = self._format_findings(findings)
        
        if progress_callback:
            progress_callback("Generating response...", 4, 4)
        
        # Update exploration session with response after analysis
        if hasattr(self, '_last_exploration_session_id'):
            try:
                from backend.services.exploration_history import save_exploration_session
                save_exploration_session(
                    session_id=self._last_exploration_session_id,
                    user_message=message,
                    response_text=None,  # Will update after we get response
                    exploration_metadata=self._last_exploration_metadata,
                    findings=findings,
                    request_id=getattr(self, '_current_request_id', None)
                )
            except Exception as e:
                logger.warning(f"Failed to update exploration session: {e}")
        
        # Search mode: Information gathering and summary (not problem-solving)
        summary_prompt = f"""You are a code information gatherer. The user asked: "{message}"

You have collected comprehensive information from the codebase:

{findings_text}

Your task is to organize and summarize what you found. Focus on INFORMATION, not problem-solving.

Provide a comprehensive summary:

1. **Information Overview**: What did you find related to the user's query?
2. **Files Found**: List all relevant files with brief descriptions
3. **Key Components**: What functions, classes, or modules are involved?
4. **Data Flows**: How does data move through the system (if relevant)?
5. **Code Locations**: Where is the relevant code located (file:line references)?
6. **Runtime Verification Points**: Where would you check logs/database/API to verify behavior?

Format:
**Summary**
[Brief overview of what information was found]

**Files Discovered**
- `file1.ts` (lines X-Y): [description]
- `file2.py` (lines A-B): [description]

**Key Components**
- Function/Class names and their purposes

**Code Snippets** (3-5 lines max per snippet, with file:line references)
[Small snippets to illustrate key points]

**Execution Path** (if relevant)
[How data flows: file1.py:123 → file2.ts:456 → file3.py:789]

**Runtime Verification Points**
[Where to check logs/database/API to verify actual behavior]

**Additional Information**
[Any other relevant details found]

Remember: You are providing INFORMATION, not solving problems. Be comprehensive and organized."""
        
        # Get summary from LLM
        try:
            summary = self._call_ollama_simple(summary_prompt, temperature=0.2)
            logger.info("Search mode summary completed")
            
            if progress_callback:
                progress_callback("Summary complete!", 4, 4)
            
            # Add search mode indicator
            summary = f"[Search Mode] {summary}"
            
            # Update exploration session with final response
            if hasattr(self, '_last_exploration_session_id'):
                try:
                    save_exploration_session(
                        session_id=self._last_exploration_session_id,
                        user_message=message,
                        response_text=summary,
                        exploration_metadata=self._last_exploration_metadata,
                        findings=findings,
                        request_id=getattr(self, '_current_request_id', None)
                    )
                except Exception as e:
                    logger.warning(f"Failed to update exploration session with response: {e}")
            
            return summary
        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            logger.error(f"Agent analysis failed: {e}\n{error_trace}")
            # Fallback to formatted findings with better error message
            error_type = type(e).__name__
            is_timeout = "timeout" in str(e).lower() or "timed out" in str(e).lower()
            is_connection = "connection" in str(e).lower() or "connect" in str(e).lower()
            
            if is_timeout:
                error_context = (
                    "**Error Type:** Request Timeout\n"
                    "**Issue:** LLM took too long to analyze the findings.\n"
                    "**Possible Causes:**\n"
                    "- Too many findings to process\n"
                    "- LLM model is slow or overloaded\n"
                    "- Network latency\n\n"
                )
            elif is_connection:
                error_context = (
                    "**Error Type:** Connection Error\n"
                    "**Issue:** Cannot connect to Ollama service.\n"
                    "**Possible Causes:**\n"
                    "- Ollama is not running\n"
                    "- Network connectivity issues\n"
                    "- Ollama service crashed\n\n"
                )
            else:
                error_context = (
                    f"**Error Type:** {error_type}\n"
                    f"**Issue:** LLM analysis encountered an error.\n"
                    f"**Error Details:** {str(e)}\n\n"
                )
            
            error_msg = (
                f"[Exploration Mode] Exploration completed but LLM analysis failed.\n\n"
                f"{error_context}"
                f"**Recovery Actions:**\n"
                f"1. Review the findings below manually\n"
                f"2. Try rephrasing your question\n"
                f"3. Check Ollama service status\n"
                f"4. Retry the request\n\n"
                f"**Exploration Results:**\n"
                f"- Found {len(findings)} relevant code sections\n"
                f"- Searched {searches_executed} terms\n"
                f"- Read {files_searched} files\n\n"
                f"**Findings:**\n{findings_text}"
            )
            return error_msg
    
    def _call_ollama_simple(self, prompt: str, temperature: float = 0.7) -> str:
        """
        Simple LLM call without full conversation context.
        Used for exploration planning and analysis.
        
        Args:
            prompt: Prompt to send
            temperature: Temperature for generation
            
        Returns:
            LLM response text
        """
        if not self.ollama_available:
            raise Exception("Ollama not available")
        
        messages = [{"role": "user", "content": prompt}]
        
        def make_request():
            response = requests.post(
                f"{self.ollama_url}/api/chat",
                json={
                    "model": self.model,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "num_predict": -1,  # -1 = unlimited output (local processing)
                        "num_ctx": 8000,
                    }
                },
                timeout=60
            )
            
            if response.status_code != 200:
                raise Exception(f"Ollama API returned status {response.status_code}")
            
            result = response.json()
            return result.get("message", {}).get("content", "")
        
        # Use retry handler with circuit breaker
        config = get_config()
        try:
            return self.retry_handler.retry_with_backoff(
                make_request,
                max_retries=config.max_retries,
                initial_delay=config.retry_initial_delay,
                max_delay=config.retry_max_delay,
                retry_on=(requests.exceptions.Timeout, requests.exceptions.ConnectionError, Exception),
                circuit_breaker_key="ollama_api"
            )
        except Exception as e:
            logger.error(f"Simple Ollama call failed after retries: {e}")
            raise
    
    def _calculate_agent_timeout(
        self,
        context_size: Optional[int],
        files_read: int,
        turns: int,
        max_turns: int
    ) -> Tuple[int, int]:
        """
        Calculate adaptive timeout based on context size and progress.
        
        Returns:
            Tuple of (overall_timeout, per_turn_timeout) in seconds
        """
        # Base timeout by context size (aggressive for performance - target <8 min)
        base_timeouts = {
            100000: 480,   # 8 min for 100k+ (reduced from 20 min)
            64000: 420,    # 7 min for 64k (reduced from 15 min)
            32000: 360,    # 6 min for 32k (reduced from 10 min)
            16000: 300,    # 5 min for 16k (reduced from 7.5 min)
            10000: 300,    # 5 min for 10k (reduced from 10 min)
        }
        
        timeout = 480  # Default 8 min (reduced from 10 min)
        if context_size:
            for size, t in sorted(base_timeouts.items(), reverse=True):
                if context_size >= size:
                    timeout = t
                    break
        
        # Extend timeout by 25% if making good progress (reduced from 50%)
        if files_read >= 3 and turns < 5:
            timeout = int(timeout * 1.25)
            logger.info(f"Extended timeout by 25% due to good progress (files_read={files_read}, turns={turns})")
        
        # Per-turn timeout (aggressive for fast first turn - target <60s)
        per_turn_timeout = 60  # 1 minute per turn (target)
        
        # Adjust based on files read (more files = more time needed)
        if files_read > 10:
            per_turn_timeout += 20  # Add 20s for large file sets
        elif files_read > 5:
            per_turn_timeout += 10  # Add 10s for medium file sets
        
        # Cap per-turn timeout
        per_turn_timeout = min(per_turn_timeout, 90)  # Max 90s per turn
        
        return timeout, per_turn_timeout
    
    def _agent_mode_conversation(
        self,
        message: str,
        max_turns: Optional[int] = None,
        progress_callback: Optional[Callable[[str, int, int], None]] = None,
        context_size: Optional[int] = None
    ) -> str:
        """
        Agent mode: autonomous problem-solving conversation where LLM executes tasks sequentially.
        
        The agent breaks down problems into tasks, executes them one by one, analyzes results,
        and decides next steps until a solution is found.
        
        Args:
            message: Initial user question/problem
            max_turns: Maximum number of conversation turns
            
        Returns:
            Final analysis response
        """
        config = get_config()
        max_turns = max_turns or config.max_conversation_turns or 10  # Default to 10 if both are None/0
        if max_turns <= 0:
            max_turns = 10  # Safety fallback
        logger.info(f"Starting Agent mode conversation (max {max_turns} turns)")
        
        # Performance tracking
        agent_start_time = time.time()
        build_plan_start = time.time()
        
        if progress_callback:
            progress_callback("Starting agent exploration...", 1, 4)
        
        explorer = self.code_explorer
        conversation_context = []
        
        # Auto-detect architecture from files that will likely be read
        # Use the same helper method as _build_prompt for consistency
        code_context_for_detection = {"files": []}
        architecture_context = self._get_framework_context(code_context_for_detection, message)
        
        # Log build_plan phase
        build_plan_ms = int((time.time() - build_plan_start) * 1000)
        self._log_perf_metric("build_plan", ms=build_plan_ms)
        
        # Fast first turn guard: if plan creation exceeds 3s, defer heavy searches
        if build_plan_ms > 3000:
            logger.warning(f"Plan creation took {build_plan_ms}ms (>3s), deferring heavy searches to next turn")
            if progress_callback:
                progress_callback("Plan created, starting with quick exploration...", 1, 4)
        
        # Initial system prompt for Agent mode (problem-solving with runtime verification)
        system_prompt = f"""You are an autonomous problem-solving agent. Your task is to solve: "{message}"{architecture_context}

YOUR MISSION: Diagnose the problem by READING THE CODEBASE FIRST, then verifying runtime behavior.

CRITICAL WORKFLOW - READ CODE FIRST:
1. READ THE CODEBASE - Use READ, GREP, SEARCH to understand actual implementation
2. VERIFY EXISTING CODE - Check what logging/functions already exist
3. TRACE ACTUAL EXECUTION PATH - Follow real code flow, not assumptions
4. IDENTIFY SPECIFIC ISSUES - Find mismatches, inconsistencies, bugs
5. VERIFY RUNTIME BEHAVIOR - Ask for logs, database queries, API responses
6. PROVIDE SPECIFIC FIXES - Based on actual code, not generic suggestions

MANDATORY: READ CODE BEFORE SUGGESTING
Before making ANY suggestions, you MUST:
1. READ actual files mentioned in the problem
2. USE GREP to find function definitions and calls
3. USE SEARCH to understand relationships and data flow
4. CHECK what logging/functions already exist
5. TRACE the actual execution path through real code
6. IDENTIFY specific bugs (mismatches, wrong field names, etc.)

NEVER:
- Assume function signatures or names
- Suggest adding logging that might already exist
- Provide generic code examples that don't match the codebase
- Make suggestions without reading the actual code first

AVAILABLE COMMANDS (USE THESE FIRST):
- READ <file> - Read actual code files (DO THIS FIRST)
- GREP <pattern> - Find function definitions, calls, specific code patterns
- SEARCH <term> - Search codebase for concepts and relationships
- TRACE <start> → <end> - Trace data flow through actual code
- ANALYZE - Provide your analysis and solution (ONLY after reading code)

MANDATORY FIRST STEPS:
1. READ the files mentioned in the problem (e.g., READ backend/services/ocr_service.py)
2. GREP for function names to find actual definitions (e.g., GREP insert_line_items)
3. SEARCH for related code to understand the flow
4. TRACE the actual execution path through real code
5. CHECK what already exists (logging, functions, etc.)
6. THEN provide specific fixes based on actual code

CRITICAL RULES:
1. USE ACTUAL FILE PATHS AND LINE NUMBERS:
   - NEVER use generic names like "file.py:100" or "ocr.py:50"
   - ALWAYS use actual paths: "backend/main.py:561" or "backend/services/ocr_service.py:266"
   - Use real line numbers from the code you've read
   - If you don't know exact line numbers, read the file first

2. VERIFY RUNTIME BEHAVIOR FIRST - BE SPECIFIC:
   - Don't say: "Check backend logs"
   - Say: "Search logs for '[LINE_ITEMS]' messages and share: how many blocks, what types, any table blocks?"
   - Don't say: "Check if data is stored"
   - Say: "Run SQL: SELECT supplier, value, (SELECT COUNT(*) FROM invoice_line_items WHERE doc_id = invoices.doc_id) as items FROM invoices ORDER BY id DESC LIMIT 1;"
   - Don't say: "Check API response"
   - Say: "Run: curl http://localhost:8000/api/upload/status?doc_id=XXX and share the response"

3. PRIORITIZE BY LIKELIHOOD:
   - Don't list all possibilities equally
   - Rank by probability: "MOST LIKELY (90%): Table blocks not detected"
   - Then: "If that's not it (5%): Table extraction returns empty"
   - Then: "Least likely (5%): Format conversion fails"

4. PROVIDE EXACT LOGGING CODE:
   - Don't say: "Add logging to verify"
   - Say: "Add: logger.info(f'[TABLE_EXTRACT] Block type: {{ocr_result.type}}') at backend/ocr/owlin_scan_pipeline.py:704"
   - Include the exact code snippet, file path, and line number
   - Use actual variable names from the code

6. TRACE ACTUAL EXECUTION PATH (read code first):
   - READ the actual files to understand the flow
   - Follow: Upload → OCR → Extraction → Storage → API → Frontend
   - Use actual file paths from code you READ: "backend/main.py:561 → backend/services/ocr_service.py:266 → backend/app/db.py:323"
   - Map actual function calls, parameter passing (doc_id, invoice_id, etc.)
   - Check each step with specific queries/logs

7. IDENTIFY SPECIFIC ISSUES (based on actual code):
   - Look for mismatches: invoice_id vs doc_id, wrong field names
   - Check query logic vs insert logic (do they match?)
   - Verify database relationships (how tables connect)
   - Find inconsistencies in actual code, not assumptions

8. FOCUS ON PRIMARY ISSUES:
   - Primary: Why is data empty/missing? (the core problem)
   - Secondary: Error handling improvements (can wait)
   - Don't fix secondary issues while primary is unsolved

9. BE CONCRETE, NOT GENERIC (based on actual code):
   - Instead of: "improve logic to handle more cases"
   - Say: "In backend/services/ocr_service.py:611, add: logger.info(f'Block types found: {{[b.type for b in blocks]}}')" (after checking it doesn't exist)
   - Instead of: "Check if data exists"
   - Say: "Run: SELECT * FROM invoices WHERE id = (SELECT MAX(id) FROM invoices);"

RESPONSE FORMAT:
When you ANALYZE, provide in this EXACT format:

**Code Analysis** (based on actual code you READ):
- **Files Read**: List files you actually read (e.g., READ backend/services/ocr_service.py)
- **Functions Found**: Actual function names from code (e.g., extract_table_from_block, not extract_table)
- **Existing Logging**: What logging already exists (from GREP results)
- **Execution Path**: Actual code flow from files you READ

**Prioritized Diagnosis (by likelihood):**

1. **MOST LIKELY (90%): [Issue name]**
   - **Check**: `backend/path/to/file.py:123-145` (actual file path from code you READ)
   - **Existing Code**: Quote actual code snippet from file (3-5 lines)
   - **Issue Found**: Specific bug/mismatch identified (e.g., "invoice_id = doc_id but query uses invoice_id")
   - **Add logging** (if doesn't exist): `logger.info(f'[TAG] Message: {{variable}}')` at `backend/path/to/file.py:130` (exact code with real variable names from actual code)
   - **Verify**: "Search logs for '[TAG]' messages and share: [specific data points]" (exact log search)
   - **If not this**: Move to next item

2. **If that's not it (5%): [Issue name]**
   - **Check**: `backend/path/to/file.py:200-220` (actual path from code you READ)
   - **Existing Code**: Quote actual code snippet
   - **Issue Found**: Specific bug identified
   - **Add logging** (if doesn't exist): `logger.info(f'[TAG2] Data: {{variable}}')` at `backend/path/to/file.py:210` (exact code)
   - **Verify**: "Run SQL: SELECT ... FROM ... WHERE ...;" (exact query)

3. **Least likely (5%): [Issue name]**
   - **Check**: `backend/path/to/file.py:300-320` (actual path)
   - **Verify**: "Run: curl http://localhost:8000/api/endpoint?param=value" (exact command)

**Execution Path** (actual file paths from code you READ):
`backend/main.py:561` (actual function name) → `backend/services/ocr_service.py:266` (actual function name) → `backend/app/db.py:323` (actual function name)

**Root Cause**: [Based on actual code analysis, not assumptions - e.g., "invoice_id/doc_id mismatch at ocr_service.py:285"]

**Fix**: [Specific code changes with actual file:line references, showing BEFORE/AFTER from actual code]

CRITICAL: 
1. READ actual code files FIRST before making suggestions
2. Use GREP to check what already exists
3. Use ACTUAL file paths, function names, and line numbers from code you READ
4. Identify SPECIFIC bugs (mismatches, wrong field names, etc.) from actual code
5. Provide fixes based on REAL code, not generic examples

Start by READING the codebase files related to the problem."""
        
        user_message = message
        turns = 0
        files_read_tracker = set()  # Track which files have been read
        commands_history = []  # Track all commands used
        discovered_files = set()  # Files discovered from searches but not yet read
        tools_used = False  # Track if any tools have been used
        key_insights = []  # Track key insights learned
        file_cache = {}  # Cache file contents to avoid re-reading
        file_relationships = {}  # Track relationships between files
        task_tracker = TaskTracker()  # Track agent tasks with status and timing
        
        # Limits to prevent unbounded growth
        MAX_FILE_CACHE_SIZE = 50
        MAX_DISCOVERED_FILES = 100
        MAX_COMMANDS_HISTORY = 200
        
        # Track consecutive invalid command attempts to prevent infinite loops
        consecutive_invalid_commands = 0
        MAX_CONSECUTIVE_INVALID = 3
        
        # Check if problem description is vague and ask clarifying questions
        if self._is_vague_problem(message):
            clarifying_questions = self._generate_clarifying_questions(message)
            if clarifying_questions:
                return f"I need more information to help you effectively. Please provide:\n\n{clarifying_questions}\n\nThis will help me read the right files and provide accurate solutions."
        
        # Use reduced context size for agent (32k default, was 128k)
        effective_context = min(context_size or 32000, 32000)  # Cap at 32k for performance
        
        # Calculate adaptive timeout based on context size
        agent_timeout, per_turn_timeout = self._calculate_agent_timeout(
            context_size=context_size,
            files_read=0,  # Will update as we go
            turns=0,
            max_turns=max_turns
        )
        logger.info(f"Agent mode timeout set to {agent_timeout/60:.1f} minutes (context_size: {effective_context}, per_turn: {per_turn_timeout}s)")
        
        # Emit early progress event for fast first turn
        if progress_callback:
            try:
                progress_callback(json.dumps({
                    "type": "agent_started",
                    "max_turns": max_turns,
                    "timeout_seconds": int(agent_timeout)
                }), 1, 4)
            except:
                pass
        
        # Heartbeat and watchdog setup
        heartbeat_active = threading.Event()
        heartbeat_active.set()  # Start active
        last_task_update_time = time.time()
        phase_counters = {"reads": {"current": 0, "total": 0}, "greps": {"current": 0, "total": 0}, 
                         "searches": {"current": 0, "total": 0}, "traces": {"current": 0, "total": 0}}
        
        def heartbeat_loop():
            """Emit heartbeat every 2s and watchdog check."""
            while heartbeat_active.is_set():
                try:
                    current_time = time.time()
                    
                    # Emit heartbeat
                    if progress_callback:
                        try:
                            progress_callback(json.dumps({
                                "type": "heartbeat",
                                "ts": int(current_time * 1000)
                            }), 2, 4)
                        except:
                            pass
                    
                    # Watchdog: if no task_update for 10s, emit heartbeat + progress dump
                    if current_time - last_task_update_time > 10:
                        if progress_callback:
                            try:
                                # Emit progress dump
                                for phase, counters in phase_counters.items():
                                    if counters["total"] > 0:
                                        progress_callback(json.dumps({
                                            "type": "progress",
                                            "phase": phase,
                                            "current": counters["current"],
                                            "total": counters["total"],
                                            "percentage": int((counters["current"] / counters["total"]) * 100) if counters["total"] > 0 else 0
                                        }), 2, 4)
                            except:
                                pass
                    
                    # Sleep for 2 seconds (use time.sleep to ensure consistent 2s interval)
                    time.sleep(2.0)
                    # Check if we should continue (event might have been cleared)
                    if not heartbeat_active.is_set():
                        break
                except Exception as e:
                    logger.warning(f"Heartbeat loop error: {e}")
                    break
        
        # Start heartbeat thread
        heartbeat_thread = threading.Thread(target=heartbeat_loop, daemon=True)
        heartbeat_thread.start()
        
        def _run_agent_core():
            """Core agent execution logic extracted from try block."""
            # == AGENT_CORE_START ==
            while turns < max_turns:
                # Recalculate timeout if we've made progress (files_read updated)
                if len(files_read_tracker) >= 3 and turns < 5:
                    agent_timeout, per_turn_timeout = self._calculate_agent_timeout(
                        context_size=context_size,
                        files_read=len(files_read_tracker),
                        turns=turns,
                        max_turns=max_turns
                    )

                # Track turn start time for per-turn timeout
                turn_start_time = time.time()

                # Check overall timeout with progress update
                elapsed_time = time.time() - agent_start_time
                if elapsed_time > agent_timeout:
                    logger.warning(f"Agent mode overall timeout reached after {turns} turns ({elapsed_time:.1f}s)")

                    # Try to provide partial results if we have findings
                    if files_read_tracker or key_insights:
                        partial_summary = f"Agent mode timed out after {elapsed_time/60:.1f} minutes ({turns} turns completed).\n\n"
                        partial_summary += f"**Partial Results:**\n"
                        if files_read_tracker:
                            partial_summary += f"- Files read: {', '.join(list(files_read_tracker)[:10])}\n"
                        if key_insights:
                            partial_summary += f"- Key insights: {', '.join(key_insights[-5:])}\n"
                        partial_summary += f"\n**Recommendation:** Break your question into smaller parts or ask about specific files.\n"
                        # Stop heartbeat and emit done event
                        heartbeat_active.clear()
                        if progress_callback:
                            try:
                                all_tasks = task_tracker.get_tasks()
                                completed = sum(1 for t in all_tasks if t["status"] == "done")
                                failed = sum(1 for t in all_tasks if t["status"] == "failed")
                                duration_ms = int((time.time() - agent_start_time) * 1000)
                                progress_callback(json.dumps({
                                    "type": "done",
                                    "summary": {
                                        "tasks_total": len(all_tasks),
                                        "completed": completed,
                                        "failed": failed,
                                        "duration_ms": duration_ms
                                    }
                                }), 4, 4)
                            except:
                                pass
                        return partial_summary

                    # Stop heartbeat and emit done event
                    heartbeat_active.clear()
                    if progress_callback:
                        try:
                            all_tasks = task_tracker.get_tasks()
                            completed = sum(1 for t in all_tasks if t["status"] == "done")
                            failed = sum(1 for t in all_tasks if t["status"] == "failed")
                            duration_ms = int((time.time() - agent_start_time) * 1000)
                            progress_callback(json.dumps({
                                "type": "done",
                                "summary": {
                                    "tasks_total": len(all_tasks),
                                    "completed": completed,
                                    "failed": failed,
                                    "duration_ms": duration_ms
                                }
                            }), 4, 4)
                        except:
                            pass
                    return f"Agent mode timed out after {elapsed_time/60:.1f} minutes ({turns} turns). Please try a more specific question or break it into smaller parts."

                # Check per-turn timeout
                turn_elapsed = time.time() - turn_start_time
                if turn_elapsed > per_turn_timeout:
                    logger.warning(f"Per-turn timeout reached after {turn_elapsed:.1f}s on turn {turns + 1}")
                    if progress_callback:
                        progress_callback(f"Turn {turns + 1} timed out after {per_turn_timeout}s. Breaking turn.", 2, 4)
                    break

                # Warn when approaching per-turn timeout (80% threshold)
                if turn_elapsed > per_turn_timeout * 0.8 and turn_elapsed > 10:
                    remaining_turn_time = per_turn_timeout - turn_elapsed
                    logger.warning(f"Approaching per-turn timeout: {remaining_turn_time:.0f}s remaining on turn {turns + 1}")

                # Send time remaining update
                time_remaining = agent_timeout - elapsed_time
                if progress_callback and time_remaining > 0:
                    minutes = int(time_remaining // 60)
                    seconds = int(time_remaining % 60)
                    formatted_time = f"{minutes}m {seconds}s" if minutes > 0 else f"{seconds}s"
                    try:
                        import json
                        progress_callback(json.dumps({
                            "type": "time_remaining",
                            "seconds": int(time_remaining),
                            "formatted": formatted_time
                        }), 2, 4)
                    except:
                        pass  # Fallback to regular callback if JSON fails

                # Warn when approaching overall timeout (at 80% of timeout)
                if elapsed_time > agent_timeout * 0.8 and turns > 0:
                    remaining_time = agent_timeout - elapsed_time
                    logger.warning(f"Agent mode approaching timeout: {remaining_time:.0f}s remaining, {turns} turns completed")
                    if progress_callback:
                        progress_callback(f"Approaching timeout ({remaining_time:.0f}s remaining)...", 2, 4)

                # Build messages for this turn
                if progress_callback:
                    progress_callback(f"Agent turn {turns + 1}/{max_turns}...", 2, 4)
                    progress_callback(f"🤔 Agent is thinking about the problem...", 2, 4)

                messages = [{"role": "system", "content": system_prompt}]

                # Optimize context: summarize old turns if conversation is getting long
                optimized_context = self._optimize_conversation_context(
                    conversation_context, 
                    files_read_tracker,
                    key_insights
                )
                messages.extend(optimized_context)
                messages.append({"role": "user", "content": user_message})

                # LLM responds with command or analysis
                # Retry logic for connection errors
                max_retries = 3
                retry_delay = 2  # seconds
                llm_response = None

                # Performance tracking for Ollama call
                analysis_call_start = time.time()
                first_token_time = None

                for retry_attempt in range(max_retries):
                    try:
                        # Timeout for agent mode (reduced for faster failure detection)
                        timeout = 60  # Reduced from 120 to 60 seconds

                        # Track first token time (for streaming, we'd check stream, but for non-streaming we approximate)
                        call_start = time.time()

                        response = requests.post(
                            f"{self.ollama_url}/api/chat",
                            json={
                                "model": self.model,
                                "messages": messages,
                                "stream": False,
                                "options": {
                                    "temperature": 0.1,  # Reduced from 0.3 for more deterministic
                                    "top_p": 0.9,
                                    "num_predict": 2000,  # Limit output (was -1 unlimited)
                                    "num_ctx": effective_context,  # Use reduced context (32k cap)
                                }
                            },
                            timeout=timeout
                        )

                        if response.status_code != 200:
                            raise Exception(f"Ollama API returned status {response.status_code}")

                        # Approximate first token time (for non-streaming, it's when we get response)
                        if first_token_time is None:
                            first_token_time = time.time()
                            first_token_ms = int((first_token_time - call_start) * 1000)
                            self._log_perf_metric("first_token", ms=first_token_ms)

                        result = response.json()
                        llm_response = result.get("message", {}).get("content", "")

                        # Show what the agent is thinking
                        if progress_callback and llm_response:
                            # Extract thinking/reasoning from response (first 200 chars)
                            thinking_preview = llm_response[:200].replace('\n', ' ')
                            if len(llm_response) > 200:
                                thinking_preview += "..."
                            progress_callback(f"💭 Agent thinking: {thinking_preview}", 2, 4)

                        # Validate LLM response is not empty
                        if not llm_response or not llm_response.strip():
                            logger.warning(f"Empty LLM response on turn {turns + 1}, attempt {retry_attempt + 1}")
                            if retry_attempt < max_retries - 1:
                                time.sleep(retry_delay)
                                continue
                            # Don't increment turn - retry same turn
                            user_message = "I received an empty response. Please try again or rephrase your question."
                            conversation_context.append({
                                "role": "user",
                                "content": user_message
                            })
                            continue  # Don't increment turns

                        # Log analysis call performance
                        analysis_call_ms = int((time.time() - analysis_call_start) * 1000)
                        self._log_perf_metric("analysis_call", 
                                             model=self.model, 
                                             ctx=effective_context, 
                                             ms=analysis_call_ms, 
                                             attempts=retry_attempt + 1)

                        # Success - break out of retry loop
                        break

                    except (requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:
                        error_type = "Connection" if isinstance(e, requests.exceptions.ConnectionError) else "Timeout"
                        logger.warning(f"Agent turn {turns + 1} {error_type} error (attempt {retry_attempt + 1}/{max_retries}): {e}")

                        if retry_attempt < max_retries - 1:
                            # Retry with exponential backoff
                            wait_time = retry_delay * (2 ** retry_attempt)
                            logger.info(f"Retrying in {wait_time} seconds...")
                            time.sleep(wait_time)
                            continue
                        else:
                            # All retries exhausted
                            logger.error(f"Agent conversation turn {turns + 1} failed after {max_retries} attempts: {e}")
                            # Check if Ollama is still available
                            self.ollama_available = self._check_ollama_available()
                            # Stop heartbeat and emit error event
                            heartbeat_active.clear()
                            if progress_callback:
                                try:
                                    progress_callback(json.dumps({
                                        "type": "error",
                                        "message": f"Agent mode failed on turn {turns + 1} due to {error_type.lower()} error: {str(e)}"
                                    }), 4, 4)
                                except:
                                    pass
                            if not self.ollama_available:
                                return "Ollama service is not available. Please ensure Ollama is running and try again."
                            return f"Agent mode failed on turn {turns + 1} due to {error_type.lower()} error: {str(e)}. Please try again or use a more specific question."

                    except Exception as e:
                        # For other exceptions, log and retry if it's a recoverable error
                        error_str = str(e).lower()
                        is_recoverable = any(keyword in error_str for keyword in ["timeout", "connection", "network", "temporary"])

                        if is_recoverable and retry_attempt < max_retries - 1:
                            logger.warning(f"Agent turn {turns + 1} recoverable error (attempt {retry_attempt + 1}/{max_retries}): {e}")
                            wait_time = retry_delay * (2 ** retry_attempt)
                            time.sleep(wait_time)
                            continue
                        else:
                            # Non-recoverable error or all retries exhausted
                            logger.error(f"Agent conversation turn {turns + 1} failed: {e}")
                            # Stop heartbeat and emit error event
                            heartbeat_active.clear()
                            if progress_callback:
                                try:
                                    progress_callback(json.dumps({
                                        "type": "error",
                                        "message": f"Agent mode failed on turn {turns + 1}: {str(e)}"
                                    }), 4, 4)
                                except:
                                    pass
                            return f"Agent mode failed on turn {turns + 1}: {str(e)}. Please try again or use a more specific question."

                # If we get here without llm_response, something went wrong
                if not llm_response:
                    logger.error(f"Agent turn {turns + 1} failed: No response after {max_retries} attempts")
                    # Stop heartbeat and emit error event
                    heartbeat_active.clear()
                    if progress_callback:
                        try:
                            progress_callback(json.dumps({
                                "type": "error",
                                "message": "Agent mode failed to get a response from Ollama. Please ensure Ollama is running and try again."
                            }), 4, 4)
                        except:
                            pass
                    return "Agent mode failed to get a response from Ollama. Please ensure Ollama is running and try again."

            # Add LLM response to conversation
                conversation_context.append({
                "role": "assistant",
                "content": llm_response
                })

            # Parse commands from LLM response
                commands = self._parse_agent_commands(llm_response)

            # Show what tasks the agent set for itself
            if progress_callback and commands:
                task_list = []
                for cmd in commands:
                    if cmd["type"] == "READ":
                        task_list.append(f"📖 Read {cmd.get('file', 'file')}")
                    elif cmd["type"] == "GREP":
                        pattern = cmd.get('pattern', 'pattern')[:30]
                        if len(cmd.get('pattern', '')) > 30:
                            pattern += "..."
                        task_list.append(f"🔍 Grep '{pattern}'")
                    elif cmd["type"] == "SEARCH":
                        term = cmd.get('term', 'term')[:30]
                        if len(cmd.get('term', '')) > 30:
                            term += "..."
                        task_list.append(f"🔎 Search '{term}'")
                    elif cmd["type"] == "TRACE":
                        start = cmd.get('start', 'start')[:20]
                        end = cmd.get('end', 'end')[:20]
                        task_list.append(f"🔗 Trace {start} → {end}")
                    elif cmd["type"] == "ANALYZE":
                        task_list.append("📊 Analyze findings")

                if task_list:
                    tasks_text = " | ".join(task_list[:3])  # Show first 3 tasks
                    if len(task_list) > 3:
                        tasks_text += f" (+{len(task_list) - 3} more)"
                    progress_callback(f"📋 Agent tasks: {tasks_text}", 2, 4)

            # Validate commands have required fields
            valid_commands = []
            for cmd in commands:
                if cmd["type"] == "READ" and not cmd.get("file"):
                    logger.warning(f"READ command missing 'file' field: {cmd}")
                    continue
                elif cmd["type"] == "GREP" and not cmd.get("pattern"):
                    logger.warning(f"GREP command missing 'pattern' field: {cmd}")
                    continue
                elif cmd["type"] == "SEARCH" and not cmd.get("term"):
                    logger.warning(f"SEARCH command missing 'term' field: {cmd}")
                    continue
                elif cmd["type"] == "TRACE" and (not cmd.get("start") or not cmd.get("end")):
                    logger.warning(f"TRACE command missing 'start' or 'end' field: {cmd}")
                    continue
                valid_commands.append(cmd)

                # Track consecutive invalid command attempts
                if len(commands) > 0 and len(valid_commands) == 0:
                    consecutive_invalid_commands += 1
                    if consecutive_invalid_commands >= MAX_CONSECUTIVE_INVALID:
                        logger.warning(f"Too many consecutive invalid commands ({consecutive_invalid_commands}), forcing analysis")
                        # Force analysis to break the loop
                        break
                else:
                    consecutive_invalid_commands = 0  # Reset counter on valid commands

                commands = valid_commands

                # Create task entries for each valid command (after validation)
                task_ids = {}  # Maps command_key -> task_id
                cmd_to_key = {}  # Maps cmd object id -> command_key (for lookup)
                cmd_key_counter = {}  # Track duplicates to make keys unique

                if commands:
                    for idx, cmd in enumerate(commands):
                        # Create a base hashable key from command fields
                        if cmd["type"] == "READ":
                            base_key = f"READ_{cmd.get('file', '')}"
                        elif cmd["type"] == "GREP":
                            base_key = f"GREP_{cmd.get('pattern', '')}"
                        elif cmd["type"] == "SEARCH":
                            base_key = f"SEARCH_{cmd.get('term', '')}"
                        elif cmd["type"] == "TRACE":
                            base_key = f"TRACE_{cmd.get('start', '')}_{cmd.get('end', '')}"
                        elif cmd["type"] == "ANALYZE":
                            base_key = f"ANALYZE_{turns}"
                        else:
                            base_key = f"{cmd['type']}_{hash(str(cmd))}_{turns}"

                        # Make cmd_key unique by including index if duplicate
                        if base_key in cmd_key_counter:
                            cmd_key_counter[base_key] += 1
                            cmd_key = f"{base_key}_{cmd_key_counter[base_key]}"
                        else:
                            cmd_key_counter[base_key] = 0
                            cmd_key = base_key

                        task_id = f"{cmd['type']}_{hash(cmd_key)}_{turns}_{idx}"
                        task_ids[cmd_key] = task_id
                        cmd_to_key[id(cmd)] = cmd_key  # Use id() to reference the cmd object

                        # Generate task description
                        if cmd["type"] == "READ":
                            description = f"Read {cmd.get('file', 'file')}"
                        elif cmd["type"] == "GREP":
                            pattern = cmd.get('pattern', 'pattern')[:50]
                            description = f"Grep '{pattern}'"
                        elif cmd["type"] == "SEARCH":
                            term = cmd.get('term', 'term')[:50]
                            description = f"Search '{term}'"
                        elif cmd["type"] == "TRACE":
                            start = cmd.get('start', 'start')[:30]
                            end = cmd.get('end', 'end')[:30]
                            description = f"Trace {start} → {end}"
                        elif cmd["type"] == "ANALYZE":
                            description = "Analyze findings"
                        else:
                            description = f"{cmd['type']} command"

                        task_tracker.add_task(task_id, cmd["type"], description)

                # Initialize phase counters based on planned tasks
                for tid in task_tracker.task_order:
                    if tid in task_tracker.tasks:
                        task = task_tracker.tasks[tid]
                        task_type = task["type"]
                        if task_type == "READ":
                            phase_counters["reads"]["total"] += 1
                        elif task_type == "GREP":
                            phase_counters["greps"]["total"] += 1
                        elif task_type == "SEARCH":
                            phase_counters["searches"]["total"] += 1
                        elif task_type == "TRACE":
                            phase_counters["traces"]["total"] += 1

                # Emit plan event immediately after building task list
                if progress_callback:
                    try:
                        # Build plan tasks list with proper format
                        plan_tasks = []
                        for tid in task_tracker.task_order:
                            if tid in task_tracker.tasks:
                                task = task_tracker.tasks[tid]
                                plan_tasks.append({
                                    "id": task["id"],
                                    "title": task["title"],
                                    "type": task["type"],
                                    "status": task["status"]
                                })

                        progress_callback(json.dumps({
                            "type": "plan",
                            "tasks": plan_tasks
                        }), 2, 4)
                    except Exception as e:
                        logger.warning(f"Failed to emit plan event: {e}")
                        # Fallback to old format
                        try:
                            progress_callback(json.dumps({
                                "type": "tasks",
                                "tasks": task_tracker.get_tasks()
                            }), 2, 4)
                        except:
                            pass

                # Show what commands will be executed
                if progress_callback and valid_commands:
                read_count = sum(1 for c in valid_commands if c["type"] == "READ")
                grep_count = sum(1 for c in valid_commands if c["type"] == "GREP")
                search_count = sum(1 for c in valid_commands if c["type"] == "SEARCH")
                trace_count = sum(1 for c in valid_commands if c["type"] == "TRACE")

                exec_summary = []
                if read_count > 0:
                    exec_summary.append(f"{read_count} READ")
                if grep_count > 0:
                    exec_summary.append(f"{grep_count} GREP")
                if search_count > 0:
                    exec_summary.append(f"{search_count} SEARCH")
                if trace_count > 0:
                    exec_summary.append(f"{trace_count} TRACE")

                if exec_summary:
                    progress_callback(f"⚙️ Executing: {', '.join(exec_summary)}", 2, 4)

                # Limit commands_history to prevent unbounded growth
                if len(commands_history) > MAX_COMMANDS_HISTORY:
                commands_history = commands_history[-MAX_COMMANDS_HISTORY:]

                # Track tool usage and command history
                tool_used_this_turn = any(cmd["type"] in ["READ", "GREP", "SEARCH", "TRACE"] for cmd in commands)
                if tool_used_this_turn:
                tools_used = True
                # Store command strings for validation
                for cmd in commands:
                    if cmd["type"] == "READ":
                        commands_history.append(f"READ {cmd.get('file', '')}")
                    elif cmd["type"] == "GREP":
                        commands_history.append(f"GREP {cmd.get('pattern', '')}")
                    elif cmd["type"] == "SEARCH":
                        commands_history.append(f"SEARCH {cmd.get('term', '')}")
                    elif cmd["type"] == "TRACE":
                        commands_history.append(f"TRACE {cmd.get('start', '')} → {cmd.get('end', '')}")

                # Track files read
                for cmd in commands:
                    if cmd["type"] == "READ" and cmd.get("file"):
                        files_read_tracker.add(cmd["file"])
                        # Remove from discovered files if it was there
                        discovered_files.discard(cmd["file"])

                # Limit discovered_files to prevent unbounded growth
                if len(discovered_files) >= MAX_DISCOVERED_FILES:
                # Keep only most recent (convert to list, keep last N)
                discovered_files = set(list(discovered_files)[-MAX_DISCOVERED_FILES:])

                # Check if LLM wants to analyze
                if any(cmd["type"] == "ANALYZE" for cmd in commands):
                if progress_callback:
                    progress_callback("Analyzing findings...", 3, 4)

                # STRICT validation before allowing analysis
                verification = self._check_verification_requirements(
                    message, files_read_tracker, commands_history
                )

                # Store verification results for confidence calculation
                self._last_verification_results = {
                    "function_verifications": verification.get("verification_results", {}),
                    "framework_results": verification.get("framework_results", {})
                }

                if not verification["can_analyze"]:
                    user_message = verification["blocking_message"]
                    turns += 1
                    continue

                # Also check if tools were used
                if not tools_used:
                    user_message = "You requested ANALYZE but haven't used any tools yet. You MUST read the codebase first:\n\n1. Use READ command on files mentioned in the problem (e.g., READ backend/services/ocr_service.py)\n2. Use GREP to find function definitions (e.g., GREP insert_line_items)\n3. Use SEARCH to understand relationships (e.g., SEARCH line items)\n\nOnly use ANALYZE after you've read actual code files. Start with READ commands."
                    turns += 1
                    continue

                # EARLY CONFIDENCE-BASED BLOCKING (before ANALYZE)
                # Calculate confidence DURING exploration, block BEFORE generation
                current_confidence = self._calculate_confidence_score(
                    files_read=files_read_tracker,
                    commands_history=commands_history,
                    key_insights=key_insights,
                    validation_issues=[],
                    verification_results=self._last_verification_results,
                    code_match_accuracy=None,  # Not available yet during exploration
                    runtime_checks=None  # Not available yet during exploration
                )

                if current_confidence < 0.7:
                    logger.warning(f"Low confidence score ({current_confidence:.0%}) before ANALYZE, blocking early")
                    blocking_message = f"⚠️ ANALYSIS BLOCKED - Low Confidence ({current_confidence:.0%})\n\n"
                    blocking_message += "Your confidence is too low to provide a reliable analysis. The analysis has low confidence due to:\n"

                    if len(files_read_tracker) < 2:
                        blocking_message += f"  - Only {len(files_read_tracker)} file(s) read (need at least 2)\n"
                    if not self._last_verification_results.get("function_verifications"):
                        blocking_message += "  - No function verification performed (use GREP to verify function names)\n"
                    if not self._last_verification_results.get("framework_results"):
                        blocking_message += "  - Framework not detected (use GREP to detect FastAPI/Flask)\n"

                    # Get exploration suggestions
                    suggestions = self._get_exploration_suggestions(
                        files_read=files_read_tracker,
                        commands_history=commands_history,
                        confidence=current_confidence,
                        verification_results=self._last_verification_results
                    )

                    if suggestions:
                        blocking_message += f"\n💡 Suggestions to improve confidence:\n{suggestions}\n"

                    blocking_message += "\nPlease complete these steps before using ANALYZE:\n"
                    blocking_message += "1. READ more files related to the problem\n"
                    blocking_message += "2. Use GREP to verify function names exist\n"
                    blocking_message += "3. Use GREP to detect framework (FastAPI/Flask)\n"
                    blocking_message += "4. Re-run ANALYZE after completing these steps\n"

                    user_message = blocking_message
                    turns += 1
                    continue

                logger.info(f"Agent requested analysis after {turns + 1} turns (files read: {len(files_read_tracker)}, confidence: {current_confidence:.0%}, verifications passed)")

                # Log total agent run time
                total_ms = int((time.time() - agent_start_time) * 1000)
                self._log_perf_metric("totals", ms=total_ms, turns=turns + 1, files_read=len(files_read_tracker))

                break

                # Execute commands (with parallel execution for READ commands)
                results = []

                # Performance tracking for command execution
                searches_start = time.time()
                reads_start = time.time()
                traces_start = time.time()
                search_count = 0
                read_count = 0
                trace_count = 0

                # Separate commands by type for parallel execution
                read_commands = [cmd for cmd in commands if cmd["type"] == "READ"]
                search_commands = [cmd for cmd in commands if cmd["type"] == "SEARCH"]
                trace_commands = [cmd for cmd in commands if cmd["type"] == "TRACE"]
                other_commands = [cmd for cmd in commands if cmd["type"] not in ["READ", "SEARCH", "TRACE"]]

                # Execute READ commands in parallel (if multiple)
                # Early stop: limit to max_files_per_plan to prevent unbounded reads
                if len(read_commands) > 0:
                reads_start = time.time()
                # Limit reads to max_files_per_plan
                if len(read_commands) > config.max_files_per_plan:
                    logger.info(f"Limiting READ commands from {len(read_commands)} to {config.max_files_per_plan}")
                    read_commands = read_commands[:config.max_files_per_plan]

                # Mark all READ tasks as running and emit task_update
                for cmd in read_commands:
                    cmd_key = cmd_to_key.get(id(cmd))
                    if cmd_key and cmd_key in task_ids:
                        task_id = task_ids[cmd_key]
                        task_tracker.start_task(task_id)
                        # Emit task_update event
                        if progress_callback:
                            try:
                                task = task_tracker.tasks[task_id]
                                last_task_update_time = time.time()
                                progress_callback(json.dumps({
                                    "type": "task_update",
                                    "task": {
                                        "id": task["id"],
                                        "status": task["status"],
                                        "started_at": task["started_at"]
                                    }
                                }), 2, 4)
                            except:
                                pass

                read_results = self._execute_read_commands_parallel(read_commands, file_cache, file_relationships, discovered_files, files_read_tracker, turns, MAX_FILE_CACHE_SIZE)
                results.extend(read_results)
                read_count = len(read_commands)
                reads_ms = int((time.time() - reads_start) * 1000)
                self._log_perf_metric("reads", files=read_count, ms=reads_ms)

                # Mark READ tasks as completed and emit task_update
                for cmd in read_commands:
                    cmd_key = cmd_to_key.get(id(cmd))
                    if cmd_key and cmd_key in task_ids:
                        task_id = task_ids[cmd_key]
                        result_count = sum(1 for r in read_results if r.get("file") == cmd.get("file") and r.get("type") != "error")
                        error = next((r.get("error") for r in read_results if r.get("file") == cmd.get("file") and r.get("type") == "error"), None)
                        task_tracker.complete_task(task_id, result_count, error)
                        # Emit task_update event
                        if progress_callback:
                            try:
                                task = task_tracker.tasks[task_id]
                                last_task_update_time = time.time()
                                phase_counters["reads"]["current"] += 1
                                progress_callback(json.dumps({
                                    "type": "task_update",
                                    "task": {
                                        "id": task["id"],
                                        "status": task["status"],
                                        "progress": task["progress"],
                                        "ended_at": task["ended_at"],
                                        "duration_ms": task["duration_ms"],
                                        "note": task.get("note")
                                    }
                                }), 2, 4)
                                # Emit progress event for reads phase
                                if phase_counters["reads"]["total"] > 0:
                                    progress_callback(json.dumps({
                                        "type": "progress",
                                        "phase": "reads",
                                        "current": phase_counters["reads"]["current"],
                                        "total": phase_counters["reads"]["total"],
                                        "percentage": int((phase_counters["reads"]["current"] / phase_counters["reads"]["total"]) * 100)
                                    }), 2, 4)
                            except:
                                pass
                elif len(read_commands) == 1:
                # Single READ command - execute normally
                cmd = read_commands[0]
                cmd_key = cmd_to_key.get(id(cmd))
                if cmd_key and cmd_key in task_ids:
                    task_id = task_ids[cmd_key]
                    task_tracker.start_task(task_id)
                    # Emit task_update event
                    if progress_callback:
                        try:
                            task = task_tracker.tasks[task_id]
                            last_task_update_time = time.time()
                            progress_callback(json.dumps({
                                "type": "task_update",
                                "task": {
                                    "id": task["id"],
                                    "status": task["status"],
                                    "started_at": task["started_at"]
                                }
                            }), 2, 4)
                        except:
                            pass

                read_results = self._execute_single_read_command(cmd, file_cache, file_relationships, discovered_files, files_read_tracker, turns, MAX_FILE_CACHE_SIZE)
                results.extend(read_results)
                read_count = 1
                reads_ms = int((time.time() - reads_start) * 1000)
                self._log_perf_metric("reads", files=read_count, ms=reads_ms)

                if cmd_key and cmd_key in task_ids:
                    task_id = task_ids[cmd_key]
                    result_count = sum(1 for r in read_results if r.get("type") != "error")
                    error = next((r.get("error") for r in read_results if r.get("type") == "error"), None)
                    task_tracker.complete_task(task_id, result_count, error)
                    # Emit task_update event
                    if progress_callback:
                        try:
                            task = task_tracker.tasks[task_id]
                            last_task_update_time = time.time()
                            phase_counters["reads"]["current"] += 1
                            progress_callback(json.dumps({
                                "type": "task_update",
                                "task": {
                                    "id": task["id"],
                                    "status": task["status"],
                                    "progress": task["progress"],
                                    "ended_at": task["ended_at"],
                                    "duration_ms": task["duration_ms"],
                                    "note": task.get("note")
                                }
                            }), 2, 4)
                            # Emit progress event for reads phase
                            if phase_counters["reads"]["total"] > 0:
                                progress_callback(json.dumps({
                                    "type": "progress",
                                    "phase": "reads",
                                    "current": phase_counters["reads"]["current"],
                                    "total": phase_counters["reads"]["total"],
                                    "percentage": int((phase_counters["reads"]["current"] / phase_counters["reads"]["total"]) * 100)
                                }), 2, 4)
                        except:
                            pass

                # Execute SEARCH commands with timing
                if search_commands:
                searches_start = time.time()
                search_count = len(search_commands)

                # Execute other commands sequentially
                for cmd in other_commands + search_commands:
                # Mark task as running and emit task_update
                cmd_key = cmd_to_key.get(id(cmd))
                if cmd_key and cmd_key in task_ids:
                    task_id = task_ids[cmd_key]
                    task_tracker.start_task(task_id)
                    # Emit task_update event
                    if progress_callback:
                        try:
                            task = task_tracker.tasks[task_id]
                            last_task_update_time = time.time()
                            progress_callback(json.dumps({
                                "type": "task_update",
                                "task": {
                                    "id": task["id"],
                                    "status": task["status"],
                                    "started_at": task["started_at"]
                                }
                            }), 2, 4)
                        except:
                            pass

                cmd_results = []
                try:
                    if cmd["type"] == "SEARCH":
                        term = cmd.get("term")
                        if not term:
                            logger.warning(f"SEARCH command missing 'term' field: {cmd}")
                            cmd_results.append({
                                "type": "error",
                                "command": cmd,
                                "error": "SEARCH command missing 'term' field"
                            })
                        else:
                            search_results = explorer.search_concept(term, max_results=config.max_search_results)
                            for result in search_results:
                                file_path = result.get("file")
                                # Track discovered files from searches
                                if file_path and file_path not in files_read_tracker:
                                    discovered_files.add(file_path)

                                cmd_results.append({
                                    "type": "search",
                                    "file": file_path,
                                    "line": result.get("line"),
                                    "match": result.get("match"),
                                    "context": result.get("context")
                                })
                    elif cmd["type"] == "TRACE":
                        if trace_count == 0:
                            traces_start = time.time()
                        trace_count += 1
                        start = cmd.get("start")
                        end = cmd.get("end")
                        if not start or not end:
                            logger.warning(f"TRACE command missing 'start' or 'end' field: {cmd}")
                            cmd_results.append({
                                "type": "error",
                                "command": cmd,
                                "error": "TRACE command missing 'start' or 'end' field"
                            })
                        else:
                            trace_results = explorer.trace_data_flow(start, end)
                            cmd_results.extend([{"type": "trace", **r} for r in trace_results])
                    elif cmd["type"] == "GREP":
                        pattern = cmd.get("pattern")
                        if not pattern:
                            logger.warning(f"GREP command missing 'pattern' field: {cmd}")
                            cmd_results.append({
                                "type": "error",
                                "command": cmd,
                                "error": "GREP command missing 'pattern' field"
                            })
                        else:
                            # Progress feedback
                            if progress_callback:
                                progress_callback(f"🔍 Searching for pattern: {pattern[:50]}...", 2, 4)

                            matches = explorer.grep_pattern(pattern)

                            # Limit matches to prevent excessive processing
                            MAX_MATCHES_PER_FILE = 20
                            MAX_TOTAL_MATCHES = 100

                            total_matches = sum(len(lines) for lines in matches.values())
                            if progress_callback:
                                progress_callback(f"Found {total_matches} matches across {len(matches)} files, processing...", 2, 4)

                            # Batch file reads: read each file once, extract all contexts
                            match_count = 0
                            file_cache_grep = {}  # Cache file contents to avoid re-reading

                            for file_path, lines in matches.items():
                                if match_count >= MAX_TOTAL_MATCHES:
                                    if progress_callback:
                                        progress_callback(f"⚠️ Limited to {MAX_TOTAL_MATCHES} matches for performance", 2, 4)
                                    break

                                # Limit matches per file
                                limited_lines = lines[:MAX_MATCHES_PER_FILE]

                                # Read file once (or use cache)
                                if file_path not in file_cache_grep:
                                    file_data = self.code_reader.read_file(file_path)
                                    if file_data.get("success"):
                                        content = file_data.get("content", "")
                                        if content:
                                            file_cache_grep[file_path] = content.split('\n')
                                        else:
                                            # Empty file, skip
                                            continue
                                    else:
                                        continue  # Skip if file read failed

                                file_lines = file_cache_grep[file_path]

                                # Extract context for each match from already-read file
                                for line_num in limited_lines:
                                    if match_count >= MAX_TOTAL_MATCHES:
                                        break

                                    # Validate line number
                                    if not isinstance(line_num, int) or line_num < 1:
                                        logger.warning(f"Invalid line number {line_num} for file {file_path}, skipping")
                                        continue

                                    # Extract context from cached file content
                                    # line_num is 1-indexed, file_lines is 0-indexed
                                    context_lines = 3
                                    line_idx = line_num - 1  # Convert to 0-indexed

                                    # Validate line_idx is within bounds
                                    if line_idx >= len(file_lines):
                                        logger.warning(f"Line number {line_num} out of bounds for file {file_path} (file has {len(file_lines)} lines), skipping")
                                        continue

                                    start_line = max(0, line_idx - context_lines)  # 0-indexed
                                    end_line = min(len(file_lines), line_idx + context_lines + 1)  # 0-indexed, +1 for inclusive

                                    # Extract context (handle empty file case)
                                    if start_line < end_line:
                                        context = '\n'.join(file_lines[start_line:end_line])
                                    else:
                                        context = file_lines[line_idx] if line_idx < len(file_lines) else ""

                                    cmd_results.append({
                                        "type": "grep",
                                        "file": file_path,
                                        "line": line_num,
                                        "context": context
                                    })
                                    match_count += 1

                            if progress_callback:
                                progress_callback(f"✅ Processed {match_count} matches", 2, 4)
                except Exception as e:
                    logger.warning(f"Command execution failed: {cmd}, error: {e}")
                    error_info = {
                        "type": "error",
                        "command": cmd,
                        "error": str(e)
                    }

                    # Add helpful suggestions based on error type
                    if cmd["type"] == "READ":
                        error_info["suggestions"] = self._suggest_similar_files(cmd.get("file", ""))
                    elif cmd["type"] == "SEARCH":
                        error_info["suggestions"] = "Try a more specific search term or use GREP with a pattern"
                    elif cmd["type"] == "GREP":
                        error_info["suggestions"] = "Check the pattern syntax. Use READ to see the file first."

                    cmd_results.append(error_info)

                # Add command results to main results
                results.extend(cmd_results)

                # Mark task as completed and emit task_update
                if cmd_key and cmd_key in task_ids:
                    task_id = task_ids[cmd_key]
                    result_count = sum(1 for r in cmd_results if r.get("type") != "error")
                    error = next((r.get("error") for r in cmd_results if r.get("type") == "error"), None)
                    task_tracker.complete_task(task_id, result_count, error)
                    # Emit task_update event
                    if progress_callback:
                        try:
                            task = task_tracker.tasks[task_id]
                            last_task_update_time = time.time()
                            # Update phase counters
                            if cmd["type"] == "GREP":
                                phase_counters["greps"]["current"] += 1
                            elif cmd["type"] == "SEARCH":
                                phase_counters["searches"]["current"] += 1
                            elif cmd["type"] == "TRACE":
                                phase_counters["traces"]["current"] += 1

                            progress_callback(json.dumps({
                                "type": "task_update",
                                "task": {
                                    "id": task["id"],
                                    "status": task["status"],
                                    "progress": task["progress"],
                                    "ended_at": task["ended_at"],
                                    "duration_ms": task["duration_ms"],
                                    "note": task.get("note")
                                }
                            }), 2, 4)

                            # Emit progress event for the phase
                            phase_name = None
                            if cmd["type"] == "GREP":
                                phase_name = "greps"
                            elif cmd["type"] == "SEARCH":
                                phase_name = "searches"
                            elif cmd["type"] == "TRACE":
                                phase_name = "traces"

                            if phase_name and phase_counters[phase_name]["total"] > 0:
                                progress_callback(json.dumps({
                                    "type": "progress",
                                    "phase": phase_name,
                                    "current": phase_counters[phase_name]["current"],
                                    "total": phase_counters[phase_name]["total"],
                                    "percentage": int((phase_counters[phase_name]["current"] / phase_counters[phase_name]["total"]) * 100)
                                }), 2, 4)
                        except:
                            pass

                # Log performance metrics for searches and traces after all commands complete
                if search_count > 0:
                searches_ms = int((time.time() - searches_start) * 1000)
                self._log_perf_metric("searches", count=search_count, ms=searches_ms)
                if trace_count > 0:
                traces_ms = int((time.time() - traces_start) * 1000)
                self._log_perf_metric("traces", count=trace_count, ms=traces_ms)

                # Send final task update after all commands complete
                if progress_callback and task_ids:
                try:
                    import json
                    progress_callback(json.dumps({
                        "type": "tasks",
                        "tasks": task_tracker.get_tasks()
                    }), 2, 4)
                except:
                    pass  # Fallback if JSON fails

                # Show what the agent found
                if progress_callback and results:
                result_summary = []
                file_count = len(set(r.get("file") for r in results if r.get("file")))
                error_count = sum(1 for r in results if r.get("type") == "error")
                match_count = sum(1 for r in results if r.get("type") in ["grep", "search"])

                if file_count > 0:
                    result_summary.append(f"{file_count} file(s)")
                if match_count > 0:
                    result_summary.append(f"{match_count} match(es)")
                if error_count > 0:
                    result_summary.append(f"{error_count} error(s)")

                if result_summary:
                    progress_callback(f"✅ Found: {', '.join(result_summary)}", 2, 4)

                # Format results and add to conversation
                if results:
                findings_text = self._format_findings(results)

                # Check if agent is providing generic suggestions without reading code
                has_actual_paths = any("backend/" in str(r.get("file", "")) or "frontend/" in str(r.get("file", "")) for r in results)

                # Check for errors in results
                has_errors = any(r.get("type") == "error" for r in results)
                error_suggestions = []
                error_warning = ""
                if has_errors:
                    error_count = sum(1 for r in results if r.get("type") == "error")
                    error_warning = f"\n\n⚠️ WARNING: {error_count} command(s) failed during execution. Results may be incomplete."
                    for r in results:
                        if r.get("type") == "error" and r.get("suggestions"):
                            error_suggestions.append(f"💡 {r.get('suggestions')}")
                        elif r.get("type") == "error":
                            error_suggestions.append(f"💡 Command failed: {r.get('command', {}).get('type', 'unknown')} - {r.get('error', 'unknown error')}")

                reminder = ""
                if not has_actual_paths and turns > 1:
                    reminder = "\n\n⚠️ REMINDER: You need to read ACTUAL code files. Use READ commands on specific files (e.g., READ backend/services/ocr_service.py) to get real file paths and line numbers. Don't make suggestions based on assumptions.\n"
                elif len(files_read_tracker) == 0 and turns > 0:
                    reminder = "\n\n⚠️ REMINDER: You haven't read any files yet. Use READ command on files mentioned in the problem before making suggestions.\n"

                progress_note = ""
                if files_read_tracker:
                    progress_note = f"\n\n📚 Progress: {len(files_read_tracker)} file(s) read so far. Continue exploring or use ANALYZE when ready."

                # Show discovered files that haven't been read yet
                discovered_note = ""
                if discovered_files and len(discovered_files) <= 5:
                    discovered_note = f"\n\n🔍 Discovered files (not yet read): {', '.join(list(discovered_files)[:5])}. Consider reading these with READ command."

                # Show related files based on relationships
                related_files_note = ""
                if file_relationships and len(files_read_tracker) > 0:
                    related = self._find_related_files(files_read_tracker, file_relationships, discovered_files)
                    if related:
                        related_files_note = f"\n\n🔗 Related files (based on imports/relationships): {', '.join(list(related)[:3])}. These might be relevant."

                # Show runtime integration hints
                runtime_hints = ""
                if turns > 1 and len(files_read_tracker) >= 2:
                    runtime_hints = self._generate_runtime_integration_hints(message, files_read_tracker, results)
                    if runtime_hints:
                        runtime_hints = f"\n\n🔧 Runtime Integration Hints:\n{runtime_hints}"

                # Context summary if we have multiple turns
                context_summary = ""
                if turns > 2 and len(key_insights) > 0:
                    context_summary = f"\n\n📝 Key insights so far:\n" + "\n".join(f"  - {insight}" for insight in key_insights[-5:])  # Last 5 insights

                # Extract insights from results
                new_insights = self._extract_insights_from_results(results, files_read_tracker)
                key_insights.extend(new_insights)

                # CONFIDENCE CALCULATION DURING EXPLORATION (after READ/GREP)
                # Calculate confidence after each exploration step to track progress
                current_confidence = self._calculate_confidence_score(
                    files_read=files_read_tracker,
                    commands_history=commands_history,
                    key_insights=key_insights,
                    validation_issues=[],
                    verification_results=getattr(self, '_last_verification_results', None),
                    code_match_accuracy=None,  # Not available during exploration
                    runtime_checks=None  # Not available during exploration
                )

                # Log confidence progress
                logger.info(f"Exploration confidence after turn {turns + 1}: {current_confidence:.0%} (files: {len(files_read_tracker)}, commands: {len(commands_history)})")

                # Add confidence indicator to progress note
                confidence_note = ""
                if current_confidence < 0.5:
                    confidence_note = f"\n\n⚠️ Low confidence ({current_confidence:.0%}) - Consider reading more files and verifying functions with GREP"
                elif current_confidence < 0.7:
                    confidence_note = f"\n\n📊 Confidence: {current_confidence:.0%} - Good progress, but more exploration recommended before ANALYZE"
                elif current_confidence >= 0.7:
                    confidence_note = f"\n\n✅ Confidence: {current_confidence:.0%} - Ready for analysis"

                # Smart suggestions if agent seems stuck
                smart_suggestions = ""
                if not commands and turns > 0:
                    smart_suggestions = self._generate_smart_suggestions(message, files_read_tracker, results, discovered_files, commands_history)

                error_help = ""
                if error_suggestions:
                    error_help = "\n\n" + "\n".join(error_suggestions)

                user_message = f"Exploration results:\n\n{findings_text}{error_warning}{reminder}{progress_note}{confidence_note}{discovered_note}{related_files_note}{context_summary}{error_help}{runtime_hints}{smart_suggestions}\n\nBefore suggesting fixes, remember:\n1. READ actual code files first - use READ command on files mentioned\n2. VERIFY FUNCTION NAMES - use GREP before mentioning any function (e.g., GREP 'def upload')\n3. VERIFY FRAMEWORK - use GREP to check FastAPI vs Flask (e.g., GREP '@app\\.|from fastapi')\n4. VERIFY LOGGING - use GREP to check if logging exists before claiming it does\n5. NEVER MAKE UP CODE - only quote code you've actually READ\n6. Use ACTUAL file paths (e.g., backend/main.py:561) not generic names\n7. Ask for SPECIFIC runtime data: exact log searches, SQL queries, curl commands\n8. Prioritize by likelihood (90%, 5%, 5%) - focus on MOST LIKELY first\n9. Provide EXACT logging code with real variable names from the ACTUAL code\n10. Focus on PRIMARY issue (why data is empty) not secondary (error handling)\n11. TRACE actual execution path through real code, not assumptions\n\nVALIDATION CHECKLIST before ANALYZE:\n□ All function names verified with GREP\n□ Framework detected (FastAPI/Flask) with GREP\n□ All code examples from actual READ files\n□ All logging claims verified with GREP\n□ All file paths are actual paths from READ\n\nContinue exploring (use READ/GREP to understand actual code) or use ANALYZE to provide your analysis (only after reading code)."
                else:
                # If no results and no commands, remind agent to use tools
                if not commands:
                    # Don't increment turn if nothing happened (prevent wasted turns)
                    smart_suggestions = self._generate_smart_suggestions(message, files_read_tracker, [], discovered_files, commands_history)
                    user_message = f"You need to use tools to explore the codebase. Try:\n- READ backend/services/ocr_service.py (or other files mentioned in the problem)\n- GREP insert_line_items (to find function definitions)\n- SEARCH line items (to find related code)\n\nDon't make suggestions without reading the actual code first.{smart_suggestions}"
                    # Don't increment turns here - continue to retry same turn
                    conversation_context.append({
                        "role": "user",
                        "content": user_message
                    })
                    continue
                else:
                    user_message = "No results found. Try a different search or use READ command on specific files mentioned in the problem."

                conversation_context.append({
                    "role": "user",
                    "content": user_message
                })

                turns += 1

                # Final analysis (outside while loop, inside helper function)
                # Log if we reached max turns
                if turns >= max_turns:
                logger.warning(f"Reached max turns ({max_turns}), forcing analysis")

                # Validation note based on what was actually read
                validation_note = ""
                if len(files_read_tracker) == 0:
                validation_note = "\n\n⚠️ WARNING: You haven't read any actual code files yet. Your analysis MUST be based on actual code you've read, not assumptions.\n"
                elif len(files_read_tracker) < 2:
                validation_note = f"\n\n📝 Note: You've read {len(files_read_tracker)} file(s). Make sure you've read all relevant files.\n"

                # Validate prompt template before sending
                template_prompt = f"""You've explored the codebase. Now provide your analysis.{validation_note}

                CRITICAL: VERIFY RUNTIME BEHAVIOR BEFORE SUGGESTING FIXES

                REMEMBER: Write primarily in TEXT explaining the problem. Use small code snippets (3-5 lines max) with file:line references to illustrate your points. DO NOT dump entire files.

                Provide in this EXACT format:

                **Code Analysis** (based on actual code you READ):
                - **Files Read**: List files you actually read
                - **Functions Found**: Actual function names from code
                - **Existing Logging**: What logging already exists
                - **Execution Path**: Actual code flow from files you READ

                **Prioritized Diagnosis (by likelihood):**

                1. **MOST LIKELY (90%): [Issue name]**
                - **Check**: `backend/path/to/file.py:123-145` (actual file path from code you READ)
                - **Existing Code**: Quote actual code snippet (3-5 lines)
                - **Issue Found**: Specific bug/mismatch identified
                - **Add logging** (if doesn't exist): Exact code with real variable names
                - **Verify**: Exact log search or SQL query

                **Root Cause**: [Based on actual code analysis, not assumptions]

                **Fix**: [Specific code changes with actual file:line references, showing BEFORE/AFTER from actual code]

                Before suggesting fixes, ask the user to:
                - Check backend logs for specific operations (with exact search terms)
                - Run database queries to see stored data (with exact SQL)
                - Check API responses to see what's returned (with exact curl commands)
                - Add diagnostic logging to verify assumptions (only if it doesn't already exist)"""

                # Validate template
                template_validation = self.response_validator.validate_template(template_prompt)
                if not template_validation.is_valid:
                logger.warning(f"Prompt template validation issues: {template_validation.issues}")
                # Add validation checklist to prompt
                validation_checklist = "\n\nVALIDATION CHECKLIST - You MUST include:\n"
                validation_checklist += "□ Code Analysis section\n"
                validation_checklist += "□ Prioritized Diagnosis section\n"
                validation_checklist += "□ Root Cause section\n"
                validation_checklist += "□ Fix section\n"
                validation_checklist += "□ Code snippets are 3-5 lines max\n"
                validation_checklist += "□ File paths are full paths (backend/...)\n"
                validation_checklist += "□ Text-first explanations, not code dumps\n"

                final_prompt = template_prompt + validation_checklist
                else:
                final_prompt = template_prompt
                conversation_context.append({
                "role": "user",
                "content": final_prompt
                })

                # Get final response with multi-pass validation
                if progress_callback:
                progress_callback("Generating final response...", 4, 4)

                messages = [{"role": "system", "content": system_prompt}]
                messages.extend(conversation_context)  # Use all conversation history (local processing)

                max_validation_passes = 3
                validation_pass = 0
                final_response = None

                while validation_pass < max_validation_passes:
                try:
                response = requests.post(
                    f"{self.ollama_url}/api/chat",
                    json={
                        "model": self.model,
                        "messages": messages,
                        "stream": False,
                        "options": {
                            "temperature": 0.2,
                            "num_predict": -1,  # -1 = unlimited output (local processing)
                            "num_ctx": effective_context,  # Use full context (local processing)
                        }
                    },
                    timeout=90
                )

                if response.status_code == 200:
                    result = response.json()
                    final_response = result.get("message", {}).get("content", "Analysis completed.")

                    # Validate response is not empty
                    if not final_response or not final_response.strip():
                        logger.warning(f"Final analysis returned empty response on pass {validation_pass + 1}")
                        if validation_pass < max_validation_passes - 1:
                            validation_pass += 1
                            continue
                        else:
                            final_response = "Analysis completed but response was empty."
                else:
                    raise Exception(f"Ollama API returned status {response.status_code}")
                except (requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:
                error_type = "Connection" if isinstance(e, requests.exceptions.ConnectionError) else "Timeout"
                logger.error(f"Final analysis {error_type.lower()} error on pass {validation_pass + 1}: {e}")

                # Retry on connection/timeout errors if not last pass
                if validation_pass < max_validation_passes - 1:
                    validation_pass += 1
                    time.sleep(2)  # Brief delay before retry
                    continue
                else:
                    # Stop heartbeat and emit error event
                    heartbeat_active.clear()
                    if progress_callback:
                        try:
                            progress_callback(json.dumps({
                                "type": "error",
                                "message": f"Multi-turn exploration completed but final analysis failed due to {error_type.lower()} error after {max_validation_passes} attempts. Conversation had {turns} turns."
                            }), 4, 4)
                        except:
                            pass
                    return f"Multi-turn exploration completed but final analysis failed due to {error_type.lower()} error after {max_validation_passes} attempts. Conversation had {turns} turns."
                except Exception as e:
                logger.error(f"Final analysis failed on pass {validation_pass + 1}: {e}")
                # For other errors, retry if not last pass
                if validation_pass < max_validation_passes - 1:
                    validation_pass += 1
                    continue
                else:
                    # Stop heartbeat and emit error event
                    heartbeat_active.clear()
                    if progress_callback:
                        try:
                            progress_callback(json.dumps({
                                "type": "error",
                                "message": f"Multi-turn exploration completed but final analysis failed. Conversation had {turns} turns."
                            }), 4, 4)
                        except:
                            pass
                    return f"Multi-turn exploration completed but final analysis failed. Conversation had {turns} turns."

                # Validate and rewrite if needed
                if validation_pass < max_validation_passes - 1:  # Don't rewrite on last pass
                # Check if response needs rewriting
                needs_rewrite = False
                rewrite_reasons = []

                # TRACK SPECIFIC ISSUES BEFORE REWRITE
                # This allows us to verify each specific issue was fixed after rewrite
                issues_before_rewrite = {
                    "code_snippets": {},
                    "function_names": {},
                    "structure_issues": [],
                    "unverified_claims": {},  # Changed from False to dict to track specific claims
                    "framework_mismatches": []
                }

                # VERIFY CODE SNIPPETS DURING MULTI-PASS (before rewrite)
                # Extract and verify code snippets from current response
                code_verifications_dict = self._extract_and_verify_code_snippets(final_response)

                # Check if code snippets don't match
                if code_verifications_dict:
                    mismatched_count = sum(1 for v in code_verifications_dict.values() if not v.get("matches", True))
                    if mismatched_count > 0:
                        needs_rewrite = True
                        rewrite_reasons.append(f"Contains {mismatched_count} code snippet(s) that don't match actual code")
                        # Track mismatched code snippets
                        for claimed_code, verification in code_verifications_dict.items():
                            if not verification.get("matches", True):
                                issues_before_rewrite["code_snippets"][claimed_code] = verification

                # VERIFY FUNCTION NAMES DURING MULTI-PASS (before rewrite)
                # Extract and verify function names from current response
                function_verifications_dict = self._extract_and_verify_function_names(final_response, files_read_tracker)

                # Check if function names don't exist
                if function_verifications_dict:
                    non_existent_count = sum(1 for v in function_verifications_dict.values() if not v.get("exists", False))
                    if non_existent_count > 0:
                        needs_rewrite = True
                        rewrite_reasons.append(f"Contains {non_existent_count} function name(s) that don't exist in codebase")
                        # Track non-existent function names
                        for func_name, verification in function_verifications_dict.items():
                            if not verification.get("exists", False):
                                issues_before_rewrite["function_names"][func_name] = verification

                # Check structure
                structure_validation = self.response_validator.validate_response(final_response)
                if not structure_validation.is_valid:
                    needs_rewrite = True
                    rewrite_reasons.extend(structure_validation.issues)
                    # Track structure issues
                    issues_before_rewrite["structure_issues"] = structure_validation.issues.copy()

                # Check for unverified claims
                unverified_warnings = self._filter_unverified_claims(final_response, files_read_tracker, commands_history)
                # Extract specific unverified claims (not just check for "⚠️" string)
                specific_unverified_claims = self._extract_specific_unverified_claims(final_response, files_read_tracker, commands_history)
                if specific_unverified_claims:
                    needs_rewrite = True
                    rewrite_reasons.append("Contains unverified claims")
                    # Track specific unverified claims (not just a boolean)
                    issues_before_rewrite["unverified_claims"] = specific_unverified_claims.copy()

                # Check for framework mismatches (pre-generation blocking)
                architecture_validation = self._validate_architecture_claims(final_response, files_read_tracker)
                if architecture_validation.get("mismatches"):
                    needs_rewrite = True
                    framework_mismatches = architecture_validation["mismatches"][:3]  # Limit to 3
                    rewrite_reasons.append(f"Framework syntax mismatches: {', '.join(framework_mismatches)}")
                    # Track framework mismatches
                    issues_before_rewrite["framework_mismatches"] = framework_mismatches.copy()

                    # Add specific suggestions from validation
                    code_validations = architecture_validation.get("code_validation_results", [])
                    for validation in code_validations[:2]:  # Limit to 2
                        suggestions = validation.get("suggestions", [])
                        if suggestions:
                            rewrite_reasons.append(f"Suggested fixes: {', '.join(suggestions[:2])}")

                if needs_rewrite:
                    logger.info(f"Response needs rewriting (pass {validation_pass + 1}/{max_validation_passes}): {rewrite_reasons}")

                    # Rewrite response
                    verification_data = getattr(self, '_last_verification_results', {})

                    # Add code verifications from current pass (dict mapping claimed_code -> verification)
                    if code_verifications_dict:
                        if "code_verifications" not in verification_data:
                            verification_data["code_verifications"] = {}
                        # Merge current verifications (current pass takes precedence)
                        verification_data["code_verifications"].update(code_verifications_dict)

                    # Add function verifications from current pass
                    if function_verifications_dict:
                        if "function_verifications" not in verification_data:
                            verification_data["function_verifications"] = {}
                        # Merge current verifications (current pass takes precedence)
                        verification_data["function_verifications"].update(function_verifications_dict)

                    # Also add code verifications from previous pass if available
                    if hasattr(self, '_last_diagnostic_results') and self._last_diagnostic_results:
                        # Prefer dict version if available, fall back to list
                        prev_verifications_dict = self._last_diagnostic_results.get("code_verifications_dict")
                        prev_verifications_list = self._last_diagnostic_results.get("code_verifications")

                        if prev_verifications_dict:
                            # Use dict version (preferred)
                            if "code_verifications" not in verification_data:
                                verification_data["code_verifications"] = {}
                            verification_data["code_verifications"].update(prev_verifications_dict)
                        elif prev_verifications_list:
                            # Convert list to dict using claimed_code as key (backward compatibility)
                            if "code_verifications" not in verification_data:
                                verification_data["code_verifications"] = {}
                            for verification in prev_verifications_list:
                                # Ensure claimed_code is present (required for dict key)
                                claimed_code = verification.get("claimed_code")
                                if not claimed_code:
                                    # Fallback: try to extract from actual_code if available
                                    # This shouldn't happen if verify_code_snippet works correctly
                                    logger.warning("Verification missing claimed_code, cannot use as dict key")
                                    continue

                                # Ensure claimed_code is stored in verification for consistency
                                verification["claimed_code"] = claimed_code
                                verification_data["code_verifications"][claimed_code] = verification

                    final_response = self.response_rewriter.rewrite_response(
                        final_response,
                        verification_results=verification_data,
                        files_read=files_read_tracker
                    )

                    # VERIFY THE REWRITE ACTUALLY FIXED SPECIFIC ISSUES
                    # Don't just check if warnings are gone - verify each specific issue was resolved

                    # 1. Verify code snippets after rewrite
                    code_verifications_after = self._extract_and_verify_code_snippets(final_response)
                    code_snippets_fixed = True
                    if issues_before_rewrite["code_snippets"]:
                        # Check if each tracked code snippet issue was fixed
                        for claimed_code_before, verification_before in issues_before_rewrite["code_snippets"].items():
                            # Find corresponding verification after rewrite
                            # The claimed_code might have changed, so check all verifications
                            found_fixed = False

                            # First, check if the original wrong code is still in the response
                            if claimed_code_before not in final_response:
                                # Code was replaced, check if replacement matches
                                actual_code_before = verification_before.get("actual_code")
                                if actual_code_before:
                                    # Check if actual_code appears in response (normalized comparison)
                                    from difflib import SequenceMatcher
                                    normalized_actual = self.response_rewriter._normalize_code(actual_code_before)

                                    # Check all code snippets in response after rewrite
                                    for claimed_code_after, verification_after in code_verifications_after.items():
                                        normalized_after = self.response_rewriter._normalize_code(claimed_code_after)
                                        similarity = SequenceMatcher(None, normalized_after, normalized_actual).ratio()
                                        if similarity >= 0.7:  # 70% similarity threshold
                                            # Check if verification says it matches
                                            if verification_after.get("matches", False):
                                                found_fixed = True
                                                break

                            # Also check if any verification after rewrite shows the code now matches
                            if not found_fixed:
                                for claimed_code_after, verification_after in code_verifications_after.items():
                                    if verification_after.get("matches", False):
                                        # Check if this matches the actual_code from before
                                        actual_code_before = verification_before.get("actual_code")
                                        if actual_code_before:
                                            from difflib import SequenceMatcher
                                            normalized_after = self.response_rewriter._normalize_code(claimed_code_after)
                                            normalized_actual = self.response_rewriter._normalize_code(actual_code_before)
                                            similarity = SequenceMatcher(None, normalized_after, normalized_actual).ratio()
                                            if similarity >= 0.7:  # 70% similarity threshold
                                                found_fixed = True
                                                break

                            if not found_fixed:
                                code_snippets_fixed = False
                                break

                    # 2. Verify function names after rewrite
                    function_verifications_after = self._extract_and_verify_function_names(final_response, files_read_tracker)
                    function_names_fixed = True
                    if issues_before_rewrite["function_names"]:
                        # Check if each tracked function name issue was fixed
                        for func_name_before, verification_before in issues_before_rewrite["function_names"].items():
                            # Check if function name was fixed (either exists now or was replaced)
                            # The function name might have been replaced, so check all verifications
                            found_fixed = False
                            for func_name_after, verification_after in function_verifications_after.items():
                                if verification_after.get("exists", False):
                                    # Check if this is the corrected version of the wrong function
                                    suggested_name = verification_before.get("suggested_name")
                                    if suggested_name and func_name_after == suggested_name:
                                        found_fixed = True
                                        break
                                    # Or check if the original name now exists (unlikely but possible)
                                    if func_name_after == func_name_before:
                                        found_fixed = True
                                        break

                            # Also check if the wrong function name is no longer in the response
                            if func_name_before not in final_response:
                                found_fixed = True

                            if not found_fixed:
                                function_names_fixed = False
                                break

                    # 3. Verify structure issues after rewrite
                    structure_validation_after = self.response_validator.validate_response(final_response)
                    structure_fixed = structure_validation_after.is_valid
                    if issues_before_rewrite["structure_issues"]:
                        # Check if structure issues were resolved
                        if not structure_fixed:
                            # Check if the same issues still exist
                            remaining_issues = [
                                issue for issue in issues_before_rewrite["structure_issues"]
                                if issue in structure_validation_after.issues
                            ]
                            if remaining_issues:
                                structure_fixed = False

                    # 4. Verify unverified claims after rewrite
                    # Check specific unverified claims, not just "⚠️" string
                    unverified_claims_after = self._extract_specific_unverified_claims(final_response, files_read_tracker, commands_history)
                    unverified_claims_before = issues_before_rewrite.get("unverified_claims", {})

                    unverified_fixed = True
                    if unverified_claims_before:
                        # Verify each specific claim type was resolved
                        # Unverified functions should be fixed (tracked separately in function_names, but check here too)
                        if unverified_claims_before.get("unverified_functions"):
                            remaining_unverified = [
                                func for func in unverified_claims_before["unverified_functions"]
                                if func in unverified_claims_after.get("unverified_functions", [])
                            ]
                            if remaining_unverified:
                                unverified_fixed = False

                        # Function name suggestions should be fixed (wrong names replaced or removed)
                        if unverified_claims_before.get("function_name_suggestions"):
                            remaining_suggestions = []
                            for suggestion_before in unverified_claims_before["function_name_suggestions"]:
                                wrong_name = suggestion_before.get("wrong_name")
                                # Check if wrong name still exists in response or still in unverified claims
                                if wrong_name in final_response:
                                    # Check if it's still in the suggestions after rewrite
                                    still_suggested = any(
                                        s.get("wrong_name") == wrong_name
                                        for s in unverified_claims_after.get("function_name_suggestions", [])
                                    )
                                    if still_suggested:
                                        remaining_suggestions.append(wrong_name)
                            if remaining_suggestions:
                                unverified_fixed = False

                        # Signature mismatches should be fixed
                        if unverified_claims_before.get("signature_mismatches"):
                            remaining_mismatches = [
                                mismatch for mismatch in unverified_claims_before["signature_mismatches"]
                                if any(
                                    m.get("function") == mismatch.get("function")
                                    for m in unverified_claims_after.get("signature_mismatches", [])
                                )
                            ]
                            if remaining_mismatches:
                                unverified_fixed = False

                        # Framework warnings should be fixed
                        if unverified_claims_before.get("framework_warnings"):
                            remaining_warnings = [
                                warning for warning in unverified_claims_before["framework_warnings"]
                                if warning in unverified_claims_after.get("framework_warnings", [])
                            ]
                            if remaining_warnings:
                                unverified_fixed = False

                        # Invalid code snippets should be fixed (tracked separately in code_snippets, but check here too)
                        if unverified_claims_before.get("invalid_code_snippets"):
                            remaining_snippets = [
                                snippet for snippet in unverified_claims_before["invalid_code_snippets"]
                                if snippet in unverified_claims_after.get("invalid_code_snippets", [])
                            ]
                            if remaining_snippets:
                                unverified_fixed = False

                        # Generic paths should be fixed
                        if unverified_claims_before.get("generic_paths"):
                            remaining_paths = [
                                path for path in unverified_claims_before["generic_paths"]
                                if path in unverified_claims_after.get("generic_paths", [])
                            ]
                            if remaining_paths:
                                unverified_fixed = False

                    # 5. Verify framework mismatches after rewrite (if any)
                    framework_fixed = True
                    if issues_before_rewrite["framework_mismatches"]:
                        architecture_validation_after = self._validate_architecture_claims(final_response, files_read_tracker)
                        if architecture_validation_after.get("mismatches"):
                            # Check if mismatches were resolved
                            remaining_mismatches = [
                                mismatch for mismatch in issues_before_rewrite["framework_mismatches"]
                                if mismatch in architecture_validation_after["mismatches"]
                            ]
                            if remaining_mismatches:
                                framework_fixed = False

                    # Only exit early if ALL tracked issues were resolved
                    all_issues_resolved = (
                        code_snippets_fixed and
                        function_names_fixed and
                        structure_fixed and
                        unverified_fixed and
                        framework_fixed
                    )

                    # Track what was fixed vs what still needs fixing
                    fix_status = {
                        "code_snippets": {
                            "fixed": code_snippets_fixed,
                            "total_issues": len(issues_before_rewrite.get("code_snippets", {})),
                            "remaining": 0 if code_snippets_fixed else len(issues_before_rewrite.get("code_snippets", {}))
                        },
                        "function_names": {
                            "fixed": function_names_fixed,
                            "total_issues": len(issues_before_rewrite.get("function_names", {})),
                            "remaining": 0 if function_names_fixed else len(issues_before_rewrite.get("function_names", {}))
                        },
                        "structure": {
                            "fixed": structure_fixed,
                            "total_issues": len(issues_before_rewrite.get("structure_issues", [])),
                            "remaining": 0 if structure_fixed else len(issues_before_rewrite.get("structure_issues", []))
                        },
                        "unverified_claims": {
                            "fixed": unverified_fixed,
                            "total_issues": len(unverified_claims_before) if unverified_claims_before else 0,
                            "remaining": 0 if unverified_fixed else (len(unverified_claims_after) if unverified_claims_after else 0)
                        },
                        "framework_mismatches": {
                            "fixed": framework_fixed,
                            "total_issues": len(issues_before_rewrite.get("framework_mismatches", [])),
                            "remaining": 0 if framework_fixed else len(issues_before_rewrite.get("framework_mismatches", []))
                        }
                    }

                    if all_issues_resolved:
                        logger.info(f"Rewrite successful - all tracked issues resolved: {fix_status}")
                        break  # Exit loop, rewrite worked
                    else:
                        # Log which issues remain with detailed tracking
                        remaining_issues = []
                        if not code_snippets_fixed:
                            remaining_issues.append("code snippets")
                        if not function_names_fixed:
                            remaining_issues.append("function names")
                        if not structure_fixed:
                            remaining_issues.append("structure")
                        if not unverified_fixed:
                            remaining_issues.append("unverified claims")
                        if not framework_fixed:
                            remaining_issues.append("framework mismatches")

                        logger.warning(f"Rewrite did not fully resolve issues. Remaining: {', '.join(remaining_issues)}. Fix status: {fix_status}. Will retry.")
                        # Continue to next pass

                    # Add rewrite instruction to conversation
                    rewrite_prompt = f"The previous response had issues: {', '.join(rewrite_reasons[:3])}. Please regenerate the response with these fixes applied."
                    conversation_context.append({
                        "role": "assistant",
                        "content": final_response
                    })
                    conversation_context.append({
                        "role": "user",
                        "content": rewrite_prompt
                    })

                    validation_pass += 1
                    continue  # Retry with rewritten response

                # Response passed validation or this is the last pass
                break

                # Process final response after validation passes
                if final_response:
                # CRITICAL: Filter unverified claims BEFORE validation
                final_response = self._filter_unverified_claims(
                final_response,
                files_read_tracker,
                commands_history
                )

                # Collect code snippet verification results for confidence calculation
                # Use the same helper method to ensure consistency
                code_verification_results_dict = self._extract_and_verify_code_snippets(final_response)

                # Convert to list for backward compatibility with diagnostic_results
                # (but also store as dict for ResponseRewriter)
                code_verification_results = list(code_verification_results_dict.values())

                # Validate architecture claims in response
                architecture_validation = self._validate_architecture_claims(final_response, files_read_tracker)
                if architecture_validation.get("mismatches"):
                arch_warnings = []
                for mismatch in architecture_validation["mismatches"]:
                    arch_warnings.append(f"  - {mismatch}")
                final_response = f"⚠️ ARCHITECTURE MISMATCH:\n" + "\n".join(arch_warnings) + "\n\n" + final_response

                # Execute diagnostic queries from response
                diagnostic_results = self._execute_diagnostic_queries(final_response)

                # Add code verification results to diagnostic_results
                # Store as dict for ResponseRewriter (preferred) and list for backward compatibility
                diagnostic_results["code_verifications"] = code_verification_results  # List for backward compatibility
                diagnostic_results["code_verifications_dict"] = code_verification_results_dict  # Dict for ResponseRewriter

                # Store for use in multi-pass validation
                self._last_diagnostic_results = diagnostic_results

                # Add runtime verification results if any queries were executed
                if diagnostic_results["sql_queries"] or diagnostic_results["api_calls"] or diagnostic_results["log_searches"]:
                runtime_summary = "\n\n🔍 Runtime Verification Results:\n"

                if diagnostic_results["sql_queries"]:
                    runtime_summary += "\n**Database Queries:**\n"
                    for q in diagnostic_results["sql_queries"]:
                        if q["result"].get("success"):
                            runtime_summary += f"  ✓ Query executed: {q['query'][:50]}...\n"
                            runtime_summary += f"    Found {q['result'].get('row_count', 0)} rows\n"
                        else:
                            runtime_summary += f"  ✗ Query failed: {q['result'].get('error', 'Unknown error')}\n"

                if diagnostic_results["api_calls"]:
                    runtime_summary += "\n**API Calls:**\n"
                    for api in diagnostic_results["api_calls"]:
                        if api["result"].get("success"):
                            runtime_summary += f"  ✓ {api['endpoint']}: Status {api['result'].get('status_code')}\n"
                        else:
                            runtime_summary += f"  ✗ {api['endpoint']}: {api['result'].get('error', 'Failed')}\n"

                if diagnostic_results["log_searches"]:
                    runtime_summary += "\n**Log Searches:**\n"
                    for log in diagnostic_results["log_searches"]:
                        if log["result"].get("found"):
                            runtime_summary += f"  ✓ Found {log['result'].get('count', 0)} matches for '{log['pattern']}'\n"
                        else:
                            runtime_summary += f"  ✗ No matches found for '{log['pattern']}'\n"

                final_response += runtime_summary

                # Validate response structure and format
                structure_validation = self.response_validator.validate_response(final_response)
                if not structure_validation.is_valid:
                logger.warning(f"Response structure validation failed: {structure_validation.issues}")
                # Add structure warnings to response
                structure_warnings = []
                if structure_validation.missing_sections:
                    structure_warnings.append(f"Missing sections: {', '.join(structure_validation.missing_sections)}")
                if structure_validation.code_snippet_issues:
                    structure_warnings.append(f"Code snippet issues: {len(structure_validation.code_snippet_issues)}")
                if structure_validation.file_path_issues:
                    structure_warnings.append(f"File path issues: {len(structure_validation.file_path_issues)}")

                if structure_warnings:
                    final_response = f"⚠️ RESPONSE FORMAT ISSUES:\n" + "\n".join(f"  - {w}" for w in structure_warnings) + "\n\n" + final_response

                # Calculate code match accuracy from actual verification
                # Handle both dict format (claimed_code -> verification) and list format (backward compatibility)
                code_match_accuracy = None
                if diagnostic_results:
                # Prefer dict format (preferred), fall back to list format
                code_verifications_dict = diagnostic_results.get("code_verifications_dict")
                code_verifications_list = diagnostic_results.get("code_verifications")

                similarities = []

                # Try dict format first (preferred - used during multi-pass)
                if code_verifications_dict:
                    for claimed_code, verification in code_verifications_dict.items():
                        if isinstance(verification, dict) and verification.get("similarity") is not None:
                            similarities.append(verification["similarity"])

                # Fall back to list format (backward compatibility)
                elif code_verifications_list and isinstance(code_verifications_list, list):
                    for verification in code_verifications_list:
                        if isinstance(verification, dict) and verification.get("similarity") is not None:
                            similarities.append(verification["similarity"])

                if similarities:
                    code_match_accuracy = sum(similarities) / len(similarities)

                # Get verification results from earlier checks
                verification_results_data = {}
                if hasattr(self, '_last_verification_results'):
                verification_results_data = self._last_verification_results

                # Validate response quality and calculate confidence
                validation_issues = self._validate_response_quality(final_response, files_read_tracker)

                # Additional validation: check function names and code claims
                strict_validation = self._validate_claims_against_code(final_response, files_read_tracker, commands_history)
                if strict_validation:
                validation_issues.extend(strict_validation)

                # Calculate enhanced confidence score
                confidence_score = self._calculate_confidence_score(
                files_read_tracker,
                commands_history,
                key_insights,
                validation_issues,
                verification_results=verification_results_data,
                code_match_accuracy=code_match_accuracy,
                runtime_checks=diagnostic_results
                )

                # CONFIDENCE-BASED BLOCKING
                if confidence_score < 0.7:
                logger.warning(f"Low confidence score ({confidence_score:.0%}), blocking response")
                blocking_message = f"⚠️ RESPONSE BLOCKED - Low Confidence ({confidence_score:.0%})\n\n"
                blocking_message += "The analysis has low confidence due to:\n"

                if len(files_read_tracker) < 2:
                    blocking_message += f"  - Only {len(files_read_tracker)} file(s) read (need at least 2)\n"
                if validation_issues:
                    blocking_message += f"  - {len(validation_issues)} validation issue(s) found\n"
                if code_match_accuracy and code_match_accuracy < 0.7:
                    blocking_message += f"  - Code match accuracy only {code_match_accuracy:.0%}\n"
                if not diagnostic_results.get("sql_queries") and not diagnostic_results.get("api_calls"):
                    blocking_message += "  - No runtime verification performed\n"

                blocking_message += "\nPlease:\n"
                blocking_message += "1. READ more files related to the problem\n"
                blocking_message += "2. Use GREP to verify function names\n"
                blocking_message += "3. Request runtime verification (SQL queries, API calls)\n"
                blocking_message += "4. Re-run analysis after completing these steps\n"

                return blocking_message

                if validation_issues:
                logger.warning(f"Response quality issues detected: {validation_issues}")
                # Add note but don't block - let user see it
                if len(files_read_tracker) == 0:
                    final_response = f"⚠️ WARNING: This analysis may not be based on actual code files. Please verify the suggestions.\n\n{final_response}"
                elif strict_validation:
                    # Add specific warnings about unverified claims
                    warnings = "\n".join([f"  - {issue}" for issue in strict_validation[:3]])
                    final_response = f"⚠️ VALIDATION WARNINGS:\n{warnings}\n\nPlease verify these claims with GREP/READ commands.\n\n{final_response}"

                # Add confidence indicator
                if confidence_score < 0.7:
                confidence_note = f"\n\n⚠️ Confidence: {confidence_score:.0%} - Analysis based on limited exploration. Consider reading more files for higher confidence."
                elif confidence_score >= 0.9:
                confidence_note = f"\n\n✅ Confidence: {confidence_score:.0%} - Analysis based on thorough exploration."
                else:
                confidence_note = f"\n\n📊 Confidence: {confidence_score:.0%} - Analysis based on good exploration."

                # Stop heartbeat before returning
                heartbeat_active.clear()

                # Emit done event
                if progress_callback:
                try:
                    all_tasks = task_tracker.get_tasks()
                    completed = sum(1 for t in all_tasks if t["status"] == "done")
                    failed = sum(1 for t in all_tasks if t["status"] == "failed")
                    duration_ms = int((time.time() - agent_start_time) * 1000)
                    progress_callback(json.dumps({
                        "type": "done",
                        "summary": {
                            "tasks_total": len(all_tasks),
                            "completed": completed,
                            "failed": failed,
                            "duration_ms": duration_ms
                        }
                    }), 4, 4)
                except:
                    pass

                # Validate final_response is not None before returning
                if final_response:
                return final_response + confidence_note
                else:
                return "Analysis failed - no response generated." + confidence_note
            # == AGENT_CORE_END ==
        
        # Wrap the core execution in try/except/finally
        try:
            result = _run_agent_core()
            return result
        except Exception as e:
            logger.error(f"Agent mode conversation failed: {e}")
            import traceback
            error_trace = traceback.format_exc()
            logger.error(f"Agent mode error trace: {error_trace}")
            # Stop heartbeat and emit error event
            try:
                heartbeat_active.clear()
                if progress_callback:
                    try:
                        progress_callback(json.dumps({
                            "type": "error",
                            "message": f"Agent mode failed: {str(e)}"
                        }), 4, 4)
                    except:
                        pass
            except:
                pass
            return f"Agent mode failed: {str(e)}"
        finally:
            # Guarantee heartbeat cleanup and final 'done' if not already sent
            try:
                heartbeat_active.clear()
                if progress_callback:
                    try:
                        # Check if we already sent done/error - if not, send done
                        progress_callback(json.dumps({
                            "type": "done",
                            "summary": {"agent_terminated": True}
                        }), 4, 4)
                    except:
                        pass
            except Exception:
                pass
        
        # If we get here, final_response is None
        return "Analysis failed - no response generated after all turns completed."
    
    def _is_vague_problem(self, message: str) -> bool:
        """
        Check if the problem description is too vague.
        
        Args:
            message: The problem description
            
        Returns:
            True if the problem is vague, False otherwise
        """
        message_lower = message.lower()
        
        # Very short messages are likely vague
        if len(message.split()) < 5:
            return True
        
        # Check for vague phrases
        vague_phrases = [
            "doesn't work",
            "not working",
            "broken",
            "issue",
            "problem",
            "bug",
            "error",
            "help"
        ]
        
        # If message only contains vague phrases without specifics
        vague_count = sum(1 for phrase in vague_phrases if phrase in message_lower)
        specific_indicators = [
            "file", "function", "endpoint", "api", "database", "query",
            "line", "code", "variable", "import", "class", "method"
        ]
        specific_count = sum(1 for indicator in specific_indicators if indicator in message_lower)
        
        # Vague if has vague phrases but no specific indicators
        if vague_count > 0 and specific_count == 0:
            return True
        
        # Vague if very short and contains vague phrases
        if len(message.split()) < 10 and vague_count > 0:
            return True
        
        return False
    
    def _suggest_similar_files(self, file_path: str) -> str:
        """
        Suggest similar files if the requested file doesn't exist.
        
        Args:
            file_path: The file path that wasn't found
            
        Returns:
            String with suggestions
        """
        if not file_path:
            return "Please provide a file path (e.g., backend/services/ocr_service.py)"
        
        suggestions = []
        
        # Try to find files with similar names
        try:
            import os
            from pathlib import Path
            
            # Extract filename and directory
            path_parts = file_path.split('/')
            filename = path_parts[-1] if path_parts else file_path
            directory = '/'.join(path_parts[:-1]) if len(path_parts) > 1 else ""
            
            # Search in common directories
            search_dirs = ["backend", "frontend", "backend/services", "backend/routes", "backend/app"]
            
            similar_files = []
            for search_dir in search_dirs:
                if os.path.exists(search_dir):
                    for root, dirs, files in os.walk(search_dir):
                        for file in files:
                            if filename.lower() in file.lower() or file.lower() in filename.lower():
                                full_path = os.path.join(root, file).replace('\\', '/')
                                similar_files.append(full_path)
                                if len(similar_files) >= 3:
                                    break
                        if len(similar_files) >= 3:
                            break
                if len(similar_files) >= 3:
                    break
            
            if similar_files:
                suggestions.append(f"Similar files found:")
                for similar in similar_files[:3]:
                    suggestions.append(f"  - {similar}")
            else:
                # Suggest common files based on problem context
                if "ocr" in file_path.lower():
                    suggestions.append("Try: READ backend/services/ocr_service.py")
                elif "route" in file_path.lower() or "api" in file_path.lower():
                    suggestions.append("Try: READ backend/routes/documents.py or READ backend/main.py")
                elif "db" in file_path.lower() or "database" in file_path.lower():
                    suggestions.append("Try: READ backend/app/db.py")
                else:
                    suggestions.append("Try searching for the file: SEARCH " + filename.split('.')[0])
        except Exception as e:
            logger.debug(f"Error suggesting similar files: {e}")
            suggestions.append("Check the file path spelling and try again")
        
        return "\n".join(suggestions) if suggestions else "Check the file path and try again"
    
    def _optimize_conversation_context(
        self, 
        conversation_context: List[Dict], 
        files_read: set,
        key_insights: List[str]
    ) -> List[Dict]:
        """
        Optimize conversation context by summarizing old turns to save tokens.
        
        Args:
            conversation_context: Full conversation history
            files_read: Files that have been read
            key_insights: Key insights learned
            
        Returns:
            Optimized conversation context
        """
        # If context is short, return as-is
        if len(conversation_context) <= 10:
            return conversation_context
        
        # Keep recent turns (last 5) and summarize older ones
        recent_turns = conversation_context[-5:]
        old_turns = conversation_context[:-5]
        
        if not old_turns:
            return recent_turns
        
        # Create summary of old turns
        summary = {
            "role": "system",
            "content": f"Previous conversation summary:\n"
                      f"- Files read: {', '.join(list(files_read)[:10])}\n"
                      f"- Key insights: {', '.join(key_insights[-5:]) if key_insights else 'None'}\n"
                      f"- {len(old_turns)} previous turns summarized"
        }
        
        return [summary] + recent_turns
    
    def _extract_file_relationships(self, file_path: str, content: str) -> Dict[str, List[str]]:
        """
        Extract relationships between files (imports, function calls, etc.).
        
        Args:
            file_path: Path to the file
            content: File content
            
        Returns:
            Dictionary with relationship information
        """
        relationships = {
            "imports": [],
            "functions": []
        }
        
        import re
        
        # Extract imports
        import_patterns = [
            r'from\s+([\w.]+)\s+import',
            r'import\s+([\w.]+)',
        ]
        
        for pattern in import_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                import_path = match.group(1)
                # Convert to file path
                if import_path.startswith('backend.') or import_path.startswith('frontend.'):
                    potential_path = import_path.replace('.', '/') + '.py'
                    relationships["imports"].append(potential_path)
        
        # Extract function definitions
        func_pattern = r'def\s+(\w+)\s*\('
        matches = re.finditer(func_pattern, content)
        for match in matches:
            relationships["functions"].append(match.group(1))
        
        return relationships
    
    def _extract_insights_from_results(self, results: List[Dict], files_read: set) -> List[str]:
        """
        Extract key insights from exploration results.
        
        Args:
            results: Results from commands
            files_read: Files that have been read
            
        Returns:
            List of insight strings
        """
        insights = []
        
        # Extract insights from search results
        search_results = [r for r in results if r.get("type") == "search"]
        if search_results:
            unique_files = set(r.get("file") for r in search_results if r.get("file"))
            if unique_files:
                insights.append(f"Found {len(unique_files)} file(s) related to the search")
        
        # Extract insights from file reads
        file_results = [r for r in results if r.get("type") == "file"]
        if file_results:
            for file_result in file_results:
                file_path = file_result.get("file", "")
                content = file_result.get("content", "")
                # Look for key functions or patterns
                if "def " in content:
                    func_count = content.count("def ")
                    insights.append(f"{file_path} contains {func_count} function(s)")
        
        # Extract insights from grep results
        grep_results = [r for r in results if r.get("type") == "grep"]
        if grep_results:
            unique_files = set(r.get("file") for r in grep_results if r.get("file"))
            insights.append(f"Pattern found in {len(unique_files)} file(s)")
        
        return insights
    
    def _get_exploration_suggestions(
        self,
        files_read: set,
        commands_history: List[str],
        confidence: float,
        verification_results: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Generate exploration suggestions based on low confidence.
        
        Args:
            files_read: Files that have been read
            commands_history: History of commands executed
            confidence: Current confidence score
            verification_results: Optional verification results
            
        Returns:
            String with suggestions for improving confidence
        """
        suggestions = []
        
        # Check if need more files
        if len(files_read) < 2:
            suggestions.append("📚 Read more files: Use READ command on at least 2-3 relevant files")
            suggestions.append("   Example: READ backend/services/ocr_service.py")
        
        # Check if need function verification
        if not verification_results or not verification_results.get("function_verifications"):
            suggestions.append("🔍 Verify function names: Use GREP to verify functions exist before mentioning them")
            suggestions.append("   Example: GREP 'def upload_file|@app.post.*upload'")
        
        # Check if need framework detection
        if not verification_results or not verification_results.get("framework_results"):
            suggestions.append("🔧 Detect framework: Use GREP to identify FastAPI/Flask, React/vanilla JS")
            suggestions.append("   Example: GREP '@app\\.|from fastapi|from flask'")
        
        # Check if need more diverse commands
        unique_commands = len(set(cmd.split()[0] if isinstance(cmd, str) else cmd.get("type", "") for cmd in commands_history))
        if unique_commands < 2:
            suggestions.append("🛠️ Use diverse commands: Try READ, GREP, SEARCH to explore different aspects")
        
        # Check if need code match verification
        if confidence < 0.6:
            suggestions.append("✅ Verify code snippets: Ensure code examples match actual code in files")
            suggestions.append("   Read files completely to see exact code structure")
        
        if suggestions:
            return "\n".join(suggestions)
        return ""
    
    def _generate_smart_suggestions(self, original_message: str, files_read: set, results: List[Dict], discovered_files: set = None, commands_history: List[Dict] = None) -> str:
        """
        Generate smart suggestions based on the problem and what's been explored.
        
        Args:
            original_message: Original problem description
            files_read: Set of files that have been read
            results: Results from previous commands
            discovered_files: Files discovered from searches but not read
            commands_history: History of all commands used
            
        Returns:
            String with smart suggestions
        """
        suggestions = []
        message_lower = original_message.lower()
        discovered_files = discovered_files or set()
        commands_history = commands_history or []
        
        # Check if we're repeating commands
        recent_reads = [cmd for cmd in commands_history[-5:] if cmd.get("type") == "READ"]
        if len(recent_reads) > 0:
            read_files = [cmd.get("file") for cmd in recent_reads]
            # Check for duplicates
            if len(read_files) != len(set(read_files)):
                suggestions.append("\n💡 SUGGESTION: You've already read some of these files. Try reading different files or use GREP/SEARCH to find new information.")
        
        # Suggest discovered files that haven't been read
        if discovered_files and len(discovered_files) <= 3:
            files_list = ", ".join(list(discovered_files)[:3])
            suggestions.append(f"\n💡 SUGGESTION: Files discovered from searches that you haven't read yet: {files_list}. Consider reading these with READ command.")
        
        # If no files read yet, suggest based on problem
        if len(files_read) == 0:
            if "line item" in message_lower or "line_item" in message_lower:
                suggestions.append("\n💡 SUGGESTION: Start by reading:\n  - READ backend/services/ocr_service.py\n  - READ backend/app/db.py\n  - GREP insert_line_items")
            elif "upload" in message_lower:
                suggestions.append("\n💡 SUGGESTION: Start by reading:\n  - READ backend/routes/documents.py\n  - READ backend/main.py\n  - SEARCH upload")
            elif "invoice" in message_lower:
                suggestions.append("\n💡 SUGGESTION: Start by reading:\n  - READ backend/services/ocr_service.py\n  - READ backend/app/db.py\n  - GREP invoice")
            elif "api" in message_lower or "endpoint" in message_lower:
                suggestions.append("\n💡 SUGGESTION: Start by reading:\n  - READ backend/main.py\n  - READ backend/routes/documents.py\n  - SEARCH api endpoint")
        
        # If files read but no results, suggest next steps
        elif len(results) == 0:
            if len(files_read) < 2:
                suggestions.append("\n💡 SUGGESTION: Read more related files to understand the full flow")
            else:
                suggestions.append("\n💡 SUGGESTION: Use GREP to find specific functions or SEARCH to find related code")
        
        # If we have search results, suggest reading those files
        search_results = [r for r in results if r.get("type") == "search"]
        if search_results and not discovered_files:
            unique_files = set(r.get("file") for r in search_results if r.get("file") and r.get("file") not in files_read)
            if unique_files and len(unique_files) <= 3:
                files_list = ", ".join(list(unique_files)[:3])
                suggestions.append(f"\n💡 SUGGESTION: Search found relevant files: {files_list}. Consider reading these with READ command.")
        
        # If errors in results, suggest fixes
        errors = [r for r in results if r.get("type") == "error"]
        if errors:
            suggestions.append("\n💡 SUGGESTION: Some commands failed. Check file paths and try again, or use SEARCH to find the right files")
        
        # If we've done a lot of exploration, suggest analyzing
        if len(commands_history) > 5 and len(files_read) >= 2:
            suggestions.append("\n💡 SUGGESTION: You've explored enough. Consider using ANALYZE to provide your findings.")
        
        return "\n".join(suggestions) if suggestions else ""
    
    def _execute_single_read_command(
        self,
        cmd: Dict,
        file_cache: Dict,
        file_relationships: Dict,
        discovered_files: set,
        files_read_tracker: set,
        turns: int,
        max_cache_size: int = 50
    ) -> List[Dict]:
        """Execute a single READ command."""
        return self._execute_read_commands_parallel([cmd], file_cache, file_relationships, discovered_files, files_read_tracker, turns, max_cache_size)
    
    def _execute_read_commands_parallel(
        self,
        read_commands: List[Dict],
        file_cache: Dict,
        file_relationships: Dict,
        discovered_files: set,
        files_read_tracker: set,
        turns: int,
        max_cache_size: int = 50
    ) -> List[Dict]:
        """
        Execute multiple READ commands in parallel for better performance.
        
        Args:
            read_commands: List of READ commands to execute
            file_cache: File cache dictionary
            file_relationships: File relationships dictionary
            discovered_files: Set of discovered files
            files_read_tracker: Set of files read
            turns: Current turn number
            
        Returns:
            List of results
        """
        if not read_commands:
            return []
        
        # Prioritize commands (files mentioned in problem first, then discovered files)
        prioritized_commands = self._prioritize_read_commands(read_commands, discovered_files, files_read_tracker)
        
        # Use config for max_lines
        config = get_config()
        max_lines = config.max_lines_per_file
        
        # Execute reads in parallel using ThreadPoolExecutor
        def execute_single_read(cmd: Dict) -> Dict:
            """Execute a single READ command."""
            file_path = cmd.get("file")
            if not file_path:
                return {
                    "type": "error",
                    "command": cmd,
                    "error": "READ command missing 'file' field"
                }
            
            # Check cache first
            if file_path in file_cache:
                logger.debug(f"Using cached content for {file_path}")
                return {
                    "type": "file",
                    "file": file_path,
                    "content": file_cache[file_path]["content"],
                    "cached": True
                }
            
            # Read file with config limit
            file_data = self.code_reader.read_file(file_path, max_lines=max_lines)
            if file_data.get("success"):
                return {
                    "type": "file",
                    "file": file_path,
                    "content": file_data["content"],
                    "file_data": file_data  # Include full file_data for caching
                }
            else:
                error_msg = file_data.get("error", "Unknown error")
                suggestions = self._suggest_similar_files(file_path)
                return {
                    "type": "error",
                    "command": cmd,
                    "error": f"Could not read file '{file_path}': {error_msg}",
                    "suggestions": suggestions
                }
        
        # Execute in parallel
        results = []
        if len(prioritized_commands) > 1:
            # Multiple files - use parallel execution
            max_workers = min(len(prioritized_commands), config.max_parallel_file_reads, min(8, os.cpu_count() or 4))
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_cmd = {
                    executor.submit(execute_single_read, cmd): cmd
                    for cmd in prioritized_commands
                }
                
                for future in as_completed(future_to_cmd):
                    cmd = future_to_cmd[future]
                    try:
                        result = future.result(timeout=30)  # Per-file timeout
                        results.append(result)
                        
                        # Update cache and relationships if successful
                        if result.get("type") == "file" and not result.get("cached"):
                            file_path = result["file"]
                            file_data = result.get("file_data", {})
                            
                            # Limit file cache size
                            if len(file_cache) >= max_cache_size:
                                oldest_key = min(file_cache.keys(), key=lambda k: file_cache[k].get("timestamp", 0))
                                del file_cache[oldest_key]
                            
                            # Cache the file content
                            file_cache[file_path] = {
                                "content": result["content"],
                                "timestamp": turns
                            }
                            
                            # Extract file relationships
                            relationships = self._extract_file_relationships(file_path, result["content"])
                            if relationships:
                                file_relationships[file_path] = relationships
                            
                            # Track files read
                            files_read_tracker.add(file_path)
                            discovered_files.discard(file_path)
                    except Exception as e:
                        logger.warning(f"Error reading file in parallel: {e}")
                        results.append({
                            "type": "error",
                            "command": cmd,
                            "error": str(e)
                        })
        else:
            # Single file - execute directly
            result = execute_single_read(prioritized_commands[0])
            results.append(result)
            
            # Update cache and relationships if successful
            if result.get("type") == "file" and not result.get("cached"):
                file_path = result["file"]
                file_data = result.get("file_data", {})
                
                # Limit file cache size
                if len(file_cache) >= max_cache_size:
                    oldest_key = min(file_cache.keys(), key=lambda k: file_cache[k].get("timestamp", 0))
                    del file_cache[oldest_key]
                
                # Cache the file content
                file_cache[file_path] = {
                    "content": result["content"],
                    "timestamp": turns
                }
                
                # Extract file relationships
                relationships = self._extract_file_relationships(file_path, result["content"])
                if relationships:
                    file_relationships[file_path] = relationships
                
                # Track files read
                files_read_tracker.add(file_path)
                discovered_files.discard(file_path)
        
        return results
    
    def _prioritize_read_commands(
        self,
        commands: List[Dict],
        discovered_files: set,
        files_read: set
    ) -> List[Dict]:
        """
        Prioritize READ commands based on importance.
        
        Priority:
        1. Files already discovered from searches (high relevance)
        2. Files in common directories (backend/services, backend/routes)
        3. Other files
        
        Args:
            commands: List of READ commands
            discovered_files: Files discovered from searches
            files_read: Files already read
            
        Returns:
            Prioritized list of commands
        """
        high_priority = []
        medium_priority = []
        low_priority = []
        
        for cmd in commands:
            file_path = cmd.get("file", "")
            
            # Skip if already read
            if file_path in files_read:
                continue
            
            # High priority: discovered files or common service files
            if (file_path in discovered_files or 
                "backend/services/" in file_path or 
                "backend/routes/" in file_path or
                "backend/app/" in file_path):
                high_priority.append(cmd)
            # Medium priority: backend files
            elif "backend/" in file_path or "frontend/" in file_path:
                medium_priority.append(cmd)
            # Low priority: everything else
            else:
                low_priority.append(cmd)
        
        return high_priority + medium_priority + low_priority
    
    def _find_related_files(
        self, 
        files_read: set, 
        file_relationships: Dict[str, Dict], 
        discovered_files: set
    ) -> set:
        """
        Find files related to the ones already read based on imports/relationships.
        
        Args:
            files_read: Files that have been read
            file_relationships: Dictionary of file relationships
            discovered_files: Files discovered from searches
            
        Returns:
            Set of related file paths
        """
        related = set()
        
        for file_path in files_read:
            if file_path in file_relationships:
                rels = file_relationships[file_path]
                # Add imported files that haven't been read
                for imported_file in rels.get("imports", []):
                    # Check if file exists and hasn't been read
                    import os
                    if os.path.exists(imported_file) and imported_file not in files_read:
                        related.add(imported_file)
                    # Also check variations
                    for variant in [imported_file.replace('/backend/', 'backend/'), 
                                   imported_file.replace('/frontend/', 'frontend/')]:
                        if os.path.exists(variant) and variant not in files_read:
                            related.add(variant)
        
        return related
    
    def _execute_diagnostic_queries(
        self,
        response: str
    ) -> Dict[str, Any]:
        """
        Extract and execute diagnostic queries from LLM response.
        
        Args:
            response: LLM response that may contain SQL queries or API calls
            
        Returns:
            Dict with executed queries and their results
        """
        import re
        results = {
            "sql_queries": [],
            "api_calls": [],
            "log_searches": []
        }
        
        # Extract SQL queries (SELECT statements)
        sql_pattern = r'SELECT\s+[^;]+;'
        sql_queries = re.findall(sql_pattern, response, re.IGNORECASE | re.DOTALL)
        
        for query in sql_queries[:3]:  # Limit to 3 queries
            query = query.strip()
            if query:
                db_result = self.runtime_verifier.query_database(query)
                results["sql_queries"].append({
                    "query": query,
                    "result": db_result
                })
        
        # Extract API calls (curl commands or endpoint mentions)
        curl_pattern = r'curl\s+([^\s]+)'
        curl_matches = re.findall(curl_pattern, response, re.IGNORECASE)
        
        # Also find endpoint mentions like "/api/..."
        endpoint_pattern = r'(/api/[\w/]+)'
        endpoints = re.findall(endpoint_pattern, response)
        
        for endpoint in (curl_matches + endpoints)[:3]:  # Limit to 3 calls
            # Extract endpoint path
            if endpoint.startswith('http'):
                # Extract path from URL
                endpoint_match = re.search(r'/api/[^\s\'"]+', endpoint)
                if endpoint_match:
                    endpoint = endpoint_match.group(0)
            
            if endpoint.startswith('/api/'):
                api_result = self.runtime_verifier.test_api_endpoint(endpoint)
                results["api_calls"].append({
                    "endpoint": endpoint,
                    "result": api_result
                })
        
        # Extract log search patterns
        log_patterns = [
            r'Search logs for ["\']([^"\']+)["\']',
            r'Check logs for ["\']([^"\']+)["\']',
            r'Look for ["\']([^"\']+)["\'] in logs'
        ]
        
        for pattern in log_patterns:
            matches = re.findall(pattern, response, re.IGNORECASE)
            for match in matches[:2]:  # Limit to 2 searches
                log_result = self.runtime_verifier.check_logs(match)
                results["log_searches"].append({
                    "pattern": match,
                    "result": log_result
                })
        
        return results
    
    def _generate_runtime_integration_hints(
        self,
        message: str,
        files_read: set,
        results: List[Dict]
    ) -> str:
        """
        Generate hints for how to actually check runtime behavior.
        
        Args:
            message: Original problem description
            files_read: Files that have been read
            results: Results from exploration
            
        Returns:
            String with runtime integration hints
        """
        hints = []
        message_lower = message.lower()
        
        # Check if database-related files were read
        db_files = [f for f in files_read if "db" in f.lower() or "database" in f.lower()]
        if db_files:
            hints.append("  • Database: Check actual data with SQL queries")
            hints.append("    Example: sqlite3 data/owlin.db \"SELECT * FROM invoices ORDER BY id DESC LIMIT 1;\"")
        
        # Check if API/route files were read
        api_files = [f for f in files_read if "route" in f.lower() or "main.py" in f.lower()]
        if api_files:
            hints.append("  • API: Test endpoints with curl")
            hints.append("    Example: curl http://localhost:8000/api/documents/submit -X POST -H 'Content-Type: application/json'")
        
        # Check if service files were read
        service_files = [f for f in files_read if "service" in f.lower()]
        if service_files:
            hints.append("  • Logs: Check backend logs for execution traces")
            hints.append("    Look for: [LINE_ITEMS], [TABLE_EXTRACT], [OCR] tags")
        
        # If line items mentioned
        if "line item" in message_lower or "line_item" in message_lower:
            hints.append("  • Verify line items: SELECT COUNT(*) FROM invoice_line_items WHERE doc_id = '...';")
        
        # If upload mentioned
        if "upload" in message_lower:
            hints.append("  • Check upload status: curl http://localhost:8000/api/upload/status?doc_id=XXX")
        
        return "\n".join(hints) if hints else ""
    
    def _validate_architecture_claims(
        self,
        response: str,
        files_read: set
    ) -> Dict[str, Any]:
        """
        Validate architecture claims in response against actual detected architecture.
        Uses ArchitectureAnalyzer to validate code suggestions match framework.
        
        Args:
            response: LLM response
            files_read: Files that were read
            
        Returns:
            Dict with 'mismatches', 'detected_architecture', and 'code_validation_results'
        """
        mismatches = []
        detected_architecture = {}
        code_validation_results = []
        
        # Detect architecture from files read
        primary_framework = None
        primary_framework_result = None
        
        for file_path in list(files_read)[:5]:  # Check up to 5 files
            try:
                framework_result = self.architecture_analyzer.detect_framework(file_path)
                if framework_result.get("confidence", 0) > 0.7:
                    framework_name = framework_result["framework"]
                    if framework_name not in detected_architecture:
                        detected_architecture[framework_name] = framework_result
                    # Track primary framework (first high-confidence detection)
                    if primary_framework is None:
                        primary_framework = framework_name
                        primary_framework_result = framework_result
            except:
                continue
        
        # Extract code snippets from response and validate them
        import re
        code_snippet_pattern = r'```(?:python|typescript|javascript)?\n(.*?)```'
        code_snippets = re.findall(code_snippet_pattern, response, re.DOTALL)
        
        # Also check inline code blocks
        inline_code_pattern = r'`([^`]+)`'
        inline_snippets = re.findall(inline_code_pattern, response)
        
        # Validate each code snippet against detected framework
        if primary_framework and primary_framework != "unknown":
            syntax_rules = primary_framework_result.get("syntax_rules", {})
            
            for snippet in code_snippets + inline_snippets[:5]:  # Limit validation
                if len(snippet.strip()) > 20:  # Only validate substantial code
                    validation = self.architecture_analyzer.validate_code_suggestion(
                        snippet,
                        primary_framework,
                        syntax_rules
                    )
                    
                    if not validation.get("valid"):
                        code_validation_results.append(validation)
                        # Add mismatches for blocking
                        for issue in validation.get("issues", []):
                            mismatches.append(f"Code suggestion issue: {issue}")
        
        # Check for framework mismatches in response (legacy checks)
        response_lower = response.lower()
        
        # Check if LLM claims Flask but codebase is FastAPI
        if "flask" in response_lower and "@app.route" in response:
            if "fastapi" in detected_architecture:
                mismatches.append("Response uses Flask syntax (@app.route) but codebase uses FastAPI")
        
        # Check if LLM claims wrong async pattern
        if "def " in response and "async def" not in response:
            # Check if files read use async and framework requires it
            if primary_framework == "fastapi":
                for file_path in files_read:
                    async_result = self.architecture_analyzer.detect_async_patterns(file_path)
                    if async_result.get("has_async") and "async def" not in response_lower:
                        # Check if response has endpoint decorators
                        if any(decorator in response for decorator in ["@app.post", "@app.get", "@router.post", "@router.get"]):
                            mismatches.append(f"Response uses endpoint decorators but missing 'async def' - FastAPI requires async")
                        break
        
        return {
            "mismatches": mismatches,
            "detected_architecture": detected_architecture,
            "code_validation_results": code_validation_results,
            "primary_framework": primary_framework
        }
    
    def _generate_clarifying_questions(self, message: str) -> str:
        """
        Generate clarifying questions based on the vague problem description.
        
        Args:
            message: The vague problem description
            
        Returns:
            String with clarifying questions
        """
        message_lower = message.lower()
        questions = []
        
        # Check what's missing
        if "file" not in message_lower and "function" not in message_lower:
            questions.append("- Which file or function is involved? (e.g., backend/services/ocr_service.py)")
        
        if "error" in message_lower or "bug" in message_lower:
            questions.append("- What is the exact error message or unexpected behavior?")
            questions.append("- When does it happen? (e.g., on upload, on submit, when viewing)")
        
        if "data" in message_lower or "empty" in message_lower or "missing" in message_lower:
            questions.append("- What data is missing or empty? (e.g., line items, invoice totals)")
            questions.append("- Where should the data appear? (e.g., in the UI, in the database)")
        
        if "api" in message_lower or "endpoint" in message_lower:
            questions.append("- Which API endpoint? (e.g., /api/documents/submit)")
            questions.append("- What should the endpoint return vs what it actually returns?")
        
        if not questions:
            # Generic clarifying questions
            questions.append("- What specific functionality is not working?")
            questions.append("- What file or component is involved?")
            questions.append("- What error message or unexpected behavior do you see?")
            questions.append("- What should happen vs what actually happens?")
        
        return "\n".join(questions)
    
    def _check_verification_requirements(
        self,
        message: str,
        files_read: set,
        commands_history: List[str]
    ) -> Dict[str, Any]:
        """
        Check if agent has done required verifications before allowing analysis.
        Uses CodeVerifier to actually verify claims against real code.
        
        Args:
            message: Original problem message
            files_read: Files that were actually read
            commands_history: History of commands executed
            
        Returns:
            Dict with 'can_analyze' (bool) and 'blocking_message' (str if blocked)
        """
        import re
        issues = []
        verification_results = {}
        
        # Extract ALL function names from message (not just hardcoded list)
        # Pattern matches: function_name(...) or function_name(...)
        func_pattern = r'\b([a-zA-Z_][a-zA-Z0-9_]*)\s*\('
        all_func_calls = re.findall(func_pattern, message, re.IGNORECASE)
        
        # Filter out common Python built-ins and keywords that don't need verification
        builtins = {
            'print', 'len', 'str', 'int', 'float', 'list', 'dict', 'set', 'tuple', 'bool',
            'range', 'enumerate', 'zip', 'map', 'filter', 'sorted', 'max', 'min', 'sum',
            'abs', 'round', 'type', 'isinstance', 'hasattr', 'getattr', 'setattr',
            'delattr', 'callable', 'iter', 'next', 'reversed', 'all', 'any', 'bytes',
            'chr', 'ord', 'hex', 'oct', 'bin', 'format', 'repr', 'open', 'file',
            'input', 'eval', 'exec', 'compile', 'globals', 'locals', 'vars', 'dir',
            'help', 'id', 'hash', 'memoryview', 'object', 'super', 'property',
            'staticmethod', 'classmethod', 'abc', 'abstractmethod', 're', 'json',
            'os', 'sys', 'pathlib', 'Path', 'logging', 'logger', 'time', 'datetime'
        }
        
        # Filter to actual function calls (not builtins, not starting with uppercase)
        mentioned_funcs = [
            f for f in all_func_calls 
            if f not in builtins and not f[0].isupper() and len(f) > 1
        ]
        
        # ACTUALLY VERIFY ALL functions exist in codebase
        unverified_funcs = []
        for func in mentioned_funcs:
            # Check if file was read that might contain this function
            likely_file = None
            for file_path in files_read:
                if any(keyword in file_path for keyword in ["main.py", "ocr_service", "db.py", "route", "service"]):
                    likely_file = file_path
                    break
            
            # Actually verify function exists
            verification = self.code_verifier.verify_function_exists(func, likely_file)
            verification_results[func] = verification
            
            if not verification.get("exists"):
                # Try fuzzy matching to find similar function name
                fuzzy_match = self.code_verifier.find_similar_function_name(func, min_similarity=0.75, file_path=likely_file)
                if fuzzy_match.get("found"):
                    verification_results[func]["fuzzy_match"] = fuzzy_match
                    verification_results[func]["suggested_name"] = fuzzy_match.get("correct_name")
                else:
                    unverified_funcs.append(func)
        
        if unverified_funcs:
            issues.append(f"Functions {unverified_funcs} do NOT exist in codebase (verified)")
        
        # PRE-VERIFY code snippets in problem description (not just response)
        code_snippet_issues = []
        # More flexible pattern: handles language tag with/without newline, spaces, or no language tag
        # Try pattern with language tag first, then fallback to pattern without language tag
        code_snippets = []
        # Pattern 1: With language tag (optional spaces/newline after tag)
        pattern_with_lang = r'```(?:python|typescript|javascript|js|ts|tsx|jsx)?\s*\n(.*?)```'
        matches_with_lang = re.findall(pattern_with_lang, message, re.DOTALL)
        code_snippets.extend([m.strip() for m in matches_with_lang if m.strip()])
        
        # Pattern 2: Without language tag (just ```) - only if not already captured
        pattern_no_lang = r'```\s*\n(.*?)```'
        matches_no_lang = re.findall(pattern_no_lang, message, re.DOTALL)
        # Only add if not already in code_snippets (avoid duplicates)
        for m in matches_no_lang:
            if m.strip() and m.strip() not in code_snippets:
                code_snippets.append(m.strip())
        
        if code_snippets:
            # Verify each code snippet against actual files
            for snippet in code_snippets[:5]:  # Limit to 5 snippets
                claimed_code = snippet.strip()
                if not claimed_code:
                    continue
                
                # Find file:line reference near snippet in message
                snippet_start = message.find(snippet)
                context = message[max(0, snippet_start-200):snippet_start+len(snippet)+200]
                file_match = re.search(r'(\w+[/\\][\w/\\]+\.(?:py|ts|tsx|js|jsx)):(\d+)', context)
                
                if file_match:
                    file_path = file_match.group(1).replace('\\', '/')
                    line_num = int(file_match.group(2))
                    line_range = (max(1, line_num - 2), line_num + 5)
                    
                    # Verify the code snippet
                    verification = self.code_verifier.verify_code_snippet(claimed_code, file_path, line_range)
                    verification_results[f"_code_snippet_{len(code_snippet_issues)}"] = verification
                    
                    if not verification.get("matches", False):
                        code_snippet_issues.append({
                            "snippet": claimed_code[:50] + "..." if len(claimed_code) > 50 else claimed_code,
                            "file_path": file_path,
                            "similarity": verification.get("similarity", 0.0)
                        })
                elif files_read:
                    # If no file:line reference, try to verify against files that were read
                    verified_against_file = False
                    # Check all files, not just first 3, to be thorough
                    for file_path in files_read:
                        verification = self.code_verifier.verify_code_snippet(claimed_code, file_path, None)
                        if verification.get("matches", False):
                            verified_against_file = True
                            break
                    
                    if not verified_against_file:
                        code_snippet_issues.append({
                            "snippet": claimed_code[:50] + "..." if len(claimed_code) > 50 else claimed_code,
                            "file_path": "unknown",
                            "similarity": 0.0
                        })
        
        if code_snippet_issues:
            snippet_descriptions = [f"{issue['snippet']} (file: {issue['file_path']}, similarity: {issue['similarity']:.0%})" 
                                   for issue in code_snippet_issues[:3]]
            issues.append(f"Code examples in problem description don't match actual code: {', '.join(snippet_descriptions)}")
        
        # VERIFY framework correctness (not just detection)
        framework_verified = False
        framework_correct = True
        framework_results = {}
        detected_framework = None
        
        if files_read:
            # Check framework for ALL files read (not just first 3) to be comprehensive
            framework_scores = {}
            for file_path in files_read:
                framework_result = self.code_verifier.verify_framework(file_path)
                framework_results[file_path] = framework_result
                framework_name = framework_result.get("framework", "unknown")
                confidence = framework_result.get("confidence", 0)
                
                # Aggregate confidence scores per framework
                if framework_name != "unknown" and confidence > 0.5:
                    if framework_name not in framework_scores:
                        framework_scores[framework_name] = []
                    framework_scores[framework_name].append(confidence)
            
            # Determine framework with highest average confidence
            if framework_scores:
                best_framework = max(framework_scores.items(), 
                                    key=lambda x: sum(x[1]) / len(x[1]))
                detected_framework = best_framework[0]
                avg_confidence = sum(best_framework[1]) / len(best_framework[1])
                if avg_confidence > 0.7:
                    framework_verified = True
        
        # Check if message mentions a framework and verify it matches detected framework
        message_lower = message.lower()
        mentioned_frameworks = []
        if "flask" in message_lower or "@app.route" in message_lower or "request.form" in message_lower:
            mentioned_frameworks.append("flask")
        if "fastapi" in message_lower or "@app.post" in message_lower or "@app.get" in message_lower or "@router" in message_lower:
            mentioned_frameworks.append("fastapi")
        if "react" in message_lower or "vue" in message_lower or "angular" in message_lower:
            mentioned_frameworks.append("frontend")
        
        if mentioned_frameworks and framework_verified:
            # Verify mentioned framework matches detected framework
            # Handle ambiguity: if multiple frameworks mentioned, check if detected is one of them
            if detected_framework not in mentioned_frameworks:
                framework_correct = False
                # Provide clearer message if multiple frameworks mentioned
                if len(mentioned_frameworks) > 1:
                    issues.append(f"Framework mismatch: Message mentions {', '.join(mentioned_frameworks)}, but code uses {detected_framework}")
                else:
                    issues.append(f"Framework mismatch: Message mentions {mentioned_frameworks[0]}, but code uses {detected_framework}")
            elif len(mentioned_frameworks) > 1:
                # Multiple frameworks mentioned, but detected is one of them - this is OK
                # But warn if there's ambiguity
                if detected_framework in mentioned_frameworks:
                    # Detected framework is in the list, so it's correct
                    pass
        
        # Check if message might contain framework-specific code but framework not detected
        if not framework_verified and ("upload" in message_lower or "endpoint" in message_lower or "api" in message_lower or 
                                       "@app" in message_lower or "@router" in message_lower):
            issues.append("Framework not detected - need to verify FastAPI vs Flask in actual code")
        
        # BETTER FILE RELEVANCE CHECK: Verify files contain relevant code, not just count
        files_with_code = 0
        empty_files = []
        if files_read:
            # Check if files are actually code files and contain functions/classes
            # Use smarter detection: check multiple sections of file, not just first 100 lines
            for file_path in files_read:
                # Read more lines (500) or full file if smaller, to catch code later in file
                file_data = self.code_reader.read_file(file_path, max_lines=500)
                if file_data.get("success"):
                    content = file_data.get("content", "")
                    # Expanded pattern to catch more code constructs:
                    # - Python: def, class, @decorator, @dataclass, async def
                    # - TypeScript/JavaScript: function, const, export, interface, type, class
                    # - React: function Component, const Component =, export default
                    code_pattern = r'\b(def|class|function|const|export|interface|type|@\w+|async\s+def|@dataclass|@property)\b'
                    if re.search(code_pattern, content, re.IGNORECASE):
                        files_with_code += 1
                    else:
                        # Also check if file is very small (might be config) - if > 200 chars, likely should have code
                        if len(content.strip()) > 200:
                            empty_files.append(file_path)
                        # If file is small (< 200 chars), might be legit config file, don't flag it
        
        # Only require at least 1 file with actual code (not arbitrary count of 2)
        if files_with_code == 0 and files_read:
            if empty_files:
                issues.append(f"Files {empty_files} don't contain code (functions/classes) - need to read files with actual code")
            else:
                issues.append("No files with actual code found - need to read relevant code files")
        elif files_with_code == 0:
            issues.append("No files read - need to read relevant code files first")
        
        if issues:
            blocking_msg = "⚠️ CANNOT ANALYZE - Missing required verifications:\n\n"
            blocking_msg += "\n".join(f"  • {issue}" for issue in issues)
            blocking_msg += "\n\nREQUIRED ACTIONS:\n"
            
            action_num = 1
            
            if code_snippet_issues:
                blocking_msg += f"  {action_num}. Code examples in problem description don't match actual code.\n"
                blocking_msg += "     READ the actual files to see the correct code before mentioning examples.\n"
                action_num += 1
            
            if unverified_funcs:
                blocking_msg += f"  {action_num}. Functions {unverified_funcs} do NOT exist. Use GREP to find correct function names:\n"
                for func in list(unverified_funcs)[:3]:
                    blocking_msg += f"     GREP \"def.*{func}|{func}\\(\"\"\n"
                action_num += 1
            
            if not framework_verified or not framework_correct:
                if not framework_verified:
                    blocking_msg += f"  {action_num}. Detect framework by reading files:\n"
                    blocking_msg += "     READ backend/main.py (or route files) to see actual framework\n"
                else:
                    blocking_msg += f"  {action_num}. Framework mismatch detected. Verify the correct framework:\n"
                    blocking_msg += f"     READ backend/main.py to see actual framework (detected: {detected_framework})\n"
                action_num += 1
            
            if files_with_code == 0:
                blocking_msg += f"  {action_num}. READ files with actual code (functions/classes), not just config files:\n"
                blocking_msg += "     Example: READ backend/main.py, READ backend/services/ocr_service.py\n"
                action_num += 1
            
            blocking_msg += "\nOnly use ANALYZE after completing these verifications."
            
            return {
                "can_analyze": False, 
                "blocking_message": blocking_msg,
                "verification_results": verification_results,
                "framework_results": framework_results
            }
        
        return {
            "can_analyze": True, 
            "blocking_message": None,
            "verification_results": verification_results,
            "framework_results": framework_results
        }
    
    def _extract_and_verify_code_snippets(self, response: str) -> Dict[str, Dict[str, Any]]:
        """
        Extract code snippets from response and verify them against actual files.
        
        Returns a dict mapping claimed_code -> verification result.
        This is used during multi-pass validation to catch wrong code early.
        
        Args:
            response: Response text containing code snippets
            
        Returns:
            Dict mapping claimed_code (str) -> verification result (dict)
        """
        import re
        
        code_verifications_dict = {}
        
        # Extract code snippets from response
        code_snippet_pattern = r'```(?:python|typescript|javascript|js|ts|tsx|jsx)?\n(.*?)```'
        code_snippets = re.findall(code_snippet_pattern, response, re.DOTALL)
        
        for snippet in code_snippets[:5]:  # Limit to 5 snippets
            claimed_code = snippet.strip()
            if not claimed_code:
                continue
                
            # Find file:line reference near snippet
            snippet_start = response.find(snippet)
            context = response[max(0, snippet_start-200):snippet_start+len(snippet)+200]
            file_match = re.search(r'(\w+[/\\][\w/\\]+\.(?:py|ts|tsx|js|jsx)):(\d+)', context)
            
            if file_match:
                file_path = file_match.group(1).replace('\\', '/')
                line_num = int(file_match.group(2))
                line_range = (max(1, line_num - 2), line_num + 5)
                
                # Verify the code snippet
                verification = self.code_verifier.verify_code_snippet(claimed_code, file_path, line_range)
                
                # Ensure claimed_code is always stored in verification (for consistency)
                # This is required for dict key usage and ResponseRewriter
                if "claimed_code" not in verification:
                    verification["claimed_code"] = claimed_code
                
                # Use claimed_code as the key (not actual_code)
                # This allows ResponseRewriter to find and replace the wrong code
                code_verifications_dict[claimed_code] = verification
        
        return code_verifications_dict
    
    def _extract_and_verify_function_names(self, response: str, files_read: set) -> Dict[str, Dict[str, Any]]:
        """
        Extract function names from response and verify they exist in codebase.
        
        Returns a dict mapping function_name -> verification result.
        This is used during multi-pass validation to catch wrong function names early.
        
        Args:
            response: Response text containing function calls
            files_read: Set of files that were read (for context)
            
        Returns:
            Dict mapping function_name (str) -> verification result (dict)
        """
        import re
        
        function_verifications_dict = {}
        
        # Extract function calls from response
        func_pattern = r'\b([a-zA-Z_][a-zA-Z0-9_]*)\s*\('
        all_func_calls = re.findall(func_pattern, response)
        
        # Filter out builtins and keywords
        builtins = {
            'print', 'len', 'str', 'int', 'float', 'list', 'dict', 'set', 'tuple',
            'bool', 'type', 'isinstance', 'hasattr', 'getattr', 'setattr',
            'range', 'enumerate', 'zip', 'map', 'filter', 'sorted', 'reversed',
            'min', 'max', 'sum', 'abs', 'round', 'any', 'all', 'open', 'file',
            'iter', 'next', 'super', 'self', 'cls', 'True', 'False', 'None',
            'Exception', 'ValueError', 'KeyError', 'TypeError', 'AttributeError',
            'pathlib', 'Path', 'logging', 'logger', 'time', 'datetime'
        }
        
        # Filter to actual function calls (not builtins, not starting with uppercase)
        mentioned_funcs = [
            f for f in all_func_calls 
            if f not in builtins and not f[0].isupper() and len(f) > 1
        ]
        
        # Verify each function
        for func in mentioned_funcs[:20]:  # Limit to 20 functions
            # Check if file was read that might contain this function
            likely_file = None
            for file_path in files_read:
                if any(keyword in file_path for keyword in ["main.py", "ocr_service", "db.py", "route", "service"]):
                    likely_file = file_path
                    break
            
            # Verify function exists
            verification = self.code_verifier.verify_function_exists(func, likely_file)
            
            # If doesn't exist, try fuzzy matching
            if not verification.get("exists"):
                fuzzy_match = self.code_verifier.find_similar_function_name(
                    func, min_similarity=0.75, file_path=likely_file
                )
                if fuzzy_match.get("found"):
                    verification["fuzzy_match"] = fuzzy_match
                    verification["suggested_name"] = fuzzy_match.get("correct_name")
            
            function_verifications_dict[func] = verification
        
        return function_verifications_dict
    
    def _filter_unverified_claims(
        self,
        response: str,
        files_read: set,
        commands_history: List[str]
    ) -> str:
        """
        Filter or flag unverified claims in response before showing to user.
        Uses CodeVerifier to actually verify claims against real code.
        
        Args:
            response: The response text to filter
            files_read: Files that were actually read
            commands_history: History of commands executed
            
        Returns:
            Filtered response with warnings for unverified claims
        """
        import re
        
        warnings = []
        
        # Find ALL function mentions in response (not just hardcoded list)
        func_pattern = r'\b([a-zA-Z_][a-zA-Z0-9_]*)\s*\('
        all_func_calls = re.findall(func_pattern, response)
        
        # Filter out common Python built-ins and keywords
        builtins = {
            'print', 'len', 'str', 'int', 'float', 'list', 'dict', 'set', 'tuple', 'bool',
            'range', 'enumerate', 'zip', 'map', 'filter', 'sorted', 'max', 'min', 'sum',
            'abs', 'round', 'type', 'isinstance', 'hasattr', 'getattr', 'setattr',
            'delattr', 'callable', 'iter', 'next', 'reversed', 'all', 'any', 'bytes',
            'chr', 'ord', 'hex', 'oct', 'bin', 'format', 'repr', 'open', 'file',
            'input', 'eval', 'exec', 'compile', 'globals', 'locals', 'vars', 'dir',
            'help', 'id', 'hash', 'memoryview', 'object', 'super', 'property',
            'staticmethod', 'classmethod', 'abc', 'abstractmethod', 're', 'json',
            'os', 'sys', 'pathlib', 'Path', 'logging', 'logger', 'time', 'datetime',
            'async', 'await', 'yield', 'return', 'pass', 'break', 'continue'
        }
        
        # Filter to actual function calls (not builtins, not starting with uppercase)
        func_mentions = [
            f for f in all_func_calls 
            if f not in builtins and not f[0].isupper() and len(f) > 1
        ]
        
        # ACTUALLY VERIFY ALL functions exist (not just common_funcs list)
        unverified_funcs = []
        verified_funcs_with_issues = []
        
        for func in func_mentions:
            # Actually verify function exists
            likely_file = None
            for file_path in files_read:
                if any(keyword in file_path for keyword in ["main.py", "ocr_service", "db.py", "route", "service"]):
                    likely_file = file_path
                    break
            
            verification = self.code_verifier.verify_function_exists(func, likely_file)
            
            if not verification.get("exists"):
                # Try fuzzy matching to find similar function name
                fuzzy_match = self.code_verifier.find_similar_function_name(func, min_similarity=0.75, file_path=likely_file)
                if fuzzy_match.get("found"):
                    verified_funcs_with_issues.append({
                        "wrong_name": func,
                        "correct_name": fuzzy_match.get("correct_name"),
                        "similarity": fuzzy_match.get("similarity", 0.0)
                    })
                else:
                    unverified_funcs.append(func)
            else:
                # Function exists, but verify signature if we can extract it from response
                # Check if response mentions parameters that don't match
                func_context_pattern = rf'\b{re.escape(func)}\s*\([^)]*\)'
                func_matches = re.finditer(func_context_pattern, response)
                for match in func_matches:
                    # Try to extract claimed signature from context
                    context_start = max(0, match.start() - 50)
                    context_end = min(len(response), match.end() + 50)
                    context = response[context_start:context_end]
                    
                    # Look for signature-like patterns
                    sig_pattern = rf'(?:def|async def)\s+{re.escape(func)}\s*\([^)]*\)'
                    if re.search(sig_pattern, context):
                        # Found a signature claim, verify it
                        claimed_sig_match = re.search(sig_pattern, context)
                        if claimed_sig_match:
                            claimed_sig = claimed_sig_match.group(0)
                            sig_verification = self.code_verifier.verify_function_signature(func, claimed_sig, likely_file)
                            if not sig_verification.get("matches") and sig_verification.get("similarity", 1.0) < 0.9:
                                verified_funcs_with_issues.append({
                                    "wrong_name": func,
                                    "issue": "signature_mismatch",
                                    "similarity": sig_verification.get("similarity", 0.0),
                                    "actual_signature": sig_verification.get("actual_signature"),
                                    "claimed_signature": claimed_sig
                                })
        
        if unverified_funcs:
            warnings.append(f"⚠️ UNVERIFIED FUNCTIONS: {unverified_funcs} do NOT exist in codebase (verified). Please use correct function names.")
        
        if verified_funcs_with_issues:
            for issue in verified_funcs_with_issues:
                if "correct_name" in issue:
                    warnings.append(f"⚠️ FUNCTION NAME SUGGESTION: '{issue['wrong_name']}' not found. Did you mean '{issue['correct_name']}'? (similarity: {issue['similarity']:.0%})")
                elif issue.get("issue") == "signature_mismatch":
                    warnings.append(f"⚠️ SIGNATURE MISMATCH: Function '{issue['wrong_name']}' signature doesn't match. Actual: {issue.get('actual_signature', 'unknown')}")
        
        # ACTUALLY VERIFY framework
        if "@app.route" in response or "request.form" in response:
            # Check actual framework in files read
            framework_verified = False
            actual_framework = None
            for file_path in files_read:
                if any(keyword in file_path for keyword in ["main.py", "route", "app"]):
                    framework_result = self.code_verifier.verify_framework(file_path)
                    if framework_result.get("confidence", 0) > 0.7:
                        actual_framework = framework_result.get("framework")
                        framework_verified = True
                        break
            
            if framework_verified and actual_framework == "fastapi":
                warnings.append("⚠️ FRAMEWORK WARNING: Flask syntax (@app.route, request.form) used but codebase uses FastAPI. Use @app.post() and async def instead.")
            elif not framework_verified:
                warnings.append("⚠️ FRAMEWORK WARNING: Framework not verified. Please check actual code.")
        
        # ACTUALLY VERIFY code snippets with file:line references
        code_snippet_pattern = r'```(?:python|typescript|javascript)?\n(.*?)```'
        code_snippets = re.findall(code_snippet_pattern, response, re.DOTALL)
        
        # Also find inline code with file:line references
        file_line_pattern = r'`([^`]+)`.*?(\w+\.(?:py|ts|tsx|js|jsx)):(\d+)'
        inline_code = re.findall(file_line_pattern, response)
        
        invalid_code_snippets = []
        for snippet in code_snippets + [code[0] for code in inline_code]:
            # Try to find file:line reference near snippet
            snippet_start = response.find(snippet)
            context = response[max(0, snippet_start-200):snippet_start+len(snippet)+200]
            
            file_match = re.search(r'(\w+[/\\][\w/\\]+\.(?:py|ts|tsx|js|jsx)):(\d+)', context)
            if file_match:
                file_path = file_match.group(1).replace('\\', '/')
                line_num = int(file_match.group(2))
                
                # Verify code snippet matches actual code
                line_range = (max(1, line_num - 2), line_num + 5)  # Check around the line
                verification = self.code_verifier.verify_code_snippet(snippet.strip(), file_path, line_range)
                
                if not verification.get("matches") and verification.get("similarity", 0) < 0.7:
                    invalid_code_snippets.append(f"{file_path}:{line_num} (similarity: {verification.get('similarity', 0):.0%})")
        
        if invalid_code_snippets:
            warnings.append(f"⚠️ CODE MISMATCH: Code examples don't match actual code at: {', '.join(invalid_code_snippets[:3])}")
        
        # Check for generic file paths
        generic_paths = re.findall(r'\b(\w+\.py):(\d+)', response)
        unverified_paths = []
        for path, line in generic_paths:
            if path not in str(files_read) and not any(path in f for f in files_read):
                unverified_paths.append(f"{path}:{line}")
        
        if unverified_paths:
            warnings.append(f"⚠️ GENERIC PATHS: {unverified_paths[:3]} - should use full paths from READ commands")
        
        # Prepend warnings if any
        if warnings:
            return "\n\n".join(warnings) + "\n\n" + response
        
        return response
    
    def _extract_specific_unverified_claims(
        self,
        response: str,
        files_read: set,
        commands_history: List[str]
    ) -> Dict[str, Any]:
        """
        Extract specific unverified claims from response (not just check for warnings).
        
        Returns structured data about unverified claims so we can verify each one
        was fixed after rewrite, instead of relying on fragile string checks.
        
        Args:
            response: The response text to check
            files_read: Files that were actually read
            commands_history: History of commands executed
            
        Returns:
            Dict with specific unverified claims:
            {
                "unverified_functions": [list of function names],
                "function_name_suggestions": [list of wrong_name -> correct_name mappings],
                "signature_mismatches": [list of function names with mismatched signatures],
                "framework_warnings": [list of framework warnings],
                "invalid_code_snippets": [list of file:line references],
                "generic_paths": [list of generic file paths]
            }
        """
        import re
        
        claims = {
            "unverified_functions": [],
            "function_name_suggestions": [],
            "signature_mismatches": [],
            "framework_warnings": [],
            "invalid_code_snippets": [],
            "generic_paths": []
        }
        
        # Find ALL function mentions in response
        func_pattern = r'\b([a-zA-Z_][a-zA-Z0-9_]*)\s*\('
        all_func_calls = re.findall(func_pattern, response)
        
        # Filter out common Python built-ins and keywords
        builtins = {
            'print', 'len', 'str', 'int', 'float', 'list', 'dict', 'set', 'tuple', 'bool',
            'range', 'enumerate', 'zip', 'map', 'filter', 'sorted', 'max', 'min', 'sum',
            'abs', 'round', 'type', 'isinstance', 'hasattr', 'getattr', 'setattr',
            'delattr', 'callable', 'iter', 'next', 'reversed', 'all', 'any', 'bytes',
            'chr', 'ord', 'hex', 'oct', 'bin', 'format', 'repr', 'open', 'file',
            'input', 'eval', 'exec', 'compile', 'globals', 'locals', 'vars', 'dir',
            'help', 'id', 'hash', 'memoryview', 'object', 'super', 'property',
            'staticmethod', 'classmethod', 'abc', 'abstractmethod', 're', 'json',
            'os', 'sys', 'pathlib', 'Path', 'logging', 'logger', 'time', 'datetime',
            'async', 'await', 'yield', 'return', 'pass', 'break', 'continue'
        }
        
        # Filter to actual function calls (not builtins, not starting with uppercase)
        func_mentions = [
            f for f in all_func_calls 
            if f not in builtins and not f[0].isupper() and len(f) > 1
        ]
        
        # Verify functions and track specific issues
        for func in func_mentions[:20]:  # Limit to 20 functions
            likely_file = None
            for file_path in files_read:
                if any(keyword in file_path for keyword in ["main.py", "ocr_service", "db.py", "route", "service"]):
                    likely_file = file_path
                    break
            
            verification = self.code_verifier.verify_function_exists(func, likely_file)
            
            if not verification.get("exists"):
                # Try fuzzy matching
                fuzzy_match = self.code_verifier.find_similar_function_name(func, min_similarity=0.75, file_path=likely_file)
                if fuzzy_match.get("found"):
                    claims["function_name_suggestions"].append({
                        "wrong_name": func,
                        "correct_name": fuzzy_match.get("correct_name"),
                        "similarity": fuzzy_match.get("similarity", 0.0)
                    })
                else:
                    claims["unverified_functions"].append(func)
            else:
                # Function exists, check signature if mentioned
                func_context_pattern = rf'\b{re.escape(func)}\s*\([^)]*\)'
                func_matches = re.finditer(func_context_pattern, response)
                for match in func_matches:
                    context_start = max(0, match.start() - 50)
                    context_end = min(len(response), match.end() + 50)
                    context = response[context_start:context_end]
                    
                    sig_pattern = rf'(?:def|async def)\s+{re.escape(func)}\s*\([^)]*\)'
                    if re.search(sig_pattern, context):
                        claimed_sig_match = re.search(sig_pattern, context)
                        if claimed_sig_match:
                            claimed_sig = claimed_sig_match.group(0)
                            sig_verification = self.code_verifier.verify_function_signature(func, claimed_sig, likely_file)
                            if not sig_verification.get("matches") and sig_verification.get("similarity", 1.0) < 0.9:
                                claims["signature_mismatches"].append({
                                    "function": func,
                                    "similarity": sig_verification.get("similarity", 0.0),
                                    "actual_signature": sig_verification.get("actual_signature")
                                })
        
        # Check framework warnings
        if "@app.route" in response or "request.form" in response:
            framework_verified = False
            actual_framework = None
            for file_path in files_read:
                if any(keyword in file_path for keyword in ["main.py", "route", "app"]):
                    framework_result = self.code_verifier.verify_framework(file_path)
                    if framework_result.get("confidence", 0) > 0.7:
                        actual_framework = framework_result.get("framework")
                        framework_verified = True
                        break
            
            if framework_verified and actual_framework == "fastapi":
                claims["framework_warnings"].append("Flask syntax used but codebase uses FastAPI")
            elif not framework_verified:
                claims["framework_warnings"].append("Framework not verified")
        
        # Check invalid code snippets
        code_snippet_pattern = r'```(?:python|typescript|javascript)?\n(.*?)```'
        code_snippets = re.findall(code_snippet_pattern, response, re.DOTALL)
        
        file_line_pattern = r'`([^`]+)`.*?(\w+\.(?:py|ts|tsx|js|jsx)):(\d+)'
        inline_code = re.findall(file_line_pattern, response)
        
        for snippet in code_snippets + [code[0] for code in inline_code]:
            snippet_start = response.find(snippet)
            context = response[max(0, snippet_start-200):snippet_start+len(snippet)+200]
            
            file_match = re.search(r'(\w+[/\\][\w/\\]+\.(?:py|ts|tsx|js|jsx)):(\d+)', context)
            if file_match:
                file_path = file_match.group(1).replace('\\', '/')
                line_num = int(file_match.group(2))
                
                line_range = (max(1, line_num - 2), line_num + 5)
                verification = self.code_verifier.verify_code_snippet(snippet.strip(), file_path, line_range)
                
                if not verification.get("matches") and verification.get("similarity", 0) < 0.7:
                    claims["invalid_code_snippets"].append(f"{file_path}:{line_num}")
        
        # Check generic file paths
        generic_paths = re.findall(r'\b(\w+\.py):(\d+)', response)
        for path, line in generic_paths:
            if path not in str(files_read) and not any(path in f for f in files_read):
                claims["generic_paths"].append(f"{path}:{line}")
        
        # Return dict only if there are any claims (empty dict means no issues)
        if any(claims.values()):
            return claims
        return {}
    
    def _validate_claims_against_code(
        self,
        response: str,
        files_read: set,
        commands_history: List[str]
    ) -> List[str]:
        """
        Validate claims in the response against actual code.
        
        Checks:
        - Function names mentioned actually exist (via GREP in history)
        - Code examples match actual code structure
        - Framework assumptions are correct
        - Logging claims are verified
        
        Args:
            response: The final analysis response
            files_read: Files that were actually read
            commands_history: History of commands executed
            
        Returns:
            List of validation warnings
        """
        warnings = []
        
        # Extract function names mentioned in response
        import re
        function_pattern = r'(\w+)\s*\([^)]*\)'
        mentioned_functions = re.findall(function_pattern, response)
        
        # Check if function names were verified with GREP
        grep_commands = [cmd for cmd in commands_history if "GREP" in cmd.upper()]
        verified_functions = set()
        
        for grep_cmd in grep_commands:
            # Extract patterns from GREP commands
            if "def " in grep_cmd or "@app" in grep_cmd or "@router" in grep_cmd:
                # Try to extract function name from grep pattern
                func_match = re.search(r'def\s+(\w+)|(\w+)\s*\(', grep_cmd)
                if func_match:
                    verified_functions.add(func_match.group(1) or func_match.group(2))
        
        # Check for unverified function mentions
        common_functions = ["upload_file", "process_document", "insert_line_items", 
                          "extract_table", "get_line_items", "submit_document",
                          "upload_document", "handle_upload", "extract_table_from_block"]
        
        for func in mentioned_functions:
            if func in common_functions and func not in verified_functions:
                # Check if file containing this function was read
                func_verified = False
                for file_path in files_read:
                    if any(keyword in file_path for keyword in ["main.py", "ocr_service", "db.py", "route"]):
                        func_verified = True
                        break
                
                if not func_verified:
                    warnings.append(f"Function '{func}()' mentioned but not verified with GREP")
        
        # Check for Flask syntax in FastAPI codebase
        if "@app.route" in response or "request.form" in response:
            # Check if framework was detected
            framework_detected = any("fastapi" in cmd.lower() or "@app.post" in cmd or "@router" in cmd 
                                    for cmd in commands_history)
            if not framework_detected:
                warnings.append("Flask syntax (@app.route, request.form) used but framework not verified - should check if FastAPI")
        
        # Check for made-up code examples
        if "```python" in response or "```" in response:
            # Count code blocks
            code_blocks = response.count("```")
            # If many code blocks but few files read, might be made up
            if code_blocks > 4 and len(files_read) < 2:
                warnings.append("Multiple code examples provided but few files read - verify examples are from actual code")
        
        # Check for generic file paths
        generic_paths = re.findall(r'(\w+\.py):(\d+)', response)
        for path, line in generic_paths:
            if path not in str(files_read) and not any(path in f for f in files_read):
                warnings.append(f"Generic file path '{path}:{line}' used - should use full path from READ commands")
        
        return warnings
    
    def _validate_response_quality(self, response: str, files_read: set) -> List[str]:
        """
        Validate that the response follows quality guidelines.
        
        Args:
            response: The response text to validate
            files_read: Set of files that were actually read
            
        Returns:
            List of validation issues found
        """
        issues = []
        
        # Check for actual file paths (not generic names)
        has_actual_paths = "backend/" in response or "frontend/" in response
        has_generic_paths = any(pattern in response.lower() for pattern in ["file.py", "ocr.py", "service.py", "utils.py"])
        
        if not has_actual_paths and has_generic_paths:
            issues.append("Response contains generic file names instead of actual paths")
        
        # Check if response mentions files that weren't read
        if files_read:
            mentioned_files = set()
            import re
            # Find file references in response
            file_pattern = r'(backend|frontend)[/\w\-\.]+\.(py|ts|tsx|js|jsx)'
            for match in re.finditer(file_pattern, response):
                mentioned_files.add(match.group(0))
            
            # Check if files mentioned were actually read
            unread_mentioned = mentioned_files - files_read
            if unread_mentioned and len(files_read) < 3:
                # Only warn if very few files were read but many mentioned
                issues.append(f"Response mentions files that weren't read: {unread_mentioned}")
        
        # Check for required format sections
        required_sections = ["Code Analysis", "Prioritized Diagnosis", "Root Cause", "Fix"]
        missing_sections = [section for section in required_sections if section not in response]
        if missing_sections and len(response) > 500:  # Only check if response is substantial
            issues.append(f"Missing required sections: {missing_sections}")
        
        # Check for specific diagnostic queries (not generic)
        has_specific_queries = any(pattern in response for pattern in ["SELECT", "curl", "Search logs for"])
        has_generic_queries = any(pattern in response.lower() for pattern in ["check logs", "check database", "check api"])
        
        if has_generic_queries and not has_specific_queries:
            issues.append("Response contains generic verification requests instead of specific queries")
        
        return issues
    
    def _calculate_confidence_score(
        self,
        files_read: set,
        commands_history: List[str],
        key_insights: List[str],
        validation_issues: List[str],
        verification_results: Optional[Dict[str, Any]] = None,
        code_match_accuracy: Optional[float] = None,
        runtime_checks: Optional[Dict[str, Any]] = None
    ) -> float:
        """
        Calculate confidence score based on exploration quality, verification results,
        code match accuracy, and runtime checks.
        
        WEIGHTING: Function verification > file count (verification is more important)
        
        Args:
            files_read: Files that were read
            commands_history: History of commands used
            key_insights: Key insights learned
            validation_issues: Validation issues found
            verification_results: Optional verification results from CodeVerifier
            code_match_accuracy: Optional average code match accuracy (0.0-1.0)
            runtime_checks: Optional runtime verification results
            
        Returns:
            Confidence score between 0.0 and 1.0
        """
        score = 0.0
        
        # Base score from files read (max 0.15) - REDUCED from 0.25
        # Files are important but verification matters more
        files_score = min(len(files_read) / 5.0, 1.0) * 0.15
        score += files_score
        
        # Score from commands used (max 0.1) - REDUCED from 0.15
        if commands_history:
            unique_commands = len(set(cmd.split()[0] if isinstance(cmd, str) else cmd.get("type", "") for cmd in commands_history))
            commands_score = min(unique_commands / 4.0, 1.0) * 0.1
            score += commands_score
        
        # Score from insights (max 0.1)
        if key_insights:
            insights_score = min(len(key_insights) / 5.0, 1.0) * 0.1
            score += insights_score
        
        # Score from verification results (max 0.35) - INCREASED from 0.25
        # Function verification is MOST IMPORTANT - weighted higher than file count
        if verification_results:
            verification_score = 0.0
            func_verifications = verification_results.get("function_verifications", {})
            framework_results = verification_results.get("framework_results", {})
            
            # Check function verification pass rate (max 0.25) - INCREASED from 0.15
            if func_verifications:
                verified_count = sum(1 for v in func_verifications.values() if v.get("exists"))
                total_count = len(func_verifications)
                if total_count > 0:
                    verification_score += (verified_count / total_count) * 0.25
            
            # Check framework detection (max 0.1)
            if framework_results:
                framework_detected = any(r.get("confidence", 0) > 0.7 for r in framework_results.values())
                if framework_detected:
                    verification_score += 0.1
            
            score += verification_score
        
        # Score from code match accuracy (max 0.2) - INCREASED from 0.15
        # Code match accuracy is important for confidence
        if code_match_accuracy is not None:
            code_score = code_match_accuracy * 0.2
            score += code_score
        
        # Score from runtime checks (max 0.1)
        if runtime_checks:
            runtime_score = 0.0
            if runtime_checks.get("sql_queries"):
                successful_queries = sum(1 for q in runtime_checks["sql_queries"] if q.get("result", {}).get("success"))
                total_queries = len(runtime_checks["sql_queries"])
                if total_queries > 0:
                    runtime_score += (successful_queries / total_queries) * 0.05
            
            if runtime_checks.get("api_calls"):
                successful_apis = sum(1 for a in runtime_checks["api_calls"] if a.get("result", {}).get("success"))
                total_apis = len(runtime_checks["api_calls"])
                if total_apis > 0:
                    runtime_score += (successful_apis / total_apis) * 0.05
            
            score += runtime_score
        
        # Penalty for validation issues (max -0.3)
        if validation_issues:
            penalty = min(len(validation_issues) / 5.0, 1.0) * 0.3
            score -= penalty
        
        # Bonus for diverse exploration (max +0.1)
        if len(files_read) >= 3 and len(commands_history) >= 5:
            score += 0.1
        
        return max(0.0, min(1.0, score))  # Clamp between 0 and 1
    
    def _extract_code_references(self, response_text: str, code_context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract code file references from response text."""
        references = []
        
        # Add files that were read
        for file_path in code_context.get("files", []):
            file_data = self.code_reader.read_file(file_path, max_lines=5000)  # Increased for local
            if file_data.get("success"):
                references.append({
                    "file": file_path,
                    "lines": None,
                    "snippet": file_data["content"][:500]  # First 500 chars
                })
        
        return references

